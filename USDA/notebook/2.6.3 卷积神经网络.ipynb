{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6524b063-bbc5-41ab-a77c-504891eb7deb",
   "metadata": {},
   "source": [
    "> Created on Tue Jan 12 13:48:15 2021 @author: Richie Bao-caDesign设计(cadesign.cn)__+updated on Sun Jan  9 16:29:44 2022 by Richie Bao "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3a9ca-3829-4d0e-847d-cc412eea42bd",
   "metadata": {},
   "source": [
    "## 2.6.3 卷积神经网络\n",
    "\n",
    "### 2.6.3.1 卷积神经网络(Convolutional neural network, CNN)—— 卷积原理与卷积神经网络\n",
    "\n",
    "在阅读卷积神经网络(CNNs)之前，如果已经阅读‘卷积’，‘计算机视觉，特征提取和尺度空间’等部分章节，可以更好的理解CNNs。其中‘尺度空间’的概念，通过降采样的不同空间分辨率影像（类似池化，pooLing）和不同$\\sigma$值的高斯核卷积(即卷积计算或称为互相关运算)，来提取图像的概貌特征，这与CNNs的多层卷积网络如出一辙，只是CNNs除了卷积层，还可以自由加入其它的数据处理层，提升图像特征捕捉的几率。\n",
    "\n",
    "一幅图像的意义来自于邻近的一组像素，而不是单个像素自身，因为单个像素并不包含关于整个图像的信息。例如下图(引自*PyTorch Artificial Intelligence Fundamentals*)很好的说明了这两者的差异。一个全连接的神经网络（密集层，dense layer），一层的每个节点都连接到下一层的每个节点，并不能很好的反映节点之间的关系，也具有较大的计算量；但是卷积网络利用像素之间的空间结构减少了层之间的连接数量，显著提高了训练的速度，减少了模型的参数，更重要的是反映了图像特征像素之间的关系，即图像内容中各个对象是由自身与邻近像素关系决定的空间结构（即特征）所表述。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_01.jpg\" height='auto' width='700' title=\"caDesign\"></a>\n",
    "\n",
    "二维卷积层的卷积计算，就是表征图像的数组shape(c,h,w)（如果为灰度图像则只有一个颜色通道，如果是彩色图像(RGB)则有三个图像通道），每个通道(h,w)二维数组中每一像素，与卷积核(filter/kernal)的加权和计算。这个过程存在一些可变动的因素：一个是步幅(stride)，卷积窗口（卷积核）从输入数组（二维图像数组）的最左上方开始，按从左到右，从上往下的顺序，依次在输入数组上滑动，每次滑动的行数和列数称为步幅。步幅越小代表扑捉不同内容对象的精度越高。下述图表（参考[*Convolutional Neural Networks*](https://cs231n.github.io/convolutional-networks/)）卷积核的滑动步幅为2，即行列均跨2步后计算；二是填充(padding)，如果不设置填充，并且从左上角滑动，会有一部分卷积核对应空值（没有图像/数据）。同时要保持四周填充的行列数相同，通常使用奇数高宽的卷积核；三是，如果是对图像数据实现卷积，通常包括1个单通道，或3个多通道的情况。对于多通道的计算是可以配置不同的卷积核对应不同的通道，各自通道分别卷积后，计算和（累加）作为结果输出。同时可以增加并行的新的卷积计算，获取多个输出，例如下图的Filter W0，和Filter W1的($3 \\times 3$)卷积核，W0和W1各自包含3个卷积核对应3个通道输入，并各自输出。\n",
    "\n",
    "卷积运算，步幅、填充配置，以及多通道卷积都没有改变图像的空间尺寸（空间分辨率），即尺度空间概念下降采样的表述（可以反映不同对象的尺度大小，或理解为只有在不同的尺度下才可以捕捉到对象的特征）。池化层(pooling)正是降采样在卷积神经网络中的表述，可以降低输入的空间维数，保留输入的深度。在图像卷积过程中，识别出比实际像素信息更多的概念意义，识别保留输入的关键信息，丢弃冗余部分。池化层不仅捕捉尺度空间下的对象特征，同时可以减少训练所需时间，减小模型的参数数量，降低模型复杂度，更好的泛化等。池化层可以用`nn.MaxPool2d`，取最大值；`nn.AvgPool2d`，取均值等方法。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_02.jpg\" height='auto' width='auto' title=\"caDesign\"></a>\n",
    "\n",
    "卷积层和池化层都有一个`dilation`参数，可以翻译为膨胀。通过dilation配置，可以调整卷积核与图像的作用域，即感受野（receptive filed）。当$3 \\times 3$的卷积核dilation=1时，没有膨胀效应；当dilation=2时，感受野扩展至$7 \\times 7$；当dilation=24时，感受野扩展至$15 \\times 15$。可以确定当dilation线性增加时，感受野时呈指数增加。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_03.jpg\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "\n",
    ">  参考文献：\n",
    "1. Jibin Mathew.PyTorch Artificial Intelligence Fundamentals: A recipe-based approach to design, build and deploy your own AI models with PyTorch 1.x[m].UK:Packt Publishing (February 28, 2020)\n",
    "2. Aston Zhang,Zack C. Lipton,Mu Li,etc.Dive into Deep Learning[M].；中文版-阿斯顿.张,李沐,扎卡里.C. 立顿,亚历山大.J. 斯莫拉.动手深度学习[M].人民邮电出版社,北京,2019-06-01\n",
    "3. [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/)\n",
    "4. [Understanding 2D Dilated Convolution Operation with Examples in Numpy and Tensorflow with Interactive Code](https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25)\n",
    "\n",
    "建立图表中的输入数据`t_input`，张量形状为(batchsize, nChannels, Height, Width)=(1,3,7,7)，即只有一幅图像，通道数为3，高度为7，宽度为7。通常查看时，只要确定最后一维内的数据为图像每一行的像素值（由上至下）。PyTorch提供的`nn.Conv2d`卷积方法不需自定义卷积核，其参数为`torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')`。如果需要自定义卷积核，则调用`torch.nn.functional.conv2d`方法，其中`weight`参数即为卷积核，其输入参数为`torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d4a28a-770f-417a-b1ea-8baf3876929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_input.shape=torch.Size([1, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0716,  0.4418,  0.2253],\n",
       "          [ 0.5419,  1.3328,  0.6916],\n",
       "          [-0.5093, -0.2429,  0.3159]],\n",
       "\n",
       "         [[-0.5181,  0.4435, -0.2968],\n",
       "          [-0.5857, -0.9900, -0.4077],\n",
       "          [-0.4969, -0.1312, -0.1665]]]], grad_fn=<ThnnConv2DBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#batchsize, nChannels, Height, Width\n",
    "t_input=torch.tensor([[[[0,0,0,0,0,0,0],[0,2,2,2,0,0,0],[0,1,0,2,0,0,0],[0,0,0,1,0,1,0],[0,1,1,1,1,0,0],[0,2,2,0,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,2,1,1,1,0],[0,2,1,0,1,1,0],[0,0,0,0,0,1,0],[0,2,0,2,1,1,0],[0,0,2,0,1,2,0],[0,0,0,0,0,0,0]],\n",
    "                      [[0,0,0,0,0,0,0],[0,1,0,0,1,1,0],[0,1,0,2,0,1,0],[0,2,0,1,2,1,0],[0,1,2,1,2,2,0],[0,1,1,2,0,0,0],[0,0,0,0,0,0,0]],\n",
    "                     ]],dtype=torch.float)\n",
    "print(\"t_input.shape={}\".format(t_input.shape))\n",
    "conv_2d_3c=nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3,stride=2,padding=0,bias=1)\n",
    "conv_2d_3c(t_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c6ff56-2ea9-4481-879f-b88d08454171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., -2., -1.],\n",
      "          [-4.,  1., -2.],\n",
      "          [ 2., -4.,  0.]]]])\n",
      "tensor([[[[7., 7., 3.],\n",
      "          [3., 4., 4.],\n",
      "          [1., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "w_0=torch.tensor([[[[-1, 1, 0],\n",
    "                    [ 0,-1, 0],\n",
    "                    [-1,-1, 1]],\n",
    "\n",
    "                   [[0,-1,-1],\n",
    "                    [-1,1,1],\n",
    "                    [-1,-1,1]],\n",
    "\n",
    "                   [[-1,1,0],\n",
    "                    [0,0,1],\n",
    "                    [0,0,-1]]]],dtype=torch.float)\n",
    "\n",
    "w_1=torch.tensor([[[[0, -1, -1],\n",
    "                    [ 1,-1, 1],\n",
    "                    [1,1, 1]],\n",
    "\n",
    "                   [[0,0,1],\n",
    "                    [0,1,1],\n",
    "                    [0,1,0]],\n",
    "\n",
    "                   [[0,0,0],\n",
    "                    [0,0,1],\n",
    "                    [0,1,-1]]]],dtype=torch.float)\n",
    "\n",
    "b_0=torch.tensor([1])\n",
    "b_1=torch.tensor([0])\n",
    "output_0=F.conv2d(input=t_input,weight=w_0,stride=2,padding=0,bias=b_0)\n",
    "output_1=F.conv2d(input=t_input,weight=w_1,stride=2,padding=0,bias=b_1)\n",
    "print(output_0)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adcf18-2f2f-4f4c-8947-faa49f57a91c",
   "metadata": {},
   "source": [
    "最大池化，即设定池化（卷积核）大小，返回覆盖范围内最大值，其参数为`torch.nn.MaxPool2d(kernel_size: Union[T, Tuple[T, ...]], stride: Optional[Union[T, Tuple[T, ...]]] = None, padding: Union[T, Tuple[T, ...]] = 0, dilation: Union[T, Tuple[T, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22545e82-5c5e-4b5e-8921-1049327c7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据形状=torch.Size([1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_input=torch.tensor([[[0,-2,-1],[-4,1,-2],[2,-4,0]]],dtype=torch.float)\n",
    "print(\"输入数据形状={}\".format(pooling_input.shape))\n",
    "\n",
    "maxPool_2d=nn.MaxPool2d(kernel_size=2,stride=1)\n",
    "maxPool_2d(pooling_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5390a60-7d84-45e8-947f-e11f136914fb",
   "metadata": {},
   "source": [
    "### 2.6.3.2 卷积_特征提取器--->_分类器，可视化卷积层/卷积核，及tensorboard\n",
    "\n",
    "#### 1）卷积层，池化层输出尺寸（形状）计算，及根据输入输出尺寸反推填充pad\n",
    "\n",
    "构建卷积神经网络，很重要的一步是确定图像经过一次或多次卷积后，输出的尺寸，用于分类器（线性模型，全连接层）的输入。在PyTorch的['torch.nn.Conv2d'](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)类方法说明中都会给出卷积输出的尺寸计算公式，但是手工的计算方式容易出错，并且耗时耗力，因此通常将其编写为代码程序。池化层的输出尺寸计算实际上与卷积的输出相同，但是为了区分，仍然在`conv2d_output_size_calculation`类中，增加了` pooling_output_shape`方法（直接调用`conv2d_output_shape`）。卷积的方式除了给定h_w/图像高宽，kernel_size/卷积核（过滤器，filter）,stride/步幅，pad/填充，以及dilation/膨胀参数，卷积核的初始位置可以分为：以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算，即`conv2d_output_shape`函数； 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算，即`convtransp2d_output_shape`函数。\n",
    "\n",
    "同时，也可以根据卷积的输入和输出的尺寸，反推填充的大小，对应卷积核初始位置的不同分别为`conv2d_get_padding`和`convtransp2d_get_padding`函数。\n",
    "\n",
    "对于卷积神经网络'net_fashionMNIST'的网络结构如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_04.jpg\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "输入图像的大小为$28 \\times 28$，经过卷积层conv1-->池化层pool-->卷积层conv2-->池化层pool后，图像尺寸的大小变为$4 \\times 4$，该值（卷积层的输出值）由自定义`conv2d_output_size_calculation`计算。注意两次池化，其参数值相同，因此卷积神经网络结构定义时，可以仅定义一个池化方法，用于生成不同位置的池化层。计算应用卷积提取特征部分的图像输出大小后，因为conv2+pool之后，输出的'out_channels'输出通道数配置为16，因此到分类器部分（线性函数/全连接层/展平层）的输入大小为$16 \\times 4 \\times 4 = 256$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e51ce897-7132-45f4-a645-d2f67ecc71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d_output_size_calculation:\n",
    "    '''\n",
    "    class - PyTorch 卷积层，池化层输出尺寸(shape)计算，及根据输入，输出尺寸(shape)反推pad填充大小\n",
    "    \n",
    "    @author:sytelus Shital Shah\n",
    "    Updated on Tue Jan 12 19:17:22 2021 @author: Richie Bao-caDesign设计(cadesign.cn)\n",
    "    '''   \n",
    "    \n",
    "    def num2tuple(self,num):\n",
    "        '''\n",
    "        function - 如果num=2，则返回(2,2)；如果num=(2,2)，则返回(2,2).\n",
    "        '''\n",
    "        return num if isinstance(num, tuple) else (num, num)\n",
    "    \n",
    "    def conv2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        funciton - 计算PyTorch的nn.Conv2d卷积方法的输出尺寸。以卷积核左上角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        \n",
    "        h_w, kernel_size, stride, pad, dilation = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = math.floor((h_w[0] + sum(pad[0]) - dilation[0]*(kernel_size[0]-1) - 1) / stride[0] + 1)\n",
    "        w = math.floor((h_w[1] + sum(pad[1]) - dilation[1]*(kernel_size[1]-1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def convtransp2d_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0):\n",
    "        '''\n",
    "        function - 以卷积核右下角对位图像左上角第一个像素值开始由左-->右，由上-->下卷积计算\n",
    "        '''\n",
    "        h_w, kernel_size, stride, pad, dilation, out_pad = self.num2tuple(h_w), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(pad), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "        pad = self.num2tuple(pad[0]), self.num2tuple(pad[1])\n",
    "        \n",
    "        h = (h_w[0] - 1)*stride[0] - sum(pad[0]) + dilation[0]*(kernel_size[0]-1) + out_pad[0] + 1\n",
    "        w = (h_w[1] - 1)*stride[1] - sum(pad[1]) + dilation[1]*(kernel_size[1]-1) + out_pad[1] + 1\n",
    "        \n",
    "        return h, w\n",
    "    \n",
    "    def conv2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n",
    "        import math\n",
    "        '''\n",
    "        function - conv2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation)\n",
    "        \n",
    "        p_h = ((h_w_out[0] - 1)*stride[0] - h_w_in[0] + dilation[0]*(kernel_size[0]-1) + 1)\n",
    "        p_w = ((h_w_out[1] - 1)*stride[1] - h_w_in[1] + dilation[1]*(kernel_size[1]-1) + 1)\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))  #((pad_up, pad_bottom)， (pad_left, pad_right))\n",
    "    \n",
    "    def convtransp2d_get_padding(self,h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0):\n",
    "        import math\n",
    "        '''\n",
    "        function - convtransp2d_output_shape 方法的逆，求填充pad\n",
    "        '''\n",
    "        h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = self.num2tuple(h_w_in), self.num2tuple(h_w_out), \\\n",
    "            self.num2tuple(kernel_size), self.num2tuple(stride), self.num2tuple(dilation), self.num2tuple(out_pad)\n",
    "            \n",
    "        p_h = -(h_w_out[0] - 1 - out_pad[0] - dilation[0]*(kernel_size[0]-1) - (h_w_in[0] - 1)*stride[0]) / 2\n",
    "        p_w = -(h_w_out[1] - 1 - out_pad[1] - dilation[1]*(kernel_size[1]-1) - (h_w_in[1] - 1)*stride[1]) / 2\n",
    "        \n",
    "        return (math.floor(p_h/2), math.ceil(p_h/2)), (math.floor(p_w/2), math.ceil(p_w/2))\n",
    "    \n",
    "    def pooling_output_shape(self,h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        '''\n",
    "        function - pooling池化层输出尺寸，同conv2d_output_shape\n",
    "        '''\n",
    "        return self.conv2d_output_shape(h_w, kernel_size=kernel_size, stride=stride, pad=pad, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74f24f79-db71-4646-8c9d-4bd18617b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_size=(24, 24)\n",
      "size_pool1=(12, 12)\n",
      "size_conv2=(8, 8)\n",
      "size_pool2=(4, 4)\n"
     ]
    }
   ],
   "source": [
    "conv2dSize_cal=conv2d_output_size_calculation()\n",
    "size_conv1=conv2dSize_cal.conv2d_output_shape(28, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool1=conv2dSize_cal.pooling_output_shape(24, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "size_conv2=conv2dSize_cal.conv2d_output_shape(12, kernel_size=5, stride=1, pad=0, dilation=1)\n",
    "size_pool2=conv2dSize_cal.pooling_output_shape(8, kernel_size=2, stride=2, pad=0, dilation=1)\n",
    "\n",
    "print(\"conv1_size={}\\nsize_pool1={}\\nsize_conv2={}\\nsize_pool2={}\".format(size_conv1,size_pool1,size_conv2,size_pool2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafc1f1-e020-4e9a-8670-e9805838e2a8",
   "metadata": {},
   "source": [
    "在设计卷积层，或者包含很多卷积层时，上述的方法可以进一步改进，定义一个函数可以一次性计算所有的卷积层。在输入参数设置时，使用了列表和元组的形式，固定输入参数'input'，'conv'和'pool'，值的参数依次为[h_w, kernel_size, stride, pad, dilation]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3b56def-2626-4f7b-9395-d3d87faeacd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷积层输出尺寸=(4, 4)\n"
     ]
    }
   ],
   "source": [
    "convs_params=[\n",
    "            ('input',(28,28)),\n",
    "            ('conv',[5,1,0,1]),  #h_w, kernel_size, stride, pad, dilation\n",
    "            ('pool',[2,2,0,1]),\n",
    "            ('conv',[5,1,0,1]),\n",
    "            ('pool',[2,2,0,1]),    \n",
    "            ]\n",
    "\n",
    "def conv2d_outputSize_A_oneTime(convs_params):\n",
    "    '''\n",
    "    fucntion - 一次性计算卷积输出尺寸\n",
    "    '''\n",
    "    conv2dSize_cal=conv2d_output_size_calculation()\n",
    "    for v in convs_params:\n",
    "        if v[0]=='input':\n",
    "            h_w=v[1]\n",
    "        elif v[0]=='conv' or v[0]=='pool':            \n",
    "            kernel_size, stride, pad, dilation=v[1]\n",
    "            h_w, kernel_size, stride, pad, dilation=conv2dSize_cal.num2tuple(h_w),conv2dSize_cal.num2tuple(kernel_size), conv2dSize_cal.num2tuple(stride),conv2dSize_cal.num2tuple(pad),conv2dSize_cal.num2tuple(dilation)\n",
    "            h_w=conv2dSize_cal.conv2d_output_shape(h_w, kernel_size, stride, pad, dilation)        \n",
    "    return h_w\n",
    " \n",
    "output_h_w=conv2d_outputSize_A_oneTime(convs_params)    \n",
    "print(\"卷积层输出尺寸={}\".format(output_h_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd84562-dacd-4774-add4-84ef31658038",
   "metadata": {},
   "source": [
    "#### 2）构建简单的卷积神经网络识别fashionMNIST数据集，与tensorboard\n",
    "\n",
    "此处构建上图给出的神经网络结构，input-->conv1(relu)-->pool-->conv2(relu)-->pool--->fc1(relu)-->fc2(relu)-->fc3-->output，同时应用tensorboard可以写入并自动的根据写入的内容图示卷积神经网络结构，损失曲线，预测结果，样本信息，以及自定义的内容等。详细内容可以查看[torch.utils.tensorboard](https://pytorch.org/docs/stable/tensorboard.html)。\n",
    "\n",
    "> 参考：[Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html?highlight=fashion%20mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0dfb03-4cb5-49f6-bfbd-f84b3cc7bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f26e25074784b6b92d5373e8f139e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc6b19afd364a8b8e1185e643d36b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cabf23df2fa438b9c096efc4f4f7e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f7347c78f045a28ff3c5fc4862cd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST_norm\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#01-下载/读取fashinMNIST数据，以及构建训练、测试可迭代对象。（如果已经下载，则直接读取）\n",
    "\n",
    "def load_fashionMNIST(root,batchsize=4,num_workers=2,resize=None,n_mean=0.5,n_std=0.5):\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    '''\n",
    "    function - 下载读取fashionMNIST数据集，并建立训练、测试可迭代数据集\n",
    "    '''\n",
    "    trans= [transforms.ToTensor(), #转换PIL图像或numpy.ndarray为tensor张量\n",
    "            transforms.Normalize((0.5,), (0.5,))] #torchvision.transforms.Normalize(mean, std, inplace=False)，用均值和标准差，标准化张量图像       \n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(size=resize))  \n",
    "    transform=transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform) \n",
    "    mnist_test=torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    #DataLoade-读取小批量\n",
    "    import torch.utils.data as data_utils\n",
    "    batch_size=batchsize\n",
    "    num_workers=num_workers\n",
    "    trainloader=data_utils.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader=data_utils.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader,testloader\n",
    "\n",
    "trainloader,testloader=load_fashionMNIST(root='./dataset/FashionMNIST_norm')\n",
    "classes=('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59c5151-e368-485c-accb-2080f07b74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_fashionMNIST(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#02-定义网络结构，特征提取层+分类器\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class net_fashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_fashionMNIST,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*4*4,120) #torch.nn.Linear(in_features: int, out_features: int, bias: bool = True)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*4*4)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "net_fashionMNIST_=net_fashionMNIST()\n",
    "print(net_fashionMNIST_)\n",
    "\n",
    "#03-定义损失函数核优化算法\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion=nn.CrossEntropyLoss() #定义损失函数\n",
    "optimizer=optim.SGD(net_fashionMNIST_.parameters(), lr=0.001, momentum=0.9) #定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88fd52bf-2f76-4671-9973-00aff9f6bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#04-定义显示图像函数（一个batch批次）\n",
    "\n",
    "import torchvision\n",
    "# helper functions\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np    \n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "#05-调入初始化tensorboard.SummaryWriter（指定数据写入的保存路径），并写入训练图像与模型\n",
    "        \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer=SummaryWriter(r'./runs/fashion_mnist_experiment_1')\n",
    "\n",
    "#提取图像（数量为batch_size） get some random training images\n",
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "#建立图像格网 create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "#显示图像 show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)\n",
    "\n",
    "writer.add_graph(net_fashionMNIST_, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c508c37-3cd2-491f-9059-e3e9dd76e6a5",
   "metadata": {},
   "source": [
    "在终端，切换到tensorboard数据保存文件夹runs所在的目录，执行`tensorboard --logdir=runs`后，通常提示在浏览器中打开`http://localhost:6006/`地址，可以查看`writer.add_image('four_fashion_mnist_images', img_grid)`写入的训练图像信息内容(其语法为：`add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')`)，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_05.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "`writer.add_graph(net_fashionMNIST_, images)`（其语法为：`add_graph(model, input_to_model=None, verbose=False)`），写入的信息内容下，可以查看网络结构及运算的流程，这对模型的构建与调整有所帮助。\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_06.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "下述在训练过程中，`riter.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i)`(其语法为：`add_scalar(tag, scalar_value, global_step=None, walltime=None)`)，写入损失数据图表，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_07.png\" height='auto' width='1000' title=\"caDesign\"></a>\n",
    "\n",
    "writer.add_figure('predictions vs. actuals',plot_classes_preds(net_fashionMNIST_, inputs, labels),global_step=epoch * len(trainloader) + i)(其语法为：`add_figure(tag, figure, global_step=None, close=True, walltime=None)`)，写入`plot_classes_preds`返回图表，包括图像，预测值及其概率，如下：\n",
    "\n",
    "<a href=\"\"><img src=\"./imgs/2_6_3_08.png\" height='auto' width='1000' title=\"caDesign\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788b13c8-5bda-4bee-b46f-a6dbc330e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#06-定义将图像应用训练的网络（模型）预测及其概率待写入tensorboard文件的函数\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    function - 用训练的网络预测给定的一组图像，并计算相应的概率 Generates predictions and corresponding probabilities from a trainednetwork and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    '''\n",
    "    function - 用训练的网络预测给定的一组图像，并计算概率后，显示图像、预测值以及概率，和实际的标签 Generates matplotlib Figure using a trained network, along with images and labels from a batch, that shows the network's top prediction along with its probability, alongside the actual label, coloring this information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1bb78d18-228d-491d-a05e-e7694d437a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ee478dfcb643e98779988dfa693e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0,running_loss=365.6714180938143\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#07-训练模型，同时向tensorboard文件写入相关信息\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "running_loss = 0.0\n",
    "epochs=1\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net_fashionMNIST_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()                  \n",
    "        if i % 1000==999:    # every 1000 mini-batches...\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i) #写入损失数值\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',plot_classes_preds(net_fashionMNIST_, inputs, labels),global_step=epoch * len(trainloader) + i)  #写入自定义函数  plot_classes_preds返回的图表，包括图像，预测值及其概率\n",
    "            loss_temp=running_loss\n",
    "            running_loss=0.0\n",
    "    \n",
    "    print(\"epoch={},running_loss={}\".format(epoch,loss_temp))     \n",
    "    loss_temp=0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b2b32-773a-4138-bb30-b736e302ad55",
   "metadata": {},
   "source": [
    "#### 3）可视化卷积层/卷积核\n",
    "\n",
    "通常一个卷积层的输出通道（output channel）有6，16，32，64，256，等不确定的输出个数及更多的输出个数，即一个卷积层包含输出通道数目的filter/kernal过滤器/卷积核；而卷积核通过$3 \\times 3$，$5 \\times 5$，$7 \\times 7$，等不确定尺寸（通常为奇数）或者更大尺寸来提取图像的特征，不同的卷积核提取的图像特征不同，或者表述为不同的卷积核关注不同的特征提取，这类似于图像的关键点描述子（位于不同的尺度空间下）。不过因为卷积核的多样性，卷积提取的特征更加丰富多样。通过一个卷积层的多个卷积核（尺度空间的水平向，表述各个像素值与各周边像素值的差异程度），及多个卷积层（尺度空间的纵深向，表述图像特征所在的（对应的）空间分辨率，例如遥感影像看清建筑轮廓的空间分辨率约为5-15m，看清行人的空间分辨率约为0.3-1m，而若要看清人的五官，空间分辨率率则约为0.01-0.05m，这与对象（特征）的尺寸有关）提取了大量的图像特征，将这些图像特征flatten展平，就构成了该图像的特征集合(feature maps)。\n",
    "\n",
    "下述定义的`conv_retriever`类用于取回卷积神经网络中所有卷积层及其权重值（卷积核），取回的卷积层可以直接输入图像数据计算该卷积。例如上述网络取回的卷积层有两个。函数`visualize_convFilte`则可以打印显示卷积核。函数`visualize_convLaye`则能打印显示指定数目的所有卷积图像结果。\n",
    "\n",
    "> 参考 [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "724aec37-3fce-47fe-b076-02e132f0e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV: Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([6, 1, 5, 5])\n",
      "CONV: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([16, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class conv_retriever:\n",
    "    '''\n",
    "    class - 取回卷积神经网络中所有卷积层及其权重\n",
    "    '''\n",
    "    def __init__(self,net):\n",
    "        self.net=net\n",
    "        self.model_weights=[] # we will save the conv layer weights in this list\n",
    "        self.conv_layers=[] # we will save the 49 conv layers in this list\n",
    "        self.model_children=list(net.children()) # get all the model children as list\n",
    "        self.counter=0\n",
    "        \n",
    "    def retriever(self):\n",
    "        import torch.nn as nn\n",
    "        for i in range(len(self.model_children)):\n",
    "            if type(self.model_children[i])==nn.Conv2d:                \n",
    "                self.counter+=1\n",
    "                self.model_weights.append(self.model_children[i].weight)\n",
    "                self.conv_layers.append(self.model_children[i])\n",
    "            elif type(self.model_children[i])==nn.Sequential:\n",
    "                for j in range(len(self.model_children[i])):\n",
    "                    for child in self.model_children[i][j].children():\n",
    "                        if type(child)==nn.Conv2d:\n",
    "                            self.counter+=1\n",
    "                            self.model_weights.append(child.weight)\n",
    "                            self.conv_layers.append(child)       \n",
    "         \n",
    "        \n",
    "conv_retriever_=conv_retriever(net_fashionMNIST_)\n",
    "conv_retriever_.retriever()\n",
    "#print(conv_retriever_.conv_layers) #conv_retriever_.model_weights\n",
    "for weight, conv in zip(conv_retriever_.model_weights, conv_retriever_.conv_layers):\n",
    "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a8dfb-05a8-4fa9-9d16-980c73fc3830",
   "metadata": {},
   "source": [
    "卷积核显示中，像素越黑，值越小，趋于0；而像素越亮，值越大，趋于255.0。因此越白的像素，在卷积过程中对应位置图像的像素的权重越大，对特征影响越重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9fb2cd4-1c60-47ff-b2d3-c459361443ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_convFilter(conv_layer,model_weight,output_name,figsize=(10,10)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    '''\n",
    "    function - 可视化卷积核 visualize the conv layer filters\n",
    "    '''\n",
    "    plt.figure(figsize=figsize)\n",
    "    kernel_size=conv_layer.kernel_size\n",
    "    for i,filter in enumerate(model_weight):\n",
    "        plt.subplot(kernel_size[0]+1,kernel_size[1]+1,i+1)\n",
    "        plt.imshow(filter[0,:,:].detach(),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(r'./results/%s'%output_name)\n",
    "    plt.show()        \n",
    "        \n",
    "visualize_convFilter(conv_retriever_.conv_layers[0],conv_retriever_.model_weights[0],output_name='fashion_MNIST_filter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e9196-08fc-48f5-8649-bffaad734e1b",
   "metadata": {},
   "source": [
    "卷积（层）结果的显示，往往可以观察到不同的特征，例如明显的黑色区域轮廓为商标标识，对象的轮廓也能够通过代表不同颜色的值区分开来等。这为观察不同的卷积核提取了图像哪些特征，为相关研究或者网络调试提供参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f054dac0-29dd-45e2-8cdc-9a6f38be4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layer:0,layer.size=torch.Size([6, 24, 24])\n",
      "\n",
      "Saving layer 0 feature maps...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layer:1,layer.size=torch.Size([16, 20, 20])\n",
      "\n",
      "Saving layer 1 feature maps...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_convLayer(imgs_batch,conv_layers,model_weights,num_show=6,figsize=(10,10)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    '''\n",
    "    function - 可视化所有卷积层（卷积结果）\n",
    "    '''\n",
    "    results=[conv_layers[0](imgs_batch)]\n",
    "    for i in range(1,len(conv_layers)):\n",
    "        results.append(conv_layers[i](results[-1]))\n",
    "    outputs=results\n",
    "    \n",
    "    for num_layer in range(len(outputs)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        layer_viz=outputs[num_layer][0,:,:,:]\n",
    "        layer_viz=layer_viz.data\n",
    "        print(\"num_layer:{},layer.size={}\".format(num_layer,layer_viz.size()))\n",
    "        \n",
    "        print()\n",
    "        kernel_size=conv_layers[num_layer].kernel_size\n",
    "        for i,filter in enumerate(layer_viz):\n",
    "            if i==num_show:\n",
    "                break\n",
    "            \n",
    "            plt.subplot(kernel_size[0]+1,kernel_size[1]+1,i+1)\n",
    "            plt.imshow(filter,cmap='gray')\n",
    "            plt.axis('off')\n",
    "        print(f\"Saving layer {num_layer} feature maps...\")\n",
    "        plt.savefig(f'./results/layer_{num_layer}.png')\n",
    "        plt.show()\n",
    "        #plt.close() #如果只保存，不需要显示打印，则可以开启plt.close()，并注释掉plt.show()      \n",
    "    \n",
    "visualize_convLayer(images,conv_retriever_.conv_layers,conv_retriever_.model_weights,num_show=6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59122885-0190-43b3-aea8-ad5e745926c5",
   "metadata": {},
   "source": [
    "### 2.6.3.3 [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "\n",
    "`torchvision.models`库包含有解决不同任务的预先模型定义(通过配置参数pretrained=True，可以下载已经训练模型的参数)，包括：image classification图像分类、pixelwise sementic segmentation像素语义分割、object detection对象检测、instance segmentation实例分割、person keypoint detection 人关键点检测和video classification视频分类等。目前包括的模型如下：\n",
    "\n",
    "| 用途      | 模型/网络 |\n",
    "| ----------- | ----------- |\n",
    "| Classification      | AlexNet, VGG,ResNet, SqueezeNet, DenseNet, Inception v3, GoogLeNet,ShuffleNet v2,MobileNet v2,ResNeXt, Wide ResNet, MNASNet|\n",
    "| Semantic Segmentation   | FCN ResNet50, ResNet101; DeepLabV3 ResNet50, ResNet101       |\n",
    "| Object Detection, Instance Segmentation and Person Keypoint Detection|Faster R-CNN ResNet-50 FPN ,Mask R-CNN ResNet-50 FPN|\n",
    "| Video classification|ResNet 3D,  ResNet Mixed Convolution,ResNet (2+1)D|\n",
    "\n",
    "#### 1） 自定义VGG网络\n",
    "VGG作者研究了在大规模图像识别设置中，卷积深度网络对精度的影响。其贡献是使用非常小的卷积核($3 \\times 3$)滤波器的架构对深度增加的网络进行全面的评估，表面深度推进到16-19个权重层，可以实现对现有配置的显著改进。同时该模型可以很好的泛化到其它数据集。VGG的网络结构可以看作不断重复的模块，因此在定义模型时可以先定义规律性的模块，然后给定配置的卷积层数(num_convs)，输入通道数(in_channels)和输出通道数(out_channels)，调用模块实现完了的架构，避免代码冗长。其中包括5个卷积模块（block），前2块为单层卷积，后3层为双层卷积。该网络总共8个卷积层和3个全连接层，所以也称为VGG-11。\n",
    "\n",
    "> 参考文献：\n",
    "```\n",
    "1. @misc{simonyan2015deep,\n",
    "      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, \n",
    "      author={Karen Simonyan and Andrew Zisserman},\n",
    "      year={2015},\n",
    "      eprint={1409.1556},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}\n",
    "```\n",
    "\n",
    "> 注： * 收集元组中所有位置参数 ** 收集字典中的所有关键字参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "890ea014-999d-4a95-aa65-22597767be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*s_position=('a', 'b', 'c', 'd', 'e')\n",
      "**d_keywords={'param_a': 1, 'param_b': 2, 'param_c': 3}\n"
     ]
    }
   ],
   "source": [
    "def func_asterisk(*s_position,**d_keywords):\n",
    "    print(\"*s_position={}\\n**d_keywords={}\".format(s_position,d_keywords))\n",
    "\n",
    "func_asterisk('a','b','c','d','e',param_a=1,param_b=2,param_c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5f97ede-9671-4969-8137-ca4c550d275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG网络结构：\n",
      " Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): flattenLayer()\n",
      "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author:《动手学深度学习》/'Dive into PyTorch'\n",
    "Updated on Thu Jan 14 17:45:17 2021 @author: Richie Bao-caDesign设计(cadesign.cn)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch  import nn,optim\n",
    "\n",
    "#将每批次样本X的形状转换为(batch_size,-1)\n",
    "from torch import nn\n",
    "class flattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        if i==0:\n",
    "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*blk)  \n",
    "\n",
    "conv_arch=((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "fc_features=512*7*7\n",
    "fc_hidden_units=4096\n",
    "\n",
    "def vgg(conv_arch,fc_features,fc_hidden_units=4096):\n",
    "    net=nn.Sequential()\n",
    "    for i,(num_convs,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_convs,in_channels,out_channels))\n",
    "    net.add_module('fc',nn.Sequential(flattenLayer(),\n",
    "                                      nn.Linear(fc_features,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,10)        \n",
    "                                     ))\n",
    "    return net\n",
    "\n",
    "VGG_net=vgg(conv_arch,fc_features,fc_hidden_units)\n",
    "print(\"VGG网络结构：\\n\",VGG_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170eaf83-265d-4beb-b8b0-0796c12d739a",
   "metadata": {},
   "source": [
    "配置完卷积层（特征提取层），可以通过上述定义的`conv2d_outputSize_A_oneTime`函数，计算全连接层的输入尺寸。计算结果为(7,7)，将其与卷积层最后一层的输出通道数相乘就为全连接层的输入尺寸$512 \\times 7 \\times 7=25088$，这与VGG的输入值相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a032c3bb-c45d-4a58-bf75-2b2841c879f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷积层输出尺寸=(7, 7)\n"
     ]
    }
   ],
   "source": [
    "VGG_convs_params=[\n",
    "            ('input',(224,224)),\n",
    "            ('conv',[3,1,1,1]),  #kernel_size, stride, pad, dilation\n",
    "            ('pool',[2,2,0,1]),\n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),\n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),  \n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1]),  \n",
    "    \n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('conv',[3,1,1,1]),\n",
    "            ('pool',[2,2,0,1])     \n",
    "            ]\n",
    "\n",
    "VGG_output_h_w=conv2d_outputSize_A_oneTime(VGG_convs_params)    \n",
    "print(\"卷积层输出尺寸={}\".format(VGG_output_h_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8129d-4580-4ca3-81f1-61f99b8028e5",
   "metadata": {},
   "source": [
    "可以先生成随机的样本数据(batchsize, nChannels, Height, Width)，逐个循环每一模块，即Sequential对象，获取每一模块计算后数据的形状，从而观察、验证并用于辅助调整模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6310ab9f-4447-4fb4-aa0c-fc755f163c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])\n",
      "fc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(1, 1, 224, 224)\n",
    "for name, blk in VGG_net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79304081-732c-4585-9fe6-7e9497ef44c4",
   "metadata": {},
   "source": [
    "VGG的原始输入图像尺寸为224，目前实验的数据为fashionMNIST数据集，一幅图像的尺寸为$28 \\times 28$，因此使用`torchvision.transforms.Resize`的方法调整图像大小，该方法已经包含于自定义`load_fashionMNIST`中，因此只需要配置输入参数`resize=224`就可以修改图像尺寸，满足VGG输入数据尺寸的要求。\n",
    "\n",
    "因为卷积后的图像输出尺寸只与图像的输入尺寸，卷积层、池化层的卷积核大小，填充、步幅等有关，因此可以自由的修改卷积层的输入、输出通道，以及全连接层隐含层的数量。在修改时只需要配置一个缩放比例参数`ratio`，将所有的相关值除以该值完成新的配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43ebffb1-7c1e-4f3d-8277-937a84fd6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG网络结构_减少通道数：\n",
      " Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): flattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\n",
    "VGG_net_= vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\n",
    "print(\"VGG网络结构_减少通道数：\\n\",VGG_net_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8c3382-e8e1-46f5-a7b4-e501e20a15b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richi\\anaconda3\\envs\\USDA_database\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\richi\\anaconda3\\envs\\USDA_database\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状： torch.Size([64, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "trainloader,testloader=load_fashionMNIST(root='./dataset/FashionMNIST_norm',batchsize=batch_size,num_workers=2,resize=224,n_mean=0.5,n_std=0.5)\n",
    "\n",
    "def dataiter_view(dataiter):\n",
    "    '''\n",
    "    function - 查看可迭代数据形状\n",
    "    '''\n",
    "    dataiter_=iter(dataiter)\n",
    "    images_,labels_=dataiter_.next()\n",
    "    print('数据形状：',images_.shape)\n",
    "    return images_,labels_\n",
    "images_,labels_=dataiter_view(testloader)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e53f7f6-5a0b-4507-b730-09063acb95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(VGG_net_.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c4a009b-a87b-489e-a3fa-5423e248d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_V2(data_iter, net, device=None):\n",
    "    '''\n",
    "    function - 模型精度计算\n",
    "    '''    \n",
    "    if device is None and isinstance(net,torch.nn.Module):\n",
    "        device=list(net.parameters())[0].device #如果没指定device就使用net的device\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                net.eval() #评估模式，会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() #改回训练模式\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "    \n",
    "def train_v2(net, train_iter, test_iter,optimizer, device, num_epochs):\n",
    "    from tqdm.auto import tqdm\n",
    "    import time\n",
    "    '''\n",
    "    function - 训练模型，v2版\n",
    "    '''\n",
    "    net = net.to(device)\n",
    "    print(\"training on-\",device)\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_pred=net(X)\n",
    "            l=loss(y_pred,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc=evaluate_accuracy_V2(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'% (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f4187ab1-bc60-4da6-8926-aae530e7365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on- cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16172b0c2504358a287efc1ceae6463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.6070, train acc 0.775, test acc 0.874, time 63.3 sec\n",
      "epoch 2, loss 0.3222, train acc 0.886, test acc 0.904, time 67.9 sec\n",
      "epoch 3, loss 0.2749, train acc 0.902, test acc 0.905, time 82.1 sec\n",
      "epoch 4, loss 0.2418, train acc 0.914, test acc 0.912, time 98.6 sec\n",
      "epoch 5, loss 0.2180, train acc 0.922, test acc 0.915, time 107.7 sec\n"
     ]
    }
   ],
   "source": [
    "train_v2(net=VGG_net_, train_iter=trainloader, test_iter=testloader,optimizer=optimizer, device=device, num_epochs=num_epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f14c2-3a31-46e3-9571-aa0d88393116",
   "metadata": {},
   "source": [
    "#### 2）torchvision.models实现VGG网络\n",
    "\n",
    "VGG网络包含于`torchvision.models`库中， 因此无需自行配置网络，直接下载使用。通常模型也包含预先训练的模型参数，可以配置`pretrained=True`下载，下载到本地的位置为`C:\\Users\\<your name>\\.cache\\torch\\hub\\checkpoints`，后缀名为.pth。有时直接下载的网络并不能直接应用到其它不同的数据集，例如fashionMNIST数据集，该数据集的图像为灰色，即只有一个通道；同时，只有10个标签。因此需要对应层修改输入、输出大小。在[Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)中，包含了模型参数微调的方法，对于VGG而言，修改卷积层，可以通过`.features[idx]`的方式读取；修改全连接层，可以通过`.classifier[idx]`的方式修改。需要修改的层为features[0]和classifier[6]这两个层。其它层不需修改。\n",
    "\n",
    "注意在训练模型之前，需要调整优化函数`optimizer=torch.optim.Adam(VGG_model.parameters(),lr=lr)`的输入模型参数为当前的网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d38976-39da-4c7a-8e6b-0de2cd25df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "VGG_model=torchvision.models.vgg11(pretrained=False) #如果为真，返回在ImageNet上预先训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1681a7bc-dc02-4fcf-9d2b-daa45449e309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(VGG_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a282361-e03d-400c-9cac-324bd6de3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch  import nn\n",
    "VGG_model.features[0]=nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "VGG_model.classifier[6]=nn.Linear(in_features=4096, out_features=10, bias=True)\n",
    "print(VGG_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23407ce3-418d-4e06-9fc7-9fe7df19a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on- cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5386d0121d4b37aebe33603b15afd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.6091, train acc 0.814, test acc 0.876, time 944.2 sec\n",
      "epoch 2, loss 0.2923, train acc 0.893, test acc 0.895, time 1089.5 sec\n",
      "epoch 3, loss 0.2509, train acc 0.910, test acc 0.908, time 1063.9 sec\n",
      "epoch 4, loss 0.2229, train acc 0.919, test acc 0.913, time 1047.5 sec\n",
      "epoch 5, loss 0.2006, train acc 0.926, test acc 0.919, time 1089.6 sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(VGG_model.parameters(),lr=lr)\n",
    "train_v2(net=VGG_model, train_iter=trainloader, test_iter=testloader,optimizer=optimizer, device=device, num_epochs=num_epochs)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
