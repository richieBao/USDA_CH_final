> Created on Jan 14 2019 @author: Richie Bao-caDesign设计(cadesign.cn)__+updated on Sun Nov 29 23:34:53 2020 by Richie Bao +updated on Sat Dec 18 10:57:16 2021 by Richie Bao

## 1.3 数据库与数据分析基本流程组织

当数据量开始膨胀，常规数据存储方式的简单文件形式，虽然逻辑简单，但可扩展性差，不能解决数据的完整性、一致性以及安全性等一系列问题，由此产生了数据管理系统（Database Management System,DBMS），即数据库（database）。数据库是按照一定规则保存数据，能给予多个用户共享，具有尽可能小的冗余度，与应用程序彼此独立，并能通过查询取回程序所需数据的数据存储和管理方式。数据库有多种类型，例如与分布处理技术结合产生的分布式数据库，与并行处理技术结合产生的并行数据库，特定领域的地理数据库、空间数据库等。Web最常使用基于关系模型的数据库，即关系型数据库（Relational Database Management System,RDBMS），或称为SQL(Structured Query Language)数据库，使用结构化查询语言操作。与之相反的是最近流行的文档数据库和键-值对数据库，即NoSQL数据库。其中关系型数据库把数据存储在表中，表的列（colunm）为字段（field），每一字段为“样本”的一个属性，行（row）为每一“样本”数据，包含一个或多个属性。常用的关系型数据库有MySQL（其替代品包括MariaDB等），以及SQLite和PostgreSQL。

在城市空间数据分析方法研究中，主要使用[SQLite](https://www.sqlite.org/index.html)和[PostgreSQL](https://www.postgresql.org/)两个数据库。当涉及到地理空间信息数据，需要配置投影坐标系统以及在[QGIS](https://www.qgis.org/en/site/)中读取地理信息建立地图时，使用PostgreSQL；否则，一般使用轻量型的数据库SQLite。

### 1.3.1 SQLite数据库

[SQLite](https://www.sqlite.org/index.html)是一个C语言库（SQL数据库引擎），小型、快速、自包含(self-contained)、高可靠性，功能齐全。已有超过1万亿(1e12)SQLite数据库在活跃的使用。其文档格式稳定、跨平台，向后兼容，同时其开发人员保证到2050年一直保持这种格式。

对SQLite关系型数据库的操作，包含通过SQLite命令执行(SQL语句)，通过python等语言执行(大多数数据库引擎都有对应的python包)。对于python，使用两个库，一个是[sqlite3](https://docs.python.org/3/library/sqlite3.html)操作SQLite数据库的库，另一是[SQLAlchemy(flask_sqlalchemy)库](https://www.sqlalchemy.org/)（数据库抽象层代码包，可以直接处理高等级的python对象，而不用关注表、文档或查询语言等数据库实体）。当然pandas等库也提供了直接读写数据库对应的方法，进一步简化了对数据库的操作。

SQLite数据库应用途径，引用*《漫画数据库》*中的数据，结合代码实现阐释。同时使用[DB Browser for SQLite(DB4S)](https://sqlitebrowser.org/)辅助查看、管理SQLite数据库。

> 参考文献
> 1.  高桥麻奈著,崔建锋译.株式会社TREND-PRO漫画制作.漫画数据库[M].科学出版社.北京,2010.5.
2. Miguel Grinberg.Flask Web Development: Developing Web Applications with Python[M]. O'Reilly Media; 2nd edition.April 3, 2018. （中文版：Miguel Grinberg.安道译.Flask Web开发：基于Python的Web应用开发实战[M].人民邮电出版社,2018.）

#### 1） 查看版本


```python
%%cmd
sqlite3 version
```

    Microsoft Windows [Version 10.0.19042.1237]
    (c) Microsoft Corporation. All rights reserved.
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>sqlite3 version
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>


```python
import sqlalchemy
sqlalchemy.__version__
```




    '1.3.19'



#### 2）根据*漫画数据库*中的销售数据集录入数据


```python
import pandas as pd
from datetime import datetime

#定义字典类型的假设数据
sales_dic={'idx':[1101,1102,1103,1104,1105],
           'date':[datetime(2020,3,5),datetime(2020,3,7),datetime(2020,3,8),datetime(2020,3,10),datetime(2020,3,12)],
           "exporting_country_ID":[12,23,25,12,25]}
exporting_country_dic={"exporting_country_ID":[12,23,25],
                       'exporting_country_name':['kenya','brazil','peru']}
sale_details_dic={'idx':[1101,1101,1102,1103,1104,1105,1105],
                  'commodity_code':[101,102,103,104,101,103,104],
                  'number':[1100,300,1700,500,2500,2000,700]}
commodity_dic={'commodity_code':[101,102,103,104],
               'commodity_name':['muskmelon','strawberry','apple','lemon']}

#为方便数据管理，将字典格式数据转换为Pandas的DataFrame格式
sales_table=pd.DataFrame.from_dict(sales_dic)
exporting_country_table=pd.DataFrame.from_dict(exporting_country_dic)
sale_details_table=pd.DataFrame.from_dict(sale_details_dic)
commodity_table=pd.DataFrame.from_dict(commodity_dic)
```

#### 3）创建数据库（链接）
在当前目录下创建数据库，使用`engine=create_engine('sqlite:///x.sqlite')`语句；相对或绝对路径创建数据库，例如`engine=create_engine('sqlite:///./data/fruits.sqlite'）`或`engine=create_engine('sqlite:///absolute/data/fruits.sqlite'）`；如果创建内存数据库，格式如下`engine=create_engine('sqlite://')`或`engine=create_engine('sqlite:///:memory:', echo=True)`。Unix、Max及Window系统的文件路径分隔符可能不同，如果出现异常，可以尝试在/或\切换，同时注意\也是转义符号，因此可能需要写成\\\\。


```python
from sqlalchemy import create_engine

db_fp=r'./database/fruits.sqlite'
engine=create_engine('sqlite:///'+'\\\\'.join(db_fp.split('\\')),echo=True) 
```

执行`create_engine`语句，只是建立了数据库链接，只有向其写入表数据(或者对数据库执行任务，例如`engine.connect()`)，才会在硬盘指定路径下找到该文件。如果存在同名数据库，重复执行此语句，只是实现数据库链接


```python
connection=engine.connect()
connection.close()
```

    2021-12-18 16:31:11,203 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 16:31:11,204 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 16:31:11,204 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 16:31:11,205 INFO sqlalchemy.engine.base.Engine ()
    

#### 4) 向数据库中写入表及数据

##### 1. 写入方法-pandas.DataFrame.to_sql 

其中参数'if_exists'，可以选择'fail'-为默认值，如果表存在，则返回异常；'replace'-先删除已经存在的表，再重新插入新表；'append'-向存在的表中追加行。

pandas方法，不需要建立模型（表结构），数据库模型的表述方法通常易于与机器学习的模型，或者算法模型的说法混淆，因此为了便于区分，这里用表结构代替模型的说法。pandas可以根据DataFrame格式数据信息，尤其包含有自动生成的数据类型，直接在数据库中建立对应数据格式的表，不需要自行预先定义表结构。但是在应用程序中调入表中数据时，又往往需要调用表结构来读取数据库信息，例如Flask Web框架（参看Flask部分阐述）等。因此可以用DB4S来查看刚刚建立的SQLite数据库及写入的表，可以看到表结构，根据表结构的数据类型信息，再手工建立表结构。表结构通常以类(class)的形式定义。但是手工定义相对比较繁琐，尤其字段比较多，不容易确定数据类型时，可以使用数据库逆向工程的方法，例如使用sqlacodegen库生成数据库表结构。


```python
def df2SQLite(db_fp,df,table,method='fail'):
    from sqlalchemy import create_engine
    
    '''
    function - pandas方法，把DataFrame格式数据写入数据库（同时创建表）
    
    Paras:
        engine - 数据库链接
        df - 待写入数据库的DataFrame格式数据
        table - 表名称
        method - 写入方法，'fail'，'replace'或'append'
    '''
    engine=create_engine('sqlite:///'+'\\\\'.join(db_fp.split('\\')),echo=True) 
    try:    
        df.to_sql(table,con=engine,if_exists="%s"%method)
        if method=='replace':            
            print("_"*10,'the %s table has been overwritten...'%table)                  
        elif method=='append':
            print("_"*10,'the %s table has been appended...'%table)
        else:
            print("_"*10,'the %s table has been written......'%table)
    except:
        print("_"*10,'the %s table has been existed......'%table)
method='fail'  
table='sales'
df=sales_table
df2SQLite(db_fp,df,table,method)
```

    2021-12-18 22:25:30,687 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 22:25:30,689 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,691 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 22:25:30,692 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,693 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("sales")
    2021-12-18 22:25:30,694 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,695 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("sales")
    2021-12-18 22:25:30,696 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,698 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE sales (
    	"index" BIGINT, 
    	idx BIGINT, 
    	date DATETIME, 
    	"exporting_country_ID" BIGINT
    )
    
    
    2021-12-18 22:25:30,699 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,708 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 22:25:30,709 INFO sqlalchemy.engine.base.Engine CREATE INDEX ix_sales_index ON sales ("index")
    2021-12-18 22:25:30,710 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 22:25:30,717 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 22:25:30,719 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 22:25:30,721 INFO sqlalchemy.engine.base.Engine INSERT INTO sales ("index", idx, date, "exporting_country_ID") VALUES (?, ?, ?, ?)
    2021-12-18 22:25:30,721 INFO sqlalchemy.engine.base.Engine ((0, 1101, '2020-03-05 00:00:00.000000', 12), (1, 1102, '2020-03-07 00:00:00.000000', 23), (2, 1103, '2020-03-08 00:00:00.000000', 25), (3, 1104, '2020-03-10 00:00:00.000000', 12), (4, 1105, '2020-03-12 00:00:00.000000', 25))
    2021-12-18 22:25:30,723 INFO sqlalchemy.engine.base.Engine COMMIT
    __________ the sales table has been written......
    

'sales'表已经写入数据库，如果配置`method='fail'`，再次写入时，则返回异常，即提示表已经存在。


```python
df2SQLite(engine,df,table,method='fail')
```

    2021-12-18 16:46:07,185 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("sales")
    2021-12-18 16:46:07,186 INFO sqlalchemy.engine.base.Engine ()
    __________ the sales table has been existed......
    

* 由sqlacodegen库生成SQLite数据库中'sales'表结构。对于sqlacodegen方法可以在命令行中输入sqlacodegen --help查看。生成的表结构写入到指定的文件中（下述代码写入到了sales_table_structure.py文件下）。

> sqlacodegen库的安装通常使用`pip install flask-sqlacodegen` 途径


```python
%%cmd
sqlacodegen sqlite:///./database/fruits.sqlite --tables sales --outfile sales_table_structure.py
```

    Microsoft Windows [Version 10.0.19042.1237]
    (c) Microsoft Corporation. All rights reserved.
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>sqlacodegen sqlite:///./database/fruits.sqlite --tables sales --outfile sales_table_structure.py
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>

打开存储有表结构的sales_table_structure.py文件，内容如下：


```python
# coding: utf-8
from sqlalchemy import BigInteger, Column, DateTime, MetaData, Table
from sqlalchemy.ext.declarative import declarative_base
metadata = MetaData()
t_sales = Table(
    'sales', metadata,
    Column('index', BigInteger, index=True),
    Column('idx', BigInteger),
    Column('date', DateTime),
    Column('exporting_country_ID', BigInteger)
)
```

##### 2.写入方法-sqlalchemy创建表结构，及写入表

定义的表结构需要继承`declarative_base()`映射类。完成表结构的定义后，执行`BASE.metadata.create_all(engine, checkfirst=True)`写入表结构，注意此时并未写入数据。


```python
from sqlalchemy.ext.declarative import declarative_base
import sqlalchemy as db

BASE=declarative_base() #基本映射类，需要自定义的表结构继承

class exporting_country(BASE):
    __tablename__='exporting_country'     
    
    index=db.Column(db.Integer, primary_key=True, autoincrement=True) #自动生成的索引列
    exporting_country_ID=db.Column(db.Integer)
    exporting_country_name=db.Column(db.Text)
    
    def __repr__(self): #用于表结构打印时输出的字符串，亦可以不用写。
        return '<exporting_country %r>'%self.exporting_country_ID 
exporting_country.__table__ #查看表结构
```




    Table('exporting_country', MetaData(bind=None), Column('index', Integer(), table=<exporting_country>, primary_key=True, nullable=False), Column('exporting_country_ID', Integer(), table=<exporting_country>), Column('exporting_country_name', Text(), table=<exporting_country>), schema=None)




```python
BASE.metadata.create_all(engine, checkfirst=True) #checkfirst=True，检查该表是否存在，如果存在则不建立，默认为True。可以增加tables=[Base.metadata.tables['exporting_country']]参数指定创建哪些表，或者直接使用exporting_country.__table__.create(engine, checkfirst=True)方法
```

    2021-12-18 19:19:19,986 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 19:19:19,987 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:19:19,988 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 19:19:19,990 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:19:19,991 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("exporting_country")
    2021-12-18 19:19:19,992 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:19:19,994 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("exporting_country")
    2021-12-18 19:19:19,995 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:19:19,996 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE exporting_country (
    	"index" INTEGER NOT NULL, 
    	"exporting_country_ID" INTEGER, 
    	exporting_country_name TEXT, 
    	PRIMARY KEY ("index")
    )
    
    
    2021-12-18 19:19:19,997 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:19:20,004 INFO sqlalchemy.engine.base.Engine COMMIT
    

将数据写入到新定义的表中。使用`session.add_all`方法可以一次性写入多组数据，但是需要将其转换为对应的格式。


```python
from sqlalchemy.orm import sessionmaker
SESSION=sessionmaker(bind=engine) #建立会话链接
session=SESSION() #实例化

def zip_dic_tableSQLite(dic,table_model):
    '''
    function - 按字典的键，成对匹配，返回用于写入SQLite数据库的列表
    
    Paras:
    dic - 字典格式数据
    table_model - 表结构（模型）。数据将写入到该表中
    '''
    keys=list(dic.keys())
    vals=dic.values()
    vals_zip=list(zip(*list(vals)))
    #[{k:i for k,i in zip(keys, v)} for v in vals_zip]     
    return [table_model(**{k:i for k,i in zip(keys, v)}) for v in vals_zip]

exporting_country_table_model=zip_dic_tableSQLite(exporting_country_dic,exporting_country)
session.add_all(exporting_country_table_model)
session.commit()
```

    2021-12-18 19:24:57,236 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 19:24:57,237 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country ("exporting_country_ID", exporting_country_name) VALUES (?, ?)
    2021-12-18 19:24:57,239 INFO sqlalchemy.engine.base.Engine (12, 'kenya')
    2021-12-18 19:24:57,242 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country ("exporting_country_ID", exporting_country_name) VALUES (?, ?)
    2021-12-18 19:24:57,243 INFO sqlalchemy.engine.base.Engine (23, 'brazil')
    2021-12-18 19:24:57,244 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country ("exporting_country_ID", exporting_country_name) VALUES (?, ?)
    2021-12-18 19:24:57,245 INFO sqlalchemy.engine.base.Engine (25, 'peru')
    2021-12-18 19:24:57,246 INFO sqlalchemy.engine.base.Engine COMMIT
    

用pandas写入SQLite数据库的方法，将剩下的两组数据写入。同时应用sqlacodegen库生成对应的数据库表结构。


```python
df2SQLite(engine,sale_details_table,table='sale_details')
df2SQLite(engine,commodity_table,table='commodity')
```

    2021-12-18 19:31:24,038 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("sale_details")
    2021-12-18 19:31:24,039 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,042 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("sale_details")
    2021-12-18 19:31:24,043 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,048 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE sale_details (
    	"index" BIGINT, 
    	idx BIGINT, 
    	commodity_code BIGINT, 
    	number BIGINT
    )
    
    
    2021-12-18 19:31:24,049 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,057 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 19:31:24,059 INFO sqlalchemy.engine.base.Engine CREATE INDEX ix_sale_details_index ON sale_details ("index")
    2021-12-18 19:31:24,060 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,067 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 19:31:24,071 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 19:31:24,072 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details ("index", idx, commodity_code, number) VALUES (?, ?, ?, ?)
    2021-12-18 19:31:24,073 INFO sqlalchemy.engine.base.Engine ((0, 1101, 101, 1100), (1, 1101, 102, 300), (2, 1102, 103, 1700), (3, 1103, 104, 500), (4, 1104, 101, 2500), (5, 1105, 103, 2000), (6, 1105, 104, 700))
    2021-12-18 19:31:24,075 INFO sqlalchemy.engine.base.Engine COMMIT
    __________ the sale_details table has been written......
    2021-12-18 19:31:24,083 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("commodity")
    2021-12-18 19:31:24,083 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,085 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("commodity")
    2021-12-18 19:31:24,085 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,088 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE commodity (
    	"index" BIGINT, 
    	commodity_code BIGINT, 
    	commodity_name TEXT
    )
    
    
    2021-12-18 19:31:24,089 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,096 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 19:31:24,097 INFO sqlalchemy.engine.base.Engine CREATE INDEX ix_commodity_index ON commodity ("index")
    2021-12-18 19:31:24,098 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 19:31:24,106 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 19:31:24,109 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 19:31:24,110 INFO sqlalchemy.engine.base.Engine INSERT INTO commodity ("index", commodity_code, commodity_name) VALUES (?, ?, ?)
    2021-12-18 19:31:24,112 INFO sqlalchemy.engine.base.Engine ((0, 101, 'muskmelon'), (1, 102, 'strawberry'), (2, 103, 'apple'), (3, 104, 'lemon'))
    2021-12-18 19:31:24,115 INFO sqlalchemy.engine.base.Engine COMMIT
    __________ the commodity table has been written......
    

使用sqlacodegen库分别生成对应的数据库表结构。


```python
%%cmd
sqlacodegen sqlite:///./database/fruits.sqlite --tables sale_details --outfile sale_details_table_structure.py
sqlacodegen sqlite:///./database/fruits.sqlite --tables commodity --outfile commodity_table_structure.py
```

    Microsoft Windows [Version 10.0.19042.1237]
    (c) Microsoft Corporation. All rights reserved.
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>sqlacodegen sqlite:///./database/fruits.sqlite --tables sale_details --outfile sale_details_table_structure.py
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>sqlacodegen sqlite:///./database/fruits.sqlite --tables commodity --outfile commodity_table_structure.py
    
    (USDA_database) C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook>

打开存储有表结构的sale_details_table_structure.py文件，内容如下：


```python
# coding: utf-8
from sqlalchemy import BigInteger, Column, MetaData, Table
from sqlalchemy.ext.declarative import declarative_base
metadata = MetaData()
t_sale_details = Table(
    'sale_details', metadata,
    Column('index', BigInteger, index=True),
    Column('idx', BigInteger),
    Column('commodity_code', BigInteger),
    Column('number', BigInteger)
)
```

打开存储有表结构的commodity_table_structure.py文件，内容如下：


```python
# coding: utf-8
from sqlalchemy import BigInteger, Column, MetaData, Table, Text
from sqlalchemy.ext.declarative import declarative_base
metadata = MetaData()
t_commodity = Table(
    'commodity', metadata,
    Column('index', BigInteger, index=True),
    Column('commodity_code', BigInteger),
    Column('commodity_name', Text)
)
```

#### 5) 查询、增删和修改数据库

##### 1. 查询数据库

* 使用`session.query()`方法

使用类定义的表结构，在应用`session.query()`读取数据库时返回的是一个对象'<exporting_country 12>'，需要给定字段读取具体的值。

读取的方法有多种，可以自行搜索查询。


```python
exporting_country_query=session.query(exporting_country).filter_by(exporting_country_ID=12).first() #.all()将读取所有匹配，.first()仅返回首个匹配对象

print("_"*50)
print(exporting_country_query)
print(exporting_country_query.exporting_country_name,exporting_country_query.exporting_country_ID)
```

    2021-12-18 19:42:21,546 INFO sqlalchemy.engine.base.Engine SELECT exporting_country."index" AS exporting_country_index, exporting_country."exporting_country_ID" AS "exporting_country_exporting_country_ID", exporting_country.exporting_country_name AS exporting_country_exporting_country_name 
    FROM exporting_country 
    WHERE exporting_country."exporting_country_ID" = ?
     LIMIT ? OFFSET ?
    2021-12-18 19:42:21,547 INFO sqlalchemy.engine.base.Engine (12, 1, 0)
    __________________________________________________
    <exporting_country 12>
    kenya 12
    

使用sqlacodegen库生成数据库表结构，是使用`sqlalchemy.Table`定义。在应用`session.query()`读取数据库时返回的是一个元组，顺序包含所有字段的值。


```python
# coding: utf-8
from sqlalchemy import BigInteger, Column, DateTime, MetaData, Table

t_sales=Table(
    'sales', metadata,
    Column('index', BigInteger, index=True),
    Column('idx', BigInteger),
    Column('date', DateTime),
    Column('exporting_country_ID', BigInteger)
)
sales_query=session.query(t_sales).filter_by(idx=1101).first()
print("_"*50)
print(sales_query)
```

    2021-12-18 19:43:35,629 INFO sqlalchemy.engine.base.Engine SELECT sales."index" AS sales_index, sales.idx AS sales_idx, sales.date AS sales_date, sales."exporting_country_ID" AS "sales_exporting_country_ID" 
    FROM sales 
    WHERE sales.idx = ?
     LIMIT ? OFFSET ?
    2021-12-18 19:43:35,630 INFO sqlalchemy.engine.base.Engine (1101, 1, 0)
    __________________________________________________
    (0, 1101, datetime.datetime(2020, 3, 5, 0, 0), 12)
    

* 使用pandas库提供的方法 

应用pandas读取数据库相对sqlite3和SQLAlchemy库而言，较为简单，不需要配置表结构，能直接读取。


```python
import sqlite3
import pandas as pd
db_fp=r'./database/fruits.sqlite'
conn=sqlite3.connect(db_fp)
df_sqlite=pd.read_sql('select * from sqlite_master',con=conn) #pd.read_sql将读取数据库结构(database structure)信息
df_sqlite
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>type</th>
      <th>name</th>
      <th>tbl_name</th>
      <th>rootpage</th>
      <th>sql</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>table</td>
      <td>sales</td>
      <td>sales</td>
      <td>2</td>
      <td>CREATE TABLE sales (\n\t"index" BIGINT, \n\tid...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>index</td>
      <td>ix_sales_index</td>
      <td>sales</td>
      <td>3</td>
      <td>CREATE INDEX ix_sales_index ON sales ("index")</td>
    </tr>
    <tr>
      <th>2</th>
      <td>table</td>
      <td>exporting_country</td>
      <td>exporting_country</td>
      <td>4</td>
      <td>CREATE TABLE exporting_country (\n\t"index" IN...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>table</td>
      <td>sale_details</td>
      <td>sale_details</td>
      <td>5</td>
      <td>CREATE TABLE sale_details (\n\t"index" BIGINT,...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>index</td>
      <td>ix_sale_details_index</td>
      <td>sale_details</td>
      <td>6</td>
      <td>CREATE INDEX ix_sale_details_index ON sale_det...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>table</td>
      <td>commodity</td>
      <td>commodity</td>
      <td>7</td>
      <td>CREATE TABLE commodity (\n\t"index" BIGINT, \n...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>index</td>
      <td>ix_commodity_index</td>
      <td>commodity</td>
      <td>8</td>
      <td>CREATE INDEX ix_commodity_index ON commodity (...</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_sales=pd.read_sql_table('sales', 'sqlite:///./database/fruits.sqlite') #pd.read_sql_table从数据库中读取指定的表
df_sales
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>idx</th>
      <th>date</th>
      <th>exporting_country_ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1101</td>
      <td>2020-03-05</td>
      <td>12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1102</td>
      <td>2020-03-07</td>
      <td>23</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1103</td>
      <td>2020-03-08</td>
      <td>25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1104</td>
      <td>2020-03-10</td>
      <td>12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>1105</td>
      <td>2020-03-12</td>
      <td>25</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_sales_query=pd.read_sql_query('select idx,exporting_country_ID from sales', con=conn) #pd.read_sql_query将根据SQL query 或 SQLAlchemy Selectable查询语句读取特定的值 
df_sales_query
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>idx</th>
      <th>exporting_country_ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1101</td>
      <td>12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1102</td>
      <td>23</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1103</td>
      <td>25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1104</td>
      <td>12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1105</td>
      <td>25</td>
    </tr>
  </tbody>
</table>
</div>



##### 2.增-删数据库

* 向已有表中增加数据

sqlacodegen库生成数据库表结构并运行，'sales'表则被存储于metadata元数据中。如果再定义一个类，同样指向这个表，则需要配置`'extend_existing': True`，表示在已有列基础上进行扩展，即sqlalchemy允许类是表的子集（一个表可以指向多个表结构的类）。


```python
metadata.tables
```




    immutabledict({'commodity': Table('commodity', MetaData(bind=None), Column('index', BigInteger(), table=<commodity>), Column('commodity_code', BigInteger(), table=<commodity>), Column('commodity_name', Text(), table=<commodity>), schema=None), 'sales': Table('sales', MetaData(bind=None), Column('index', BigInteger(), table=<sales>), Column('idx', BigInteger(), table=<sales>), Column('date', DateTime(), table=<sales>), Column('exporting_country_ID', BigInteger(), table=<sales>), schema=None)})




```python
class sales(BASE):
    __tablename__='sales'     
    __table_args__={'extend_existing': True} 
    
    index=db.Column(db.Integer, primary_key=True, autoincrement=True) #因为该sales类是在执行t_sales之后定义，只能是在原有表上扩展，无法修改原表结构属性，因此index字段并不会实现自动增加的属性。需要手动增加index字段值
    idx=db.Column(db.Integer)
    date=db.Column(db.DateTime)
    exporting_country_ID=db.Column(db.Integer)
    
from sqlalchemy.orm import sessionmaker
SESSION=sessionmaker(bind=engine) #建立会话链接
session=SESSION() #实例化    
    
new_sale=sales(index=5,idx=1106,date=datetime(2020,12,18),exporting_country_ID=25)
session.add(new_sale)
session.commit()
```

    2021-12-18 19:57:40,321 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 19:57:40,322 INFO sqlalchemy.engine.base.Engine INSERT INTO sales ("index", idx, date, "exporting_country_ID") VALUES (?, ?, ?, ?)
    2021-12-18 19:57:40,324 INFO sqlalchemy.engine.base.Engine (5, 1106, '2020-12-18 00:00:00.000000', 25)
    2021-12-18 19:57:40,328 INFO sqlalchemy.engine.base.Engine COMMIT
    

从表中读取新增加的数据


```python
del_sale=session.query(sales).filter_by(idx=1106).first() #如果该行中有值为空，例如在增加该行数据时未定义写入index=5字段，该语句返回值会为空。如允许出现空值，在定义表结构时需要配置nullabley=True
print("_"*50)
print(del_sale.exporting_country_ID,del_sale.date)
```

    2021-12-18 20:01:09,814 INFO sqlalchemy.engine.base.Engine SELECT sales."index" AS sales_index, sales.idx AS sales_idx, sales.date AS sales_date, sales."exporting_country_ID" AS "sales_exporting_country_ID" 
    FROM sales 
    WHERE sales.idx = ?
     LIMIT ? OFFSET ?
    2021-12-18 20:01:09,816 INFO sqlalchemy.engine.base.Engine (1106, 1, 0)
    __________________________________________________
    25 2020-12-18 00:00:00
    

* 从表中删除已有的数据


```python
session.delete(del_sale)
session.commit()
```

    2021-12-18 20:05:34,748 INFO sqlalchemy.engine.base.Engine DELETE FROM sales WHERE sales."index" = ?
    2021-12-18 20:05:34,749 INFO sqlalchemy.engine.base.Engine (5,)
    2021-12-18 20:05:34,751 INFO sqlalchemy.engine.base.Engine COMMIT
    

##### 3.修改数据库


```python
mod_sale=session.query(sales).filter_by(idx=1105).first()
mod_sale.exporting_country_ID=23 #修改字段值
session.commit()
```

    2021-12-18 20:06:31,332 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 20:06:31,334 INFO sqlalchemy.engine.base.Engine SELECT sales."index" AS sales_index, sales.idx AS sales_idx, sales.date AS sales_date, sales."exporting_country_ID" AS "sales_exporting_country_ID" 
    FROM sales 
    WHERE sales.idx = ?
     LIMIT ? OFFSET ?
    2021-12-18 20:06:31,334 INFO sqlalchemy.engine.base.Engine (1105, 1, 0)
    2021-12-18 20:06:31,336 INFO sqlalchemy.engine.base.Engine UPDATE sales SET "exporting_country_ID"=? WHERE sales."index" = ?
    2021-12-18 20:06:31,336 INFO sqlalchemy.engine.base.Engine (23, 4)
    2021-12-18 20:06:31,338 INFO sqlalchemy.engine.base.Engine COMMIT
    

### 1.3.2 SQLite数据库的表间关系(多表关联)

上文建立表结构时，并未配置表间关联，各个表是独立的，如果想通过一个表的数据字段查询另一个表的字段内容就比较困难，例如想根据销售数量，查询对应的商品名称时，是无法直接在'commodity'商品表中直接查询商品名称的，需要先根据待查询的销售数量例如300，在'sale_details'销售明细表里找到对应的commodity_code商品编码为102，根据这个商品编码，再在'commodity'商品表找到对应的商品名称为'strawberry'。因为这个过程很繁琐，尤其数据库结构和表结构进一步复杂时，这个问题会更凸显，因此需要建立表间的联系。

SQLite的表间关系配置，可以包括1对多，多对1，1对1和多对多，SQLAlchemy给出表结构配置的方法[Relationship Configuration](https://docs.sqlalchemy.org/en/14/orm/relationships.html)，可以根据其阐述进行配置。在配置时，参数'back_populates'定义反向引用，用于建立双向关系，例如销售明细表->商品表，均包括`relationship()`语句，显示定义关系属性。如果使用参数'backref'添加反向引用，会自动在另一侧建立关系属性，为'back_populates'的简化形式。参数`uselist=True`（默认值）时，为1对多关系，如果配置1对1时，需要将其配置为`uselist=False`。在配置表关系时，为了能够清晰易读，通常以表名作为变量名，例如销售明细表->商品表，销售明细表为父表(parent)，商品表为子表(child)，父表中语句`commodity=relationship('commodity',uselist=False, back_populates="sale_details") `以子表为变量名，而子表中语句`sale_details=relationship('sale_details',back_populates="commodity")`以父表为变量名，这样可以更清晰的表述表之间的关系。

可以使用sqlacodegen库生成数据库表结构，往往应用于类似pandas写入数据库而没有定义表结构的情况下，这是一种逆向工程。下述已经定义4个表结构，那么则可以使用逆向工程反馈表的内容和表之间的关系，例如使用[Visual Paradigm](https://www.visual-paradigm.com/)反馈有下表关系，即统一建模语言(Unified Modeling Language,UML)。可以清晰直观的读出表结构和表间关系，其中'sales'是'exporting_country'的父表，连接的关键字段（ForeignKey）是'exporting_country_ID'；同时'sales'是'sale_details'的子表，联系的关键字段是'idx'，其它的关系以此类推，一目了然。

<a href=""><img src="./imgs/1_3_01.png" height='auto' width=800 title="caDesign"></a>


```python
from sqlalchemy import create_engine

db_fp=r'./database/fruits_relational.sqlite'
engine=create_engine('sqlite:///'+'\\\\'.join(db_fp.split('\\')),echo=True) 
```


```python
from sqlalchemy.ext.declarative import declarative_base
import sqlalchemy as db
from sqlalchemy.orm import relationship
from sqlalchemy.schema import ForeignKey

BASE=declarative_base() #基本映射类，需要自定义的表结构继承

#销售明细表
class sale_details(BASE):
    __tablename__='sale_details'
    index=db.Column(db.Integer, primary_key=True, autoincrement=True) 
    commodity_code=db.Column(db.Integer)
    number=db.Column(db.Integer)    
    idx=db.Column(db.Integer)
    
    sales=relationship('sales',uselist=False, back_populates="sale_details")
    
    commodity=relationship('commodity',uselist=False, back_populates="sale_details")  

#销售表
class sales(BASE):
    __tablename__='sales'     
    
    index=db.Column(db.Integer, primary_key=True, autoincrement=True) #自动生成的索引列    
    date=db.Column(db.DateTime)
    exporting_country_ID=db.Column(db.Integer)
    
    exporting_country=relationship('exporting_country',uselist=False,back_populates="sales")
    
    idx=db.Column(db.Integer,ForeignKey('sale_details.idx'))
    sale_details=relationship('sale_details',back_populates="sales")    

#出口国表    
class exporting_country(BASE):
    __tablename__='exporting_country'     
    
    index=db.Column(db.Integer, primary_key=True, autoincrement=True) 
    exporting_country_name=db.Column(db.Text)    
    
    exporting_country_ID=db.Column(db.Integer,db.ForeignKey('sales.exporting_country_ID'))
    sales=relationship('sales',back_populates="exporting_country")  

#商品表
class commodity(BASE):
    __tablename__='commodity'
    index=db.Column(db.Integer, primary_key=True, autoincrement=True)     
    commodity_name=db.Column(db.Text)
    
    commodity_code=db.Column(db.Integer,ForeignKey('sale_details.commodity_code'))
    sale_details=relationship('sale_details',back_populates="commodity")
    
BASE.metadata.create_all(engine, checkfirst=True)#将所有表结构写入数据库
```


    2021-12-18 20:49:03,478 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 20:49:03,480 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,482 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1
    2021-12-18 20:49:03,483 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,484 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("sale_details")
    2021-12-18 20:49:03,486 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,488 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("sale_details")
    2021-12-18 20:49:03,489 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,490 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("sales")
    2021-12-18 20:49:03,490 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,491 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("sales")
    2021-12-18 20:49:03,491 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,492 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("exporting_country")
    2021-12-18 20:49:03,492 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,493 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("exporting_country")
    2021-12-18 20:49:03,493 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,494 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info("commodity")
    2021-12-18 20:49:03,494 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,494 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info("commodity")
    2021-12-18 20:49:03,495 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,496 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE sale_details (
    	"index" INTEGER NOT NULL, 
    	commodity_code INTEGER, 
    	number INTEGER, 
    	idx INTEGER, 
    	PRIMARY KEY ("index")
    )
    
    
    2021-12-18 20:49:03,496 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,505 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 20:49:03,507 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE sales (
    	"index" INTEGER NOT NULL, 
    	date DATETIME, 
    	"exporting_country_ID" INTEGER, 
    	idx INTEGER, 
    	PRIMARY KEY ("index"), 
    	FOREIGN KEY(idx) REFERENCES sale_details (idx)
    )
    
    
    2021-12-18 20:49:03,507 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,515 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 20:49:03,515 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE commodity (
    	"index" INTEGER NOT NULL, 
    	commodity_name TEXT, 
    	commodity_code INTEGER, 
    	PRIMARY KEY ("index"), 
    	FOREIGN KEY(commodity_code) REFERENCES sale_details (commodity_code)
    )
    
    
    2021-12-18 20:49:03,516 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,522 INFO sqlalchemy.engine.base.Engine COMMIT
    2021-12-18 20:49:03,523 INFO sqlalchemy.engine.base.Engine 
    CREATE TABLE exporting_country (
    	"index" INTEGER NOT NULL, 
    	exporting_country_name TEXT, 
    	"exporting_country_ID" INTEGER, 
    	PRIMARY KEY ("index"), 
    	FOREIGN KEY("exporting_country_ID") REFERENCES sales ("exporting_country_ID")
    )
    
    
    2021-12-18 20:49:03,523 INFO sqlalchemy.engine.base.Engine ()
    2021-12-18 20:49:03,529 INFO sqlalchemy.engine.base.Engine COMMIT
    

将数据写入各个表。


```python
from sqlalchemy.orm import sessionmaker

SESSION=sessionmaker(bind=engine) #建立会话链接
session=SESSION() #实例化

sales_=zip_dic_tableSQLite(sales_dic,sales)
exporting_country_=zip_dic_tableSQLite(exporting_country_dic,exporting_country)
sale_details_=zip_dic_tableSQLite(sale_details_dic,sale_details)
commodity_=zip_dic_tableSQLite(commodity_dic,commodity)

session.add_all(sales_)
session.add_all(exporting_country_)
session.add_all(sale_details_)
session.add_all(commodity_)
session.commit()
```

    2021-12-18 20:51:17,983 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 20:51:17,985 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,986 INFO sqlalchemy.engine.base.Engine (101, 1100, 1101)
    2021-12-18 20:51:17,988 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,989 INFO sqlalchemy.engine.base.Engine (102, 300, 1101)
    2021-12-18 20:51:17,990 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,991 INFO sqlalchemy.engine.base.Engine (103, 1700, 1102)
    2021-12-18 20:51:17,993 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,994 INFO sqlalchemy.engine.base.Engine (104, 500, 1103)
    2021-12-18 20:51:17,995 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,995 INFO sqlalchemy.engine.base.Engine (101, 2500, 1104)
    2021-12-18 20:51:17,996 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,998 INFO sqlalchemy.engine.base.Engine (103, 2000, 1105)
    2021-12-18 20:51:17,998 INFO sqlalchemy.engine.base.Engine INSERT INTO sale_details (commodity_code, number, idx) VALUES (?, ?, ?)
    2021-12-18 20:51:17,999 INFO sqlalchemy.engine.base.Engine (104, 700, 1105)
    2021-12-18 20:51:18,000 INFO sqlalchemy.engine.base.Engine INSERT INTO sales (date, "exporting_country_ID", idx) VALUES (?, ?, ?)
    2021-12-18 20:51:18,001 INFO sqlalchemy.engine.base.Engine ('2020-03-05 00:00:00.000000', 12, 1101)
    2021-12-18 20:51:18,002 INFO sqlalchemy.engine.base.Engine INSERT INTO sales (date, "exporting_country_ID", idx) VALUES (?, ?, ?)
    2021-12-18 20:51:18,002 INFO sqlalchemy.engine.base.Engine ('2020-03-07 00:00:00.000000', 23, 1102)
    2021-12-18 20:51:18,003 INFO sqlalchemy.engine.base.Engine INSERT INTO sales (date, "exporting_country_ID", idx) VALUES (?, ?, ?)
    2021-12-18 20:51:18,003 INFO sqlalchemy.engine.base.Engine ('2020-03-08 00:00:00.000000', 25, 1103)
    2021-12-18 20:51:18,004 INFO sqlalchemy.engine.base.Engine INSERT INTO sales (date, "exporting_country_ID", idx) VALUES (?, ?, ?)
    2021-12-18 20:51:18,004 INFO sqlalchemy.engine.base.Engine ('2020-03-10 00:00:00.000000', 12, 1104)
    2021-12-18 20:51:18,005 INFO sqlalchemy.engine.base.Engine INSERT INTO sales (date, "exporting_country_ID", idx) VALUES (?, ?, ?)
    2021-12-18 20:51:18,005 INFO sqlalchemy.engine.base.Engine ('2020-03-12 00:00:00.000000', 25, 1105)
    2021-12-18 20:51:18,006 INFO sqlalchemy.engine.base.Engine INSERT INTO commodity (commodity_name, commodity_code) VALUES (?, ?)
    2021-12-18 20:51:18,007 INFO sqlalchemy.engine.base.Engine ('muskmelon', 101)
    2021-12-18 20:51:18,009 INFO sqlalchemy.engine.base.Engine INSERT INTO commodity (commodity_name, commodity_code) VALUES (?, ?)
    2021-12-18 20:51:18,010 INFO sqlalchemy.engine.base.Engine ('strawberry', 102)
    2021-12-18 20:51:18,010 INFO sqlalchemy.engine.base.Engine INSERT INTO commodity (commodity_name, commodity_code) VALUES (?, ?)
    2021-12-18 20:51:18,011 INFO sqlalchemy.engine.base.Engine ('apple', 103)
    2021-12-18 20:51:18,011 INFO sqlalchemy.engine.base.Engine INSERT INTO commodity (commodity_name, commodity_code) VALUES (?, ?)
    2021-12-18 20:51:18,011 INFO sqlalchemy.engine.base.Engine ('lemon', 104)
    2021-12-18 20:51:18,012 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country (exporting_country_name, "exporting_country_ID") VALUES (?, ?)
    2021-12-18 20:51:18,013 INFO sqlalchemy.engine.base.Engine ('kenya', 12)
    2021-12-18 20:51:18,014 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country (exporting_country_name, "exporting_country_ID") VALUES (?, ?)
    2021-12-18 20:51:18,014 INFO sqlalchemy.engine.base.Engine ('brazil', 23)
    2021-12-18 20:51:18,015 INFO sqlalchemy.engine.base.Engine INSERT INTO exporting_country (exporting_country_name, "exporting_country_ID") VALUES (?, ?)
    2021-12-18 20:51:18,015 INFO sqlalchemy.engine.base.Engine ('peru', 25)
    2021-12-18 20:51:18,016 INFO sqlalchemy.engine.base.Engine COMMIT
    

通过正向引用或者反向引用轻松的获取关联表中对应的数据。例如由商品销售数量找到对应的商品名称。


```python
sale_details_info=session.query(sale_details).filter_by(number=300).first()
commodity_info=sale_details_info.commodity
commodity_name=commodity_info.commodity_name
print("_"*50)
print("销量number=300的商品名为:%s"%commodity_name)
```

    2021-12-18 20:51:52,589 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
    2021-12-18 20:51:52,590 INFO sqlalchemy.engine.base.Engine SELECT sale_details."index" AS sale_details_index, sale_details.commodity_code AS sale_details_commodity_code, sale_details.number AS sale_details_number, sale_details.idx AS sale_details_idx 
    FROM sale_details 
    WHERE sale_details.number = ?
     LIMIT ? OFFSET ?
    2021-12-18 20:51:52,591 INFO sqlalchemy.engine.base.Engine (300, 1, 0)
    2021-12-18 20:51:52,594 INFO sqlalchemy.engine.base.Engine SELECT commodity."index" AS commodity_index, commodity.commodity_name AS commodity_commodity_name, commodity.commodity_code AS commodity_commodity_code 
    FROM commodity 
    WHERE ? = commodity.commodity_code
    2021-12-18 20:51:52,595 INFO sqlalchemy.engine.base.Engine (102,)
    __________________________________________________
    销量number=300的商品名为:strawberry
    

* 使用[DB Browser for SQLite(DB4S)](https://sqlitebrowser.org/)查看数据库

<a href=""><img src="./imgs/1_3_04.jpg" height='auto' width=800 title="caDesign"></a>

### 1.3.3 PostgreSQL数据库

[PostgreSQL](https://www.postgresql.org/)是一个强大开源的对象关系数据库系统（open source object-relational database system）。经过30多年的发展，其在可靠性、特征的健壮性和性能方面赢得了很高的声誉。同时，因为PostgreSQL可以存储具有投影坐标系统的地理空间数据，在[QGIS](https://www.postgresql.org/)等地理信息系统工具平台下可以直接从PostgreSQL（PostGIS）中读入与显示数据，建立地图，弥补了python在地图表达上的不足，而又可以充分利用python的数据处理能力。

通常使用开源的[pgAdmin](https://www.pgadmin.org/)工具查看管理PostgreSQL数据库。

用[Array of Things(AoT) 城市环境传感器](https://arrayofthings.github.io/)数据演示python数据处理、写入和读取PostgreSQL数据库，及使用[QGIS](https://www.qgis.org/en/site/)调入数据库中的数据，建立地图的方法。[数据下载地址](https://www.mcs.anl.gov/research/projects/waggle/downloads/datasets/index.php)

> 2019年10月，AoT团队基于已有研究成功申请了国家科学基金（美）的资助，创建新的软硬件基础设施构建的城市环境传感器网络，并开源了获取的实时传感器数据。关于数据的详细说明可以查看数据包中的说明文档，详细解释了各字段的含义，同时给出了所使用传感器的型号和详细说明链接，这对于研究城市环境下的局地小气候具有重要价值。


#### 1）读取nodes数据，并转换为GeoDataFrame格式

nodes.csv数据文件，包括所有布置于城市的传感器位置节点编号、坐标(wgs84)、地址等信息。使用经纬度，通过[shapely库](https://shapely.readthedocs.io/en/stable/manual.html)建立地理空间点后，用[geopandas库](https://geopandas.org/en/stable/)，给定坐标系统wgs84的epsg编号4326转换为GeoDataFrame格式数据，方便地理信息数据的存储、分析和写入PostgreSQL数据库。


```python
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

AoT_nodes_fp='./data/AoT_Chicago.complete.2021-12-20/nodes.csv'
AoT_nodes_df=pd.read_csv(AoT_nodes_fp,sep=",",header=0)

epsg_wgs84=4326
AoT_nodes_df["geometry"]=AoT_nodes_df.apply(lambda row:Point(row.lon,row.lat),axis=1) #使用shapely库建立几何点数据
AoT_nodes_gdf=gpd.GeoDataFrame(AoT_nodes_df,crs=epsg_wgs84)
print("crs{}{}".format("-"*10,AoT_nodes_gdf.crs))
AoT_nodes_gdf
```

    crs----------epsg:4326
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>node_id</th>
      <th>project_id</th>
      <th>vsn</th>
      <th>address</th>
      <th>lat</th>
      <th>lon</th>
      <th>description</th>
      <th>start_timestamp</th>
      <th>end_timestamp</th>
      <th>geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>001e0610ba46</td>
      <td>AoT_Chicago</td>
      <td>004</td>
      <td>State St &amp; Jackson Blvd Chicago IL</td>
      <td>41.878377</td>
      <td>-87.627678</td>
      <td>AoT Chicago (S) [C]</td>
      <td>2017/10/09 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.62768 41.87838)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>001e0610ba3b</td>
      <td>AoT_Chicago</td>
      <td>006</td>
      <td>18th St &amp; Lake Shore Dr Chicago IL</td>
      <td>41.858136</td>
      <td>-87.616055</td>
      <td>AoT Chicago (S)</td>
      <td>2017/08/08 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.61606 41.85814)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>001e0610f02f</td>
      <td>AoT_Chicago</td>
      <td>00A</td>
      <td>Lake Shore Drive &amp; Fullerton Ave Chicago IL</td>
      <td>41.926261</td>
      <td>-87.630758</td>
      <td>AoT Chicago (S) [CA]</td>
      <td>2018/05/07 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.63076 41.92626)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>001e0610ba8f</td>
      <td>AoT_Chicago</td>
      <td>00D</td>
      <td>Cornell &amp; 47th St Chicago IL</td>
      <td>41.810342</td>
      <td>-87.590228</td>
      <td>AoT Chicago (S)</td>
      <td>2017/08/08 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.59023 41.81034)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>001e0610ba16</td>
      <td>AoT_Chicago</td>
      <td>010</td>
      <td>Homan Ave &amp; Roosevelt Rd Chicago IL</td>
      <td>41.866349</td>
      <td>-87.710543</td>
      <td>AoT Chicago (S) [C]</td>
      <td>2018/07/18 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.71054 41.86635)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>121</th>
      <td>001e06118433</td>
      <td>AoT_Chicago</td>
      <td>10E</td>
      <td>ComEd Training Center</td>
      <td>41.829806</td>
      <td>-87.659467</td>
      <td>AoT Chicago (S) [CP] {ComEd}</td>
      <td>2019/04/25 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.65947 41.82981)</td>
    </tr>
    <tr>
      <th>122</th>
      <td>001e061183bf</td>
      <td>AoT_Chicago</td>
      <td>11A</td>
      <td>ComEd Training Center</td>
      <td>41.829806</td>
      <td>-87.659467</td>
      <td>AoT Chicago (S) [CP] {ComEd}</td>
      <td>2019/04/25 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.65947 41.82981)</td>
    </tr>
    <tr>
      <th>123</th>
      <td>001e0611804d</td>
      <td>AoT_Chicago</td>
      <td>11E</td>
      <td>ComEd Training Center</td>
      <td>41.829806</td>
      <td>-87.659467</td>
      <td>AoT Chicago (S) [CP] {ComEd}</td>
      <td>2019/04/25 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.65947 41.82981)</td>
    </tr>
    <tr>
      <th>124</th>
      <td>001e061182a2</td>
      <td>AoT_Chicago</td>
      <td>13B</td>
      <td>ComEd Training Center</td>
      <td>41.829806</td>
      <td>-87.659467</td>
      <td>AoT Chicago (S) [CP] {ComEd}</td>
      <td>2019/04/25 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.65947 41.82981)</td>
    </tr>
    <tr>
      <th>125</th>
      <td>001e061144be</td>
      <td>AoT_Chicago</td>
      <td>890</td>
      <td>UChicago, Regenstine Chicago IL</td>
      <td>41.792543</td>
      <td>-87.600008</td>
      <td>AoT Chicago (S) [C] {UChicago}</td>
      <td>2018/03/15 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.60001 41.79254)</td>
    </tr>
  </tbody>
</table>
<p>126 rows × 10 columns</p>
</div>



从[Chicago Data Portal](https://data.cityofchicago.org/)中搜索下载行政区划数据，读取后与nodes数据叠合显示，方便定位传感器在城市中的位置。


```python
Chicago_community_areas_fp='./data/Chicago Community Areas/Chicago Community Areas.shp'
Chicago_community_areas=gpd.read_file(Chicago_community_areas_fp)
print("crs<Chicago_community_areas>:{}".format(Chicago_community_areas.crs))
Chicago_community_areas
```

    crs<Chicago_community_areas>:epsg:4326
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>area</th>
      <th>area_num_1</th>
      <th>area_numbe</th>
      <th>comarea</th>
      <th>comarea_id</th>
      <th>community</th>
      <th>perimeter</th>
      <th>shape_area</th>
      <th>shape_len</th>
      <th>geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>35</td>
      <td>35</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>DOUGLAS</td>
      <td>0.0</td>
      <td>4.600462e+07</td>
      <td>31027.054510</td>
      <td>POLYGON ((-87.60914 41.84469, -87.60915 41.844...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>36</td>
      <td>36</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>OAKLAND</td>
      <td>0.0</td>
      <td>1.691396e+07</td>
      <td>19565.506153</td>
      <td>POLYGON ((-87.59215 41.81693, -87.59231 41.816...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>37</td>
      <td>37</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>FULLER PARK</td>
      <td>0.0</td>
      <td>1.991670e+07</td>
      <td>25339.089750</td>
      <td>POLYGON ((-87.62880 41.80189, -87.62879 41.801...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>38</td>
      <td>38</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>GRAND BOULEVARD</td>
      <td>0.0</td>
      <td>4.849250e+07</td>
      <td>28196.837157</td>
      <td>POLYGON ((-87.60671 41.81681, -87.60670 41.816...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>39</td>
      <td>39</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>KENWOOD</td>
      <td>0.0</td>
      <td>2.907174e+07</td>
      <td>23325.167906</td>
      <td>POLYGON ((-87.59215 41.81693, -87.59215 41.816...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.0</td>
      <td>74</td>
      <td>74</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>MOUNT GREENWOOD</td>
      <td>0.0</td>
      <td>7.558429e+07</td>
      <td>48665.130539</td>
      <td>POLYGON ((-87.69646 41.70714, -87.69644 41.706...</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.0</td>
      <td>75</td>
      <td>75</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>MORGAN PARK</td>
      <td>0.0</td>
      <td>9.187734e+07</td>
      <td>46396.419362</td>
      <td>POLYGON ((-87.64215 41.68508, -87.64249 41.685...</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.0</td>
      <td>76</td>
      <td>76</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>OHARE</td>
      <td>0.0</td>
      <td>3.718356e+08</td>
      <td>173625.984660</td>
      <td>MULTIPOLYGON (((-87.83658 41.98640, -87.83658 ...</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.0</td>
      <td>77</td>
      <td>77</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>EDGEWATER</td>
      <td>0.0</td>
      <td>4.844999e+07</td>
      <td>31004.830946</td>
      <td>POLYGON ((-87.65456 41.99817, -87.65456 41.998...</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.0</td>
      <td>9</td>
      <td>9</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>EDISON PARK</td>
      <td>0.0</td>
      <td>3.163631e+07</td>
      <td>25937.226841</td>
      <td>POLYGON ((-87.80676 42.00084, -87.80676 42.000...</td>
    </tr>
  </tbody>
</table>
<p>77 rows × 10 columns</p>
</div>




```python
import matplotlib.pyplot as plt

fig, ax=plt.subplots(figsize=(10,15))
Chicago_community_areas.plot(ax=ax,color='white', edgecolor='black')
AoT_nodes_gdf.plot(ax=ax,markersize=10,column='description',legend=True,cmap='tab20c',legend_kwds={'loc': 'lower left'})
#ax.axis('off')
plt.show()
```


<a href=""><img src="./imgs/1_3_07.png" height='auto' width=500 title="caDesign"></a>   


#### 2）GeoDataFrame数据读写PostgreSQL数据库

将GeoDataFrame数据读写PostgreSQL数据库分别定义为`gpd2postSQL`和`postSQL2gpd`函数，可以放置于自定义的.py文件下，例如本书定义的util_database.py文件，方便日后调用。需要注意，建立数据库时，首先本地安装PostgreSQL，再使用安装的pgAdmin工具建立数据库，例如本例建立数据库名为'AoT',用户名为'postgres'，密码为'123456'。同时，要在pgAdmin的Query Tool下执行`CREATE EXTENSION postgis;`命令，从而可以存储具有坐标系统的地理几何对象，否则将GeoDataFrame数据写入PostgreSQL数据库时，会提示错误。


```python
def gpd2postSQL(gdf,table_name,**kwargs):
    from sqlalchemy import create_engine
    
    '''
    function - 将GeoDataFrame格式数据写入PostgreSQL数据库
    
    Paras:
        gdf - GeoDataFrame格式数据，含geometry字段（几何对象，点、线和面，数据值对应定义的坐标系统）
        table_name - 写入数据库中的表名
        **kwargs - 连接数据库相关信息，包括myusername（数据库的用户名），mypassword（用户密钥），mydatabase（数据库名）
    '''     
    engine=create_engine("postgres://{myusername}:{mypassword}@localhost:5432/{mydatabase}".format(myusername=kwargs['myusername'],mypassword=kwargs['mypassword'],mydatabase=kwargs['mydatabase']))  
    gdf.to_postgis(table_name, con=engine, if_exists='replace', index=False,)  
    print("_"*50)
    print('The GeoDataFrame has been written to the PostgreSQL database.The table name is {}.'.format(table_name))

def postSQL2gpd(table_name,geom_col='geometry',**kwargs):
    from sqlalchemy import create_engine
    import geopandas as gpd
    
    '''
    function - 读取PostgreSQL数据库中的表为GeoDataFrame格式数据
    
    Paras:
        table_name - 待读取数据库中的表名
        geom_col='geometry' - 几何对象，常规默认字段为'geometry'
        **kwargs - 连接数据库相关信息，包括myusername（数据库的用户名），mypassword（用户密钥），mydatabase（数据库名）
    '''
    engine=create_engine("postgres://{myusername}:{mypassword}@localhost:5432/{mydatabase}".format(myusername=kwargs['myusername'],mypassword=kwargs['mypassword'],mydatabase=kwargs['mydatabase']))  
    gdf=gpd.read_postgis(table_name, con=engine,geom_col=geom_col)
    print("_"*50)
    print('The data has been read from PostgreSQL database. The table name is {}.'.format(table_name))    
    return gdf  
    
gpd2postSQL(AoT_nodes_gdf,table_name='AoT_nodes',myusername='postgres',mypassword='123456',mydatabase='AoT') 
gpd2postSQL(Chicago_community_areas,table_name='Chicago_community_areas',myusername='postgres',mypassword='123456',mydatabase='AoT') 
```

    __________________________________________________
    The GeoDataFrame has been written to the PostgreSQL database.The table name is AoT_nodes.
    __________________________________________________
    The GeoDataFrame has been written to the PostgreSQL database.The table name is Chicago_community_areas.
    

为方便后期QIGS调用建立地图，可以直接定义投影后再写入数据库。


```python
epsg_Chicago=32616
gpd2postSQL(AoT_nodes_gdf.to_crs(epsg_Chicago),table_name='AoT_nodes_prj',myusername='postgres',mypassword='123456',mydatabase='AoT') 
gpd2postSQL(Chicago_community_areas.to_crs(epsg_Chicago),table_name='Chicago_community_areas_prj',myusername='postgres',mypassword='123456',mydatabase='AoT') 
```

    __________________________________________________
    The GeoDataFrame has been written to the PostgreSQL database.The table name is AoT_nodes_prj.
    __________________________________________________
    The GeoDataFrame has been written to the PostgreSQL database.The table name is Chicago_community_areas_prj.
    

在建立一个研究项目时，通常将基本的数据写入数据库后，再从数据库中调用对应的表读入数据，进行后续的分析。


```python
AoT_nodes_gdf=postSQL2gpd(table_name='AoT_nodes',geom_col='geometry',myusername='postgres',mypassword='123456',mydatabase='AoT')
Chicago_community_areas=postSQL2gpd(table_name='Chicago_community_areas',geom_col='geometry',myusername='postgres',mypassword='123456',mydatabase='AoT')
```

    __________________________________________________
    The data has been read from PostgreSQL database. The table name is AoT_nodes.
    __________________________________________________
    The data has been read from PostgreSQL database. The table name is Chicago_community_areas.
    

打开pgAdmin工具，可以查看写入的数据。

<a href=""><img src="./imgs/1_3_06.jpg" height='auto' width='auto' title="caDesign">
    
#### 3）读取data传感器记录的数据与初步处理

截止December 20 2021 19:18:35 CS时，data数据约有39.5GB，并且为单独一个csv格式文件。如果内存量较小，则需要分批读入处理再写入数据库。这里仅示范读取2018/01/01一天所记录的数据，并计算每小时的温度均值（对应处理'value_hrf'字段），再将其对应'node_id'字段，与`AoT_nodes_gdf`变量合并后，写入数据库。


```python
AoT_data_fp=r"F:\data\AoT_Chicago.complete.2021-12-20\data"
chunksize=10**6
for chunk in pd.read_csv(AoT_data_fp,chunksize=chunksize) :
    AoT_data_part=chunk
    break
AoT_data_20180101=AoT_data_part[(AoT_data_part['timestamp'] >= '2018/01/01 00:00:00') & (AoT_data_part['timestamp'] <= '2018/01/01 23:59:59')]
print("parameter-Sensor parameter that was measured:{}\n{}".format(AoT_data_20180101.parameter.unique(),"_"*50))
print("sensor-Sensor that was measured:{}\n{}".format(AoT_data_20180101.sensor.unique(),"_"*50))
AoT_data_20180101  
```

    parameter-Sensor parameter that was measured:['temperature' 'id' 'concentration' 'pressure' 'humidity' 'ir_intensity'
     'uv_intensity' 'visible_light_intensity' 'intensity']
    __________________________________________________
    sensor-Sensor that was measured:['at0' 'at1' 'at2' 'at3' 'chemsense' 'co' 'h2s' 'lps25h' 'no2' 'o3'
     'oxidizing_gases' 'reducing_gases' 'sht25' 'si1145' 'so2' 'apds_9006_020'
     'hih6130' 'ml8511' 'mlx75305' 'tmp421' 'tsl250rd' 'tsl260rd' 'bmp180'
     'hih4030' 'htu21d' 'metsense' 'pr103j2' 'spv1840lr5h_b' 'tmp112' 'tsys01']
    __________________________________________________
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>node_id</th>
      <th>subsystem</th>
      <th>sensor</th>
      <th>parameter</th>
      <th>value_raw</th>
      <th>value_hrf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018/01/01 00:00:06</td>
      <td>001e0610e532</td>
      <td>chemsense</td>
      <td>at0</td>
      <td>temperature</td>
      <td>-1106</td>
      <td>-11.06</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018/01/01 00:00:06</td>
      <td>001e0610e532</td>
      <td>chemsense</td>
      <td>at1</td>
      <td>temperature</td>
      <td>-1077</td>
      <td>-10.77</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018/01/01 00:00:06</td>
      <td>001e0610e532</td>
      <td>chemsense</td>
      <td>at2</td>
      <td>temperature</td>
      <td>-1009</td>
      <td>-10.09</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018/01/01 00:00:06</td>
      <td>001e0610e532</td>
      <td>chemsense</td>
      <td>at3</td>
      <td>temperature</td>
      <td>-972</td>
      <td>-9.72</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018/01/01 00:00:06</td>
      <td>001e0610e532</td>
      <td>chemsense</td>
      <td>chemsense</td>
      <td>id</td>
      <td>NaN</td>
      <td>541eec3ebfa6</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>769628</th>
      <td>2018/01/01 23:59:59</td>
      <td>001e0610e540</td>
      <td>metsense</td>
      <td>pr103j2</td>
      <td>temperature</td>
      <td>372</td>
      <td>-17.15</td>
    </tr>
    <tr>
      <th>769629</th>
      <td>2018/01/01 23:59:59</td>
      <td>001e0610e540</td>
      <td>metsense</td>
      <td>spv1840lr5h_b</td>
      <td>intensity</td>
      <td>811</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>769630</th>
      <td>2018/01/01 23:59:59</td>
      <td>001e0610e540</td>
      <td>metsense</td>
      <td>tmp112</td>
      <td>temperature</td>
      <td>NaN</td>
      <td>-17.81</td>
    </tr>
    <tr>
      <th>769631</th>
      <td>2018/01/01 23:59:59</td>
      <td>001e0610e540</td>
      <td>metsense</td>
      <td>tsl250rd</td>
      <td>intensity</td>
      <td>2</td>
      <td>0.101</td>
    </tr>
    <tr>
      <th>769632</th>
      <td>2018/01/01 23:59:59</td>
      <td>001e0610e540</td>
      <td>metsense</td>
      <td>tsys01</td>
      <td>temperature</td>
      <td>NaN</td>
      <td>-18.47</td>
    </tr>
  </tbody>
</table>
<p>769633 rows × 7 columns</p>
</div>



读取的data数据，各个字段数据为字符串类型，应用`pd.to_numeric`方法将'value_hrf'数值字段（已转换的各传感器测量值）转换为数值类型。


```python
from tqdm import tqdm

AoT_data_20180101_temperature=AoT_data_20180101[(AoT_data_20180101.parameter=='temperature')] #&(AoT_data_20180101.sensor=='at2')
print("列数据类型：\n{}".format(AoT_data_20180101_temperature.dtypes))
print("_"*50)
print("列名（数据类型为字符串-str-object）:{}".format(AoT_data_20180101_temperature.columns[AoT_data_20180101_temperature.dtypes.eq('object')]))
columns_dtypeEQstr=['value_raw', 'value_hrf'] 
AoT_data_20180101_temperature[columns_dtypeEQstr]=AoT_data_20180101_temperature[columns_dtypeEQstr].apply(pd.to_numeric,errors='coerce', axis=1)

node_id_unqiue=AoT_data_20180101_temperature.node_id.unique()
print("node_id-ID of node which did the measurement:{}\n{}".format(node_id_unqiue,"_"*50))

AoT_data_20180101_temperature_grouped=AoT_data_20180101_temperature.groupby(AoT_data_20180101_temperature.node_id)

value_raw_dic={}
for n_id in tqdm(node_id_unqiue):
    sub_df=AoT_data_20180101_temperature_grouped.get_group(n_id)
    sub_df.set_index(pd.to_datetime(sub_df["timestamp"]),inplace=True)                   
    value_raw_dic[n_id]=sub_df.groupby(sub_df.index.hour)[['value_hrf']].mean()['value_hrf']

value_raw_df=pd.DataFrame.from_dict(value_raw_dic,orient='columns')
value_raw_df
```

    列数据类型：
    timestamp    object
    node_id      object
    subsystem    object
    sensor       object
    parameter    object
    value_raw    object
    value_hrf    object
    dtype: object
    __________________________________________________
    列名（数据类型为字符串-str-object）:Index(['timestamp', 'node_id', 'subsystem', 'sensor', 'parameter', 'value_raw',
           'value_hrf'],
          dtype='object')        

    node_id-ID of node which did the measurement:['001e0610e532' '001e0610bc07' '001e0610ef27' '001e0610e540'
     '001e0610ee61' '001e0610fb4c' '001e0610ba18' '001e0610ba3b'
     '001e0610ba57' '001e0610eef4']
    __________________________________________________
    

    100%|██████████| 10/10 [00:00<00:00, 61.43it/s]
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>001e0610e532</th>
      <th>001e0610bc07</th>
      <th>001e0610ef27</th>
      <th>001e0610e540</th>
      <th>001e0610ee61</th>
      <th>001e0610fb4c</th>
      <th>001e0610ba18</th>
      <th>001e0610ba3b</th>
      <th>001e0610ba57</th>
      <th>001e0610eef4</th>
    </tr>
    <tr>
      <th>timestamp</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-9.495390</td>
      <td>-9.917552</td>
      <td>-10.226980</td>
      <td>-8.861876</td>
      <td>-9.366752</td>
      <td>-8.686524</td>
      <td>-9.450483</td>
      <td>-7.710883</td>
      <td>-9.393620</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-10.813103</td>
      <td>-10.995152</td>
      <td>-11.326974</td>
      <td>-10.034667</td>
      <td>-10.435219</td>
      <td>-9.652314</td>
      <td>-10.546560</td>
      <td>-9.042917</td>
      <td>-10.692798</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-11.573774</td>
      <td>-11.687457</td>
      <td>-12.102297</td>
      <td>-11.064686</td>
      <td>-11.291581</td>
      <td>-10.516943</td>
      <td>-11.084158</td>
      <td>-9.999010</td>
      <td>-11.409276</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-12.369585</td>
      <td>-12.489413</td>
      <td>-12.751727</td>
      <td>-11.809482</td>
      <td>-11.942571</td>
      <td>-10.883657</td>
      <td>-11.918073</td>
      <td>-10.383381</td>
      <td>-12.361975</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-13.095918</td>
      <td>-13.507696</td>
      <td>-13.738810</td>
      <td>-12.568714</td>
      <td>-12.657502</td>
      <td>-11.868581</td>
      <td>-12.701017</td>
      <td>-11.531895</td>
      <td>-12.953586</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-13.551785</td>
      <td>-14.439482</td>
      <td>-14.571918</td>
      <td>-13.237114</td>
      <td>-13.476429</td>
      <td>-12.675438</td>
      <td>-13.376158</td>
      <td>-12.122713</td>
      <td>-13.156931</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-14.569456</td>
      <td>-15.302564</td>
      <td>-15.470538</td>
      <td>-14.197971</td>
      <td>-14.364105</td>
      <td>-13.381010</td>
      <td>-14.159520</td>
      <td>-12.947673</td>
      <td>-13.956623</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-15.245185</td>
      <td>-15.877248</td>
      <td>-16.133722</td>
      <td>-14.912029</td>
      <td>-15.140795</td>
      <td>-14.087324</td>
      <td>-14.920011</td>
      <td>-13.660048</td>
      <td>-14.691400</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-15.873405</td>
      <td>-16.625105</td>
      <td>-16.862518</td>
      <td>-15.496039</td>
      <td>-15.749362</td>
      <td>-14.682562</td>
      <td>-15.698409</td>
      <td>-14.333693</td>
      <td>-15.350267</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-16.401733</td>
      <td>-17.160867</td>
      <td>-17.465528</td>
      <td>-16.081622</td>
      <td>-16.401067</td>
      <td>-15.227514</td>
      <td>-16.457512</td>
      <td>-14.739315</td>
      <td>-15.775695</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-16.785810</td>
      <td>-17.597057</td>
      <td>-17.824492</td>
      <td>-16.495790</td>
      <td>-16.710400</td>
      <td>-15.573029</td>
      <td>-16.724333</td>
      <td>-15.053427</td>
      <td>-16.113267</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>11</th>
      <td>-17.216205</td>
      <td>-17.946057</td>
      <td>-18.192976</td>
      <td>-16.960333</td>
      <td>-16.979152</td>
      <td>-15.872752</td>
      <td>-17.060516</td>
      <td>-15.608010</td>
      <td>-16.528219</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-17.611728</td>
      <td>-18.381014</td>
      <td>-18.359292</td>
      <td>-17.252210</td>
      <td>-17.358428</td>
      <td>-16.358029</td>
      <td>-17.462519</td>
      <td>-16.109831</td>
      <td>-16.914276</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-17.806830</td>
      <td>-18.529229</td>
      <td>-18.495518</td>
      <td>-17.293212</td>
      <td>-17.529314</td>
      <td>-16.459971</td>
      <td>-17.504382</td>
      <td>-16.060376</td>
      <td>-17.081043</td>
      <td>-17.358937</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-17.651686</td>
      <td>-18.058190</td>
      <td>-16.940301</td>
      <td>-16.946705</td>
      <td>-16.176010</td>
      <td>-15.323431</td>
      <td>-17.179048</td>
      <td>-14.384124</td>
      <td>-15.849938</td>
      <td>-16.302238</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-17.015144</td>
      <td>-15.972505</td>
      <td>-15.721166</td>
      <td>-16.296347</td>
      <td>-14.262914</td>
      <td>-13.819606</td>
      <td>-16.529705</td>
      <td>-12.838782</td>
      <td>-14.290691</td>
      <td>-15.339267</td>
    </tr>
    <tr>
      <th>16</th>
      <td>-16.036328</td>
      <td>-15.292181</td>
      <td>-13.998253</td>
      <td>-14.868127</td>
      <td>-12.801219</td>
      <td>-12.685005</td>
      <td>-14.946886</td>
      <td>-12.192076</td>
      <td>-13.180194</td>
      <td>-13.971981</td>
    </tr>
    <tr>
      <th>17</th>
      <td>-14.950979</td>
      <td>-13.869676</td>
      <td>-12.780328</td>
      <td>-13.304919</td>
      <td>-12.256590</td>
      <td>-10.676219</td>
      <td>-13.021371</td>
      <td>-10.194095</td>
      <td>-12.244765</td>
      <td>-12.332029</td>
    </tr>
    <tr>
      <th>18</th>
      <td>-13.876210</td>
      <td>-12.862067</td>
      <td>-12.558569</td>
      <td>-12.037333</td>
      <td>-12.352848</td>
      <td>-9.764724</td>
      <td>-11.578667</td>
      <td>-8.927877</td>
      <td>-10.622000</td>
      <td>-11.304867</td>
    </tr>
    <tr>
      <th>19</th>
      <td>-12.404682</td>
      <td>-12.737584</td>
      <td>-12.022304</td>
      <td>-11.406105</td>
      <td>-11.506419</td>
      <td>-9.759933</td>
      <td>-10.299181</td>
      <td>-8.096674</td>
      <td>-10.116876</td>
      <td>-10.750838</td>
    </tr>
    <tr>
      <th>20</th>
      <td>-12.065452</td>
      <td>-12.428440</td>
      <td>-11.959846</td>
      <td>-10.974516</td>
      <td>-10.648790</td>
      <td>-9.756133</td>
      <td>-9.839695</td>
      <td>-8.805656</td>
      <td>-10.319571</td>
      <td>-10.314295</td>
    </tr>
    <tr>
      <th>21</th>
      <td>-12.485923</td>
      <td>-13.010326</td>
      <td>-12.590530</td>
      <td>-11.916676</td>
      <td>-10.988782</td>
      <td>-10.503259</td>
      <td>-11.377819</td>
      <td>-10.707610</td>
      <td>-11.273475</td>
      <td>-10.693224</td>
    </tr>
    <tr>
      <th>22</th>
      <td>-13.456077</td>
      <td>-14.234840</td>
      <td>-14.133451</td>
      <td>-13.413810</td>
      <td>-12.211169</td>
      <td>-11.952600</td>
      <td>-12.797833</td>
      <td>-12.155292</td>
      <td>-12.818308</td>
      <td>-12.757085</td>
    </tr>
    <tr>
      <th>23</th>
      <td>-14.141487</td>
      <td>-15.405905</td>
      <td>-15.061102</td>
      <td>-14.382365</td>
      <td>-13.325823</td>
      <td>-12.918876</td>
      <td>-13.744962</td>
      <td>-13.581819</td>
      <td>-13.886686</td>
      <td>-13.877945</td>
    </tr>
  </tbody>
</table>
</div>




```python
AoT_nodes_temperature_gdf=pd.merge(AoT_nodes_gdf,value_raw_df.T.reset_index().rename(columns={'index':'node_id'}),on="node_id")
AoT_nodes_temperature_gdf
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>node_id</th>
      <th>project_id</th>
      <th>vsn</th>
      <th>address</th>
      <th>lat</th>
      <th>lon</th>
      <th>description</th>
      <th>start_timestamp</th>
      <th>end_timestamp</th>
      <th>geometry</th>
      <th>...</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>001e0610ba3b</td>
      <td>AoT_Chicago</td>
      <td>006</td>
      <td>18th St &amp; Lake Shore Dr Chicago IL</td>
      <td>41.858136</td>
      <td>-87.616055</td>
      <td>AoT Chicago (S)</td>
      <td>2017/08/08 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.61606 41.85814)</td>
      <td>...</td>
      <td>-14.384124</td>
      <td>-12.838782</td>
      <td>-12.192076</td>
      <td>-10.194095</td>
      <td>-8.927877</td>
      <td>-8.096674</td>
      <td>-8.805656</td>
      <td>-10.707610</td>
      <td>-12.155292</td>
      <td>-13.581819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>001e0610ba18</td>
      <td>AoT_Chicago</td>
      <td>01D</td>
      <td>Damen Ave &amp; Cermak Chicago IL</td>
      <td>41.852179</td>
      <td>-87.675825</td>
      <td>AoT Chicago (S)</td>
      <td>2017/12/15 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.67583 41.85218)</td>
      <td>...</td>
      <td>-17.179048</td>
      <td>-16.529705</td>
      <td>-14.946886</td>
      <td>-13.021371</td>
      <td>-11.578667</td>
      <td>-10.299181</td>
      <td>-9.839695</td>
      <td>-11.377819</td>
      <td>-12.797833</td>
      <td>-13.744962</td>
    </tr>
    <tr>
      <th>2</th>
      <td>001e0610eef4</td>
      <td>AoT_Chicago</td>
      <td>034</td>
      <td>Milwaukee Ave &amp; Wabansia Ave Chicago IL</td>
      <td>41.912681</td>
      <td>-87.681052</td>
      <td>AoT Chicago (S)</td>
      <td>2017/10/09 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.68105 41.91268)</td>
      <td>...</td>
      <td>-16.302238</td>
      <td>-15.339267</td>
      <td>-13.971981</td>
      <td>-12.332029</td>
      <td>-11.304867</td>
      <td>-10.750838</td>
      <td>-10.314295</td>
      <td>-10.693224</td>
      <td>-12.757085</td>
      <td>-13.877945</td>
    </tr>
    <tr>
      <th>3</th>
      <td>001e0610bc07</td>
      <td>AoT_Chicago</td>
      <td>03C</td>
      <td>Kedzie Ave &amp; 5th Ave Chicago IL</td>
      <td>41.878372</td>
      <td>-87.706042</td>
      <td>AoT Chicago (S)</td>
      <td>2017/11/20 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.70604 41.87837)</td>
      <td>...</td>
      <td>-18.058190</td>
      <td>-15.972505</td>
      <td>-15.292181</td>
      <td>-13.869676</td>
      <td>-12.862067</td>
      <td>-12.737584</td>
      <td>-12.428440</td>
      <td>-13.010326</td>
      <td>-14.234840</td>
      <td>-15.405905</td>
    </tr>
    <tr>
      <th>4</th>
      <td>001e0610ee61</td>
      <td>AoT_Chicago</td>
      <td>03F</td>
      <td>Pulaski Rd &amp; Madison St Chicago IL</td>
      <td>41.880732</td>
      <td>-87.725660</td>
      <td>AoT Chicago (S)</td>
      <td>2017/11/20 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.72566 41.88073)</td>
      <td>...</td>
      <td>-16.176010</td>
      <td>-14.262914</td>
      <td>-12.801219</td>
      <td>-12.256590</td>
      <td>-12.352848</td>
      <td>-11.506419</td>
      <td>-10.648790</td>
      <td>-10.988782</td>
      <td>-12.211169</td>
      <td>-13.325823</td>
    </tr>
    <tr>
      <th>5</th>
      <td>001e0610ba57</td>
      <td>AoT_Chicago</td>
      <td>041</td>
      <td>Western Ave &amp; Madison St Chicago IL</td>
      <td>41.881172</td>
      <td>-87.686359</td>
      <td>AoT Chicago (S)</td>
      <td>2017/11/20 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.68636 41.88117)</td>
      <td>...</td>
      <td>-15.849938</td>
      <td>-14.290691</td>
      <td>-13.180194</td>
      <td>-12.244765</td>
      <td>-10.622000</td>
      <td>-10.116876</td>
      <td>-10.319571</td>
      <td>-11.273475</td>
      <td>-12.818308</td>
      <td>-13.886686</td>
    </tr>
    <tr>
      <th>6</th>
      <td>001e0610ef27</td>
      <td>AoT_Chicago</td>
      <td>04C</td>
      <td>Western Ave &amp; 25th St Chicago IL</td>
      <td>41.846579</td>
      <td>-87.685557</td>
      <td>AoT Chicago (S) [C]</td>
      <td>2017/12/15 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.68556 41.84658)</td>
      <td>...</td>
      <td>-16.940301</td>
      <td>-15.721166</td>
      <td>-13.998253</td>
      <td>-12.780328</td>
      <td>-12.558569</td>
      <td>-12.022304</td>
      <td>-11.959846</td>
      <td>-12.590530</td>
      <td>-14.133451</td>
      <td>-15.061102</td>
    </tr>
    <tr>
      <th>7</th>
      <td>001e0610fb4c</td>
      <td>AoT_Chicago</td>
      <td>04D</td>
      <td>Leavitt St &amp; Milwaukee Ave Chicago IL</td>
      <td>41.913583</td>
      <td>-87.682414</td>
      <td>AoT Chicago (S)</td>
      <td>2017/10/09 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.68241 41.91358)</td>
      <td>...</td>
      <td>-15.323431</td>
      <td>-13.819606</td>
      <td>-12.685005</td>
      <td>-10.676219</td>
      <td>-9.764724</td>
      <td>-9.759933</td>
      <td>-9.756133</td>
      <td>-10.503259</td>
      <td>-11.952600</td>
      <td>-12.918876</td>
    </tr>
    <tr>
      <th>8</th>
      <td>001e0610e532</td>
      <td>AoT_Chicago</td>
      <td>053</td>
      <td>Racine Ave &amp; 18th St Chicago IL</td>
      <td>41.857959</td>
      <td>-87.656427</td>
      <td>AoT Chicago (S) [C]</td>
      <td>2017/12/15 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.65643 41.85796)</td>
      <td>...</td>
      <td>-17.651686</td>
      <td>-17.015144</td>
      <td>-16.036328</td>
      <td>-14.950979</td>
      <td>-13.876210</td>
      <td>-12.404682</td>
      <td>-12.065452</td>
      <td>-12.485923</td>
      <td>-13.456077</td>
      <td>-14.141487</td>
    </tr>
    <tr>
      <th>9</th>
      <td>001e0610e540</td>
      <td>AoT_Chicago</td>
      <td>05A</td>
      <td>Fort Dearborn Dr &amp; 31st St Chicago IL</td>
      <td>41.838618</td>
      <td>-87.607817</td>
      <td>AoT Chicago (S) [C]</td>
      <td>2017/11/29 00:00:00</td>
      <td>NaN</td>
      <td>POINT (-87.60782 41.83862)</td>
      <td>...</td>
      <td>-16.946705</td>
      <td>-16.296347</td>
      <td>-14.868127</td>
      <td>-13.304919</td>
      <td>-12.037333</td>
      <td>-11.406105</td>
      <td>-10.974516</td>
      <td>-11.916676</td>
      <td>-13.413810</td>
      <td>-14.382365</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 34 columns</p>
</div>




```python
gpd2postSQL(AoT_nodes_temperature_gdf,table_name='AoT_nodes_temperature',myusername='postgres',mypassword='123456',mydatabase='AoT') 
gpd2postSQL(AoT_nodes_temperature_gdf.to_crs(epsg_Chicago),table_name='AoT_nodes_temperature_prj',myusername='postgres',mypassword='123456',mydatabase='AoT') 
```

    __________________________________________________
    has been written to into the PostSQL database...
    __________________________________________________
    has been written to into the PostSQL database...
    

#### 4）QGIS读取PostgreSQL数据库

虽然python中的GeoPandas，以及其它图表库可以直接打印地图，但是不是很方便处理地图的细节表达，尤其用于专著、论文或者其它传播时。应用QGIS来构建地图，可以直接从PostgreSQL数据库中读取表（在PostGIS下建立数据库连接），和python基本无缝结合，这可以充分利用python的数据处理能力，和QGIS的地图表达能力。

<a href=""><img src="./imgs/1_3_05.jpg" height='auto' width='auto' title="caDesign">


#### 5）计算样本总长度（附）

data数据为单独的一个csv格式文件，可以通过下述代码来计算总样本数，即行数。也可以读取指定范围的部分行。但是读取部分行时，仍旧需要耗费一定时间来略过需要忽略的行，通常可以分批处理后，分别写入数据库或存储为单独的文件再读取处理。


```python
from tqdm.auto import tqdm

count=0
for chunk in tqdm(pd.read_csv(AoT_data_fp,chunksize=chunksize)):
    count+= 1 #样本分组数
    last_len=len(chunk)  #最后一组的样本数量
data_length=(count*chunksize+last_len-chunksize) #数据行（样本）总长度
print("数据行（样本）总长度={}".format(data_length))
```


    0it [00:00, ?it/s]


    数据行（样本）总长度=573074785
    


```python
rows_diff=data_length-chunksize
AoT_data_lastChunck=pd.read_csv(AoT_data_fp,skiprows=range(1,rows_diff),nrows=chunksize-1)
```


```python
AoT_data_lastChunck
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>node_id</th>
      <th>subsystem</th>
      <th>sensor</th>
      <th>parameter</th>
      <th>value_raw</th>
      <th>value_hrf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018/05/15 08:00:25</td>
      <td>001e06113a24</td>
      <td>lightsense</td>
      <td>hmc5883l</td>
      <td>magnetic_field_y</td>
      <td>564</td>
      <td>512.727</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018/05/15 08:00:25</td>
      <td>001e06113a24</td>
      <td>lightsense</td>
      <td>hmc5883l</td>
      <td>magnetic_field_z</td>
      <td>52</td>
      <td>53.061</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018/05/15 08:00:25</td>
      <td>001e06113a24</td>
      <td>lightsense</td>
      <td>ml8511</td>
      <td>intensity</td>
      <td>9312</td>
      <td>38.133</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018/05/15 08:00:25</td>
      <td>001e06113a24</td>
      <td>lightsense</td>
      <td>mlx75305</td>
      <td>intensity</td>
      <td>405</td>
      <td>-4.151</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018/05/15 08:00:25</td>
      <td>001e06113a24</td>
      <td>lightsense</td>
      <td>tmp421</td>
      <td>temperature</td>
      <td>12464</td>
      <td>48.69</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>999994</th>
      <td>2018/05/15 11:12:05</td>
      <td>001e0610ee41</td>
      <td>metsense</td>
      <td>tsl250rd</td>
      <td>intensity</td>
      <td>157</td>
      <td>7.906</td>
    </tr>
    <tr>
      <th>999995</th>
      <td>2018/05/15 11:12:05</td>
      <td>001e0610ee41</td>
      <td>metsense</td>
      <td>tsys01</td>
      <td>temperature</td>
      <td>NaN</td>
      <td>20.76</td>
    </tr>
    <tr>
      <th>999996</th>
      <td>2018/05/15 11:12:05</td>
      <td>001e0610f703</td>
      <td>alphasense</td>
      <td>opc_n2</td>
      <td>fw</td>
      <td>ffff</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>999997</th>
      <td>2018/05/15 11:12:05</td>
      <td>001e0610f703</td>
      <td>lightsense</td>
      <td>apds_9006_020</td>
      <td>intensity</td>
      <td>34</td>
      <td>2.733</td>
    </tr>
    <tr>
      <th>999998</th>
      <td>2018/05/15 11:12:05</td>
      <td>001e0610f703</td>
      <td>lightsense</td>
      <td>hih6130</td>
      <td>humidity</td>
      <td>23629</td>
      <td>44.22</td>
    </tr>
  </tbody>
</table>
<p>999999 rows × 7 columns</p>
</div>



### 1.3.4 数据分析基本流程组织

开始一个以数据分析为主的研究课题（项目），通常建立一个单独的文件夹，在该文件夹下放置该项目的所有代码、以及相关的数据、图表等内容。一般定义的子文件夹包括：
1. data - 存放原始的数据
2. data_processed - 存放处理的过程数据
3. database - 放置数据库
4. graph - 存放图表（一般由python代码直接输出）
5. imgs - 存放一般的图像
6. map - 放置地图文件（例如QGIS）
7. model - 存储训练好的模型（例如机器/深度学习，网络模型等）

文件夹命名可以参考上述，亦可自行灵活调整。因为代码工程量会随着分析内容的深入不断增加，为了防止代码丢失，非常必要将其推送（push）到[GitHub](https://github.com/)代码托管平台，或国内相关的代码托管平台上。当代码更新时，可以推送更新云端仓库（repository）。可以使用GitHub的[桌面版（desktop）](https://desktop.github.com/)操作，具体方法可以查看官网。

通过子文件夹的结构，可以明了数据分析基本流程组织。从data下读取原始数据（如果数据文件较大，也会存储于外置硬盘中）；处理的过程数据则放置于data_processed中；为了方便数据的管理和读写，优先选择将数据写入数据库，SQLite为单独的文件，可以直接放置于database中，而postgreSQL是直接写入默认的安装路径下，在单独一个项目完结后，可以备份导出数据库；分析过程图表（用于说明分析结果，或过程数据分析描述，往往为书写科研论文的重要部分或用以报告）放置于graph下；其它非python直接输出的图表或图像，存放于imgs下；QGIS读取数据库构建的地图放在map下；训练的模型则存储于model子文件中；.py的代码则直接位于根目录下，与子文件夹并列，这是为了方便直接读写数据。例如`db_fp=r'./database/fruits.sqlite'`和`AoT_nodes_fp='./data/AoT_Chicago.complete.2021-12-20/nodes.csv'`等，使用相对路径比较简单明了。如果有特殊需要，再建立存储代码的子文件夹。

常用自定义或者迁移的代码工具，为了方便调用，通常存储于单独的.py文件下，例如本书的util_database.py（用于数据库操作的代码函数），util_misc.py（包括显示文件的结构等杂项代码类或函数）等。

下述迁移的代码可以查看文件夹的结构，打印了截止当前，本书代码工程项目的文件夹内容结构。


```python
class DisplayablePath(object):
    '''
    class - 返回指定路径下所有文件夹及其下文件的结构。代码未改动，迁移于'https://stackoverflow.com/questions/9727673/list-directory-tree-structure-in-python'
    '''
    
    display_filename_prefix_middle = '├──'
    display_filename_prefix_last = '└──'
    display_parent_prefix_middle = '    '
    display_parent_prefix_last = '│   '

    def __init__(self, path, parent_path, is_last):
        from pathlib import Path
        
        self.path = Path(str(path))
        self.parent = parent_path
        self.is_last = is_last
        if self.parent:
            self.depth = self.parent.depth + 1
        else:
            self.depth = 0

    @property
    def displayname(self):
        if self.path.is_dir():
            return self.path.name + '/'
        return self.path.name

    @classmethod
    def make_tree(cls, root, parent=None, is_last=False, criteria=None):
        from pathlib import Path
        
        root = Path(str(root))
        criteria = criteria or cls._default_criteria

        displayable_root = cls(root, parent, is_last)
        yield displayable_root

        children = sorted(list(path
                               for path in root.iterdir()
                               if criteria(path)),
                          key=lambda s: str(s).lower())
        count = 1
        for path in children:
            is_last = count == len(children)
            if path.is_dir():
                yield from cls.make_tree(path,
                                         parent=displayable_root,
                                         is_last=is_last,
                                         criteria=criteria)
            else:
                yield cls(path, displayable_root, is_last)
            count += 1

    @classmethod
    def _default_criteria(cls, path):
        return True

    @property
    def displayname(self):
        if self.path.is_dir():
            return self.path.name + '/'
        return self.path.name

    def displayable(self):
        if self.parent is None:
            return self.displayname

        _filename_prefix = (self.display_filename_prefix_last
                            if self.is_last
                            else self.display_filename_prefix_middle)

        parts = ['{!s} {!s}'.format(_filename_prefix,
                                    self.displayname)]

        parent = self.parent
        while parent and parent.parent is not None:
            parts.append(self.display_parent_prefix_middle
                         if parent.is_last
                         else self.display_parent_prefix_last)
            parent = parent.parent

        return ''.join(reversed(parts))
    
app_root=r'C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook'

from pathlib import Path
paths = DisplayablePath.make_tree(Path(app_root))
for path in paths:
    print(path.displayable())
```

    notebook/
    ├── .ipynb_checkpoints/
    │   ├── 1.3_数据库与数据分析基本流程组织-checkpoint.ipynb
    │   └── 2.1.1 数据POI与描述性统计和正态分布-checkpoint.ipynb
    ├── 1.3_数据库与数据分析基本流程组织.ipynb
    ├── 2.1.1 数据POI与描述性统计和正态分布.ipynb
    ├── __pycache__/
    │   ├── coordinate_transformation.cpython-38.pyc
    │   └── util_database.cpython-38.pyc
    ├── commodity_table_structure.py
    ├── coordinate_transformation.py
    ├── data/
    │   ├── AoT_Chicago.complete.2021-12-20/
    │   │   ├── nodes.csv
    │   │   ├── offsets.csv
    │   │   ├── provenance.csv
    │   │   ├── README.md
    │   │   └── sensors.csv
    │   ├── Chicago Community Areas/
    │   │   ├── Chicago Community Areas.dbf
    │   │   ├── Chicago Community Areas.prj
    │   │   ├── Chicago Community Areas.shp
    │   │   └── Chicago Community Areas.shx
    │   ├── poi_csv.csv
    │   └── poi_json.json
    ├── data_processed/
    ├── database/
    │   ├── AoT.1.3.sql
    │   ├── fruits.sqlite
    │   └── fruits_relational.sqlite
    ├── graph/
    ├── imgs/
    │   ├── 1_3_01.png
    │   ├── 1_3_02.jpg
    │   ├── 1_3_03.jpg
    │   ├── 1_3_04.jpg
    │   ├── 1_3_04.psd
    │   ├── 1_3_05.jpg
    │   └── 1_3_06.jpg
    ├── map/
    │   └── AoT.qgz
    ├── model/
    ├── sale_details_table_structure.py
    ├── sales_table_structure.py
    ├── util_database.py
    └── util_misc.py
    
