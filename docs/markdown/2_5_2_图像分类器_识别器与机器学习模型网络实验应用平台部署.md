> Created on  Jan 14 2019 @author: Richie Bao-caDesign设计(cadesign.cn)__+updated on Sun Dec 20 09:41:18 2020 by Richie Bao +updated on Sat Jan  8 15:17:34 2022 by Richie Bao 

# 2.5.2 图像分类器、识别器与机器学习模型网络实验应用平台部署

### 2.5.2.1 [Flask] 构建实验用网络应用平台

在很多场景中我们需要借助网络完成相关任务，例如展示研究内容，开展问卷调查收集数据，提供服务（例如机器学习或深度学习中已训练好的模型在线预测等）。因为这些任务需要更多的自由性，能够‘放任’的读写和处理数据，也需要根据不同的任务调整网页内容，因此使用类似[WIX](https://www.wix.com/)快速网页构建服务提供的方式很难满足数据分析、可视化、模型预测服务及布局调研信息等内容。那么[Flask英文](https://flask.palletsprojects.com/en/1.1.x/)，([中文](https://dormousehole.readthedocs.io/en/latest/))成为空间数据分析研究最好的网络实验构建平台。一方面Flask是应用python语言编写，是数据分析、大数据分析、机器学习和深度学习广泛使用的语言；同时，Flask是一个轻量级的可定制框架，灵活、轻便、安全，容易上手，能够快速的学习并实现自行搭建网络平台完成相关网络实验部署；Flask也并不限制使用何种数据库，何种模板样式，具有强劲的自由拓展性，实现自由的定制需求。

学习Flask推荐阅读[官方文档](https://flask.palletsprojects.com/en/1.1.x/)，以及教材*Flask Web Development: Developing Web Applications with Python(FWD)*（有中文版）。城市空间数据分析网络实验平台的建设，是以上述教材提供的案例为基础（包含社交博客搭建），在此之上扩展不同的实验任务，位于'Experiment'标签之下，网络实验的内容以及架构代码位于GitHub上[caDesign_ExperimentPlatform](https://github.com/richieBao/caDesign_ExperimentPlatform)代码仓库中。

Flask目前已经完全的整合进了[PyCharm](https://www.jetbrains.com/pycharm/)，因此推荐使用Pycharm构建Flask网络实验平台，无需自行搭建Flask的python环境，同时网页的开发Pycharm提供了非常友好的写代码的环境，能够节约研究者搭建的时间。FWD教材提供的案例电子邮件传输协议(SMTP)基于Google Gmail，如果在中国可以使用QQ邮箱提供的服务。FWD教材的写作方式是按照学习的过程递进的讲述，后续代码要基于前述的代码，因此对于初学者跳跃式的阅读并不是很好的选择。虽然递进的讲述符合学习的规律，但是作者并未给出整体的结构框架，丰富的代码类和函数不断的更新，变量之间关系的复杂性，容易让读者失去方向，因此给出代码工程的统一建模语言(Unified Modeling Language，UML)很必要。

借助代码逆向工程(Reverse Engineering)，使用[Visual Paradigm(VP)](https://www.visual-paradigm.com/)完成部分代码的逆向工程实现，目前除了VP，还有[Pyreverse](https://www.logilab.org/blogentry/6883)等大量逆向工程工具自动生成UML图表。但是因为代码的复杂性和结构的丰富性，目前很难找到自动生成全部关联的工具，因此下图对于FWD教材案例的UML图表绘制，大部分却是手工添加。对Flask结构的把握，主要包括A-配置+初始化（主程序）；B-模板和页面；C-路由与视图函数；D-Web表单；E-数据库模型(基于SQLAlchemy)；F-数据库(SQLite)，等7大部分。Flask的编写也是抓住这几个部分的关系，完成不同功能实现。因为代码书写的关键是不断的调试来验证已有代码是否顺利运行，达到书写的目的，而Flask是网页的开发，因此对于代码的验证，模板页面部分需要查看页面是否显示正常，而数据处理部分仍然可以不断的用pring()方法，打印的结果会显示在运行窗口下。建议代码书写前执行`set FLASK_DEBUG=1`，这样运行窗口下的错误提示，也将在页面中显示。对于简单的Flask开发，因为实现功能简单，因此实现函数通常位于同一文件之下，但是如果项目工程比较大，众多实现功能如果不加以明确区分，往往造成代码的混乱，因此Flask引入了蓝本(blueprint)的概念，简单讲就是将不同功能实现放置于不同的文件夹（包）下，并构建子文件夹下代码与主程序代码的关联，可以互相调用方法、函数和属性。网络实验部署是采用蓝图的大型应用工厂，使用一个应用进程得到多个应用实例，易于操作。对于Flask大型应用的把握，需要一开始查看文件夹的结构，这反映了当前应用是如果用Flask架构的。

对Flask的理解需要把握应用包(文件夹)的结构，蓝本实现的方法，配置与初始化的关系，路由与视图函数的关系，视图函数与模板页面的关系，Web表单与视图函数的关系，应用数据库模型(表单)读写数据库(SQLite)及与视图函数的关系，以及显示和隐式的代码关系。对隐式代码关系的理解尤为重要，因为突然冒出来的属性变量往往找不到源头，这因为Flask已经帮助完成了相关的任务，只给了输出。同时，因为涉及到页面模板，需要动态的读写视图函数的数据，同时也需要在页面模板内处理数据，Flask应用的[Jinjia](https://jinja.palletsprojects.com/en/2.11.x/)模板引擎实现这些功能，其语法也遵循大部分程序语言的结构。



<a href=""><img src="./imgs/2_5_2_01.jpg" height='auto' width='auto' title="caDesign"></a>

网络实验平台的工程名为'caDesign_ExperimentPlatform'（即根目录），配置(config.py)和主程序(caDesign_Experiment.py)，以及SQLite数据库(data-dev.sqlite)位于根目录下，migrations为数据库迁移文件夹（可以管理工程版本），test文件夹为测试内容，venv是应用PyCharm建立Flask工程时自动创建的python环境。所有应用位于app文件包下，static文件夹（系统生成）放置图片,.css等文件，templates文件夹（系统生成）放置.html的页面模板，main,auth为FWD教材社交博客的功能应用，data文件夹用于放置相关数据，visual_perception文件夹为视觉感知-基于图像空间分类实验的网络实现。

<a href=""><img src="./imgs/2_5_2_02.png" height='auto' width=800 title="caDesign"></a>

下述仅列出了主要的模板页面，类似博客等页面是嵌套在主页等模板页面中，为了便于将其部署于不同模板页面下，通常将此类模板设计成单独的子模板，其模板名为'_post.html'，方便迁移。此外还有403、404、500等错误页面模板。

<a href=""><img src="./imgs/2_5_2_03.jpg" height='auto' width='auto' title="caDesign"></a>

### 2.5.2.2 问卷调研

计算机视觉的发展应用领域日益广泛，例如机器人、无人驾驶、文档分析、医疗诊断等智能自主系统。其在规划设计领域的作用也日益凸显， 尤其百度、Google的街景图像，以及无人驾驶项目带来的大量序列图像和社交网络的图像，都推动着计算机视觉在规划领域潜在的应用前景。视觉感知部分包括系列实验，例如基于图像的空间分类与城市空间类别分布、图像分割下空间分类识别、视觉评价、绿量研究，以及遥感影像用地类型解译及依据标准的空间生成等内容。

基于图像的空间分类，方法一是应用Star、SIFT提取特征点（关键点）和描述子，聚类(K-Means)图像特征，进而建立视觉词袋（bag-of-words,BOW）。 BOW作为特征向量，输入到图像分类器（例如应用Extremely randomized trees, Extra-Trees/ET）进行训练。训练好的模型作为图像识别器预测新的图像，并应用到更广泛的城市区域内，通过预测的空间分类研究城市类别分布。这个基于图像分类的空间类型可以根据不同的目的进行分类，例如研究城市空间地面视野的郁密度，空间的开合程度，可以分类有林荫道、窄巷（步行为主）多建筑、窄巷有林木、宽道（1-2条）多建筑、宽道多林木、 干道（大于3条，4条居多）多建筑、干道多林木、干道开阔等。方法二是，并不计算图像特征，而是直接应用深度学习的方法训练模型。

如果将多项视觉感知的子项研究综合起来，以及结合非视觉感知类的分析技术，能够进一步拓展城市空间类型或感知的研究范畴。视觉感知-基于图像的空间分类：问卷调研部分，是使用KITTI数据集中的图像作为城市空间识别的素材，并以FWD教材案例为网络实验平台的基础，在此基础上扩展实验部分内容。其下代码是迁移了指定路径下返回所有文件夹及其下文件结构的代码，列出了'caDesign_ExperimentPlatform'网络实验平台下app应用文件夹的文件结构，在FWD教材案例基础上增加了文件夹（蓝本）visual_perception，该阶段包括'__init__.py','forms.py'和'views.py'3个文件，其中'forms.py'中基于'wtforms'库定义了问卷调研表格。因为单选按钮为纵向排列，并没有使用，而是直接在.html模板中直接自行定义。模板文件夹'templates'下新增了'vp'文件夹，该阶段包括'vp.html','imgs_classification.html'两个文件，分别为'视觉感知-基于图像的空间分类'的说明导航首页，及’参与图像分类‘的问卷调研页。


```python
from pathlib import Path
import util_misc    
app_root=r'C:\Users\richi\omen_richiebao\omen_github\caDesign_ExperimentPlatform\app'
paths=util_misc.DisplayablePath.make_tree(Path(app_root))
for path in paths:
    print(path.displayable())
```

    app/
    ├── __init__.py
    ├── __pycache__/
    │   ├── __init__.cpython-39.pyc
    │   ├── decorators.cpython-39.pyc
    │   ├── email.cpython-39.pyc
    │   ├── exceptions.cpython-39.pyc
    │   ├── fake.cpython-39.pyc
    │   └── models.cpython-39.pyc
    ├── auth/
    │   ├── __init__.py
    │   ├── __pycache__/
    │   │   ├── __init__.cpython-39.pyc
    │   │   ├── forms.cpython-39.pyc
    │   │   └── views.cpython-39.pyc
    │   ├── forms.py
    │   └── views.py
    ├── data/
    │   └── info_2011_09_26_drive_0009_sync.pkl
    ├── decorators.py
    ├── email.py
    ├── exceptions.py
    ├── fake.py
    ├── main/
    │   ├── __init__.py
    │   ├── __pycache__/
    │   │   ├── __init__.cpython-39.pyc
    │   │   ├── errors.cpython-39.pyc
    │   │   ├── forms.cpython-39.pyc
    │   │   └── views.cpython-39.pyc
    │   ├── errors.py
    │   ├── forms.py
    │   └── views.py
    ├── models.py
    ├── static/
    │   ├── favicon.ico
    │   ├── KITTI/
    │   │   ├── 2011_09_26_drive_0002_sync/
    │   │   │   ├── 0000000006.png
    │   │   │   ├── 0000000008.png
    │   │   │   ├── 0000000018.png
    │   │   │   ├── 0000000020.png
    │   │   │   ├── 0000000031.png
    │   │   │   ├── 0000000038.png
    │   │   │   ├── 0000000045.png
    │   │   │   └── 0000000069.png
    │   │   ├── 2011_09_26_drive_0005_sync/
    │   │   │   ├── 0000000001.png
    │   │   │   ├── 0000000006.png
        │   └── styles.css
    ├── templates/
    │   ├── 403.html
    │   ├── 404.html
    │   ├── 500.html
    │   ├── _comments.html
    │   ├── _macros.html
    │   ├── _posts.html
    │   ├── auth/
    │   │   ├── change_email.html
    │   │   ├── change_password.html
    │   │   ├── email/
    │   │   │   ├── change_email.html
    │   │   │   ├── change_email.txt
    │   │   │   ├── confirm.html
    │   │   │   ├── confirm.txt
    │   │   │   ├── reset_password.html
    │   │   │   └── reset_password.txt
    │   │   ├── login.html
    │   │   ├── register.html
    │   │   ├── reset_password.html
    │   │   └── unconfirmed.html
    │   ├── base.html
    │   ├── edit_post.html
    │   ├── edit_profile.html
    │   ├── followers.html
    │   ├── index.html
    │   ├── mail/
    │   │   ├── new_user.html
    │   │   └── new_user.txt
    │   ├── moderate.html
    │   ├── post.html
    │   ├── user.html
    │   └── vp/
    │       ├── img_prediction.html
    │       ├── imgs_classification.html
    │       └── vp.html
    ├── uploads/
    │   ├── 0000000020.png
    │   ├── 0000000020_1.png
    │   ├── 0000000030.png
    │   ├── 0000000069.png
    │   ├── 0000000069_1.png
    │   ├── 01.png
    │   ├── 02.png
    │   ├── 05.png
    │   └── 06.png
    └── visual_perception/
        ├── __init__.py
        ├── __pycache__/
        │   ├── __init__.cpython-39.pyc
        │   ├── forms.cpython-39.pyc
        │   ├── ImageTag_extractor.cpython-39.pyc
        │   └── views.cpython-39.pyc
        ├── forms.py
        ├── ImageTag_extractor.py
        ├── views.py
        └── vp_model/
            ├── ERF_clf.pkl
            ├── visual_BOW.pkl
            └── visual_feature.pkl
    

#### 1）配置*视觉感知-基于图像的空间分类*蓝本

当建立一个新的子项目时，是在app下建立一个独属的文件夹（蓝本/子包），此处视觉感知实验的蓝本文件夹名为'visual_perception'。并在子包中新建__init__.py文件，调入'Blueprint'，实例化一个蓝本类对象，并调入views(.py)，把路由与蓝本关联起来，即在全局作用域下可以使用该蓝本下的view.py下视图函数及相关方法。同时，需要在app下的__init__.py文件内对应增加配置。


```python
#app/visual_perception/__init__.py
from flask import Blueprint

visual_perception=Blueprint('visual_perception', __name__) #参数：蓝本名称，和蓝本所在的包或模块，默认使用__name__。
from . import views
```


```python
#app/__init__.py
from .visual_perception import visual_perception as vp_blueprint
app.register_blueprint(vp_blueprint, url_prefix='/vp')
```

#### 2）定义SQLite表结构，写入图像信息

* 定义SQLite表结构

定义两个表结构（模型），`class vp_imgs(db.Model)`用于存储原始图像的信息，包括图像路径（位于static文件夹下），经纬度和高程信息。`class vp_classification(db.Model)`表结构，用于存储分类的结果，定义有8个分类，对应c1-c8，同时也引入图像路径字段'imgs_fp'，以及分类结果'classification'字段。一开始时，只有'vp_imgs'表中有数据，是处理好的图像信息，直接写入到该表中。而'vp_classification'表没有数据，只有在模板页面中点击了单选按钮，选择分类提交后，将该信息写入到该表中。同时这两个表之间建立了1对1的关系，指定的外键为`db.ForeignKey('vp_imgs.index')`，并建立了双向关系('back_populates="vp_imgs"'，和'back_populates="vp_classification"')，这样可以之间通过一个表的信息读取另一个表的字段信息。例如在'imgs_classification.html'模板中，`{{ img_info.vp_classification.classification}}`的Jinja语句下通过"vp_imgs"表的一个行信息'img_info'，链接到"vp_classification"表中的分类信息'classification'，从而可以在页面中显示当前图像的分类信息。

定义好表结构后，可以在Pycharm的Terminal终端下敲入：`flask shell`启动shell会话，然后执行`from caDesign_Experiment import db`调入数据库实例化对象db，'caDesign_Experiment'为对应的app名，再执行`db.create_all()`，将新建的表结构写入到SQLite数据库（属于库已有的表保持不变）。


```python
#app/models.py
class vp_imgs(db.Model):
    __tablename__ = 'vp_imgs'
    index = db.Column(db.Integer, primary_key=True, autoincrement=True)
    imgs_fp=db.Column(db.Text,unique=True,nullable=False)
    lat=db.Column(db.Float)
    lon=db.Column(db.Float)
    alt=db.Column(db.Float)
    vp_classification=db.relationship('vp_classification',uselist=False, back_populates="vp_imgs")

class vp_classification(db.Model):
    __tablename__ = 'vp_classification'
    id=db.Column(db.Integer,primary_key=True,autoincrement=True)
    imgs_fp=db.Column(db.Text,unique=True,nullable=False)
    c_1 = db.Column(db.Integer)
    c_2 = db.Column(db.Integer)
    c_3 = db.Column(db.Integer)
    c_4 = db.Column(db.Integer)
    c_5 = db.Column(db.Integer)
    c_6 = db.Column(db.Integer)
    c_7 = db.Column(db.Integer)
    c_8 = db.Column(db.Integer)
    classification=db.Column(db.Text)
    timestamp=db.Column(db.DateTime,default=datetime.now)
    #index = db.Column(db.Integer)
    index=db.Column(db.Integer,db.ForeignKey('vp_imgs.index'))
    vp_imgs=db.relationship('vp_imgs',back_populates="vp_classification")
```

* 写入图像信息

先定义表结构，并写入数据库后再向表中写入数据，而不是直接应用pandas的方法直接默认表结构写入到数据库。这是因为要建立表之间的关系，默认的方式则无法建立表关系。定义函数`imgs_compression_cv`实现图像的压缩，因为图像要在网络页面中显示，较大的图像加载速度慢，影响体验。函数`KITTI_info_gap`是针对KITTI数据集的操作，因为该数据集是用于无人驾驶场景下计算机视觉算法评测数据集，图像连续，因此通过该函数可以处理KITTI提供的.txt（包含经纬度，高程等信息）文件，保持数据与图像压缩函数给定的参数`gap`保持一致，即隔一段距离提取一张图像。`KITTI_info2sqlite`函数，则是将提取的图像信息写入到数据库表中，因为表以及存在，因此使用`method="append"`方法。


```python
# -*- coding: utf-8 -*-
"""
Created on Wed Dec 16 10:11:02 2020

@author: Richie Bao-caDesign设计(cadesign.cn).Chicago
"""
def imgs_compression_cv(imgs_root,imwrite_root,imgsPath_fp,gap=1,png_compression=9,jpg_quality=100):
    from pathlib import Path
    import cv2 as cv
    import numpy as np
    from tqdm import tqdm
    import os
    import pandas as pd
    '''
    function - 使用OpenCV的方法压缩保存图像    
    
    Paras:
    imgs_root - 待处理的图像文件根目录
    imwrite_root - 图像保存根目录
    gap - 无人驾驶场景下的图像通常是紧密连续的，可以剔除部分图像避免干扰， 默认值为1
    png_compression - png格式压缩值，默认为9,
    jpg_quality - jpg格式压缩至，默认为100    
    
    jpg_quality: for jpeg only. 0 - 100 (higher means better). Default is 95.
    png_compression: For png only. 0 - 9 (higher means a smaller size and longer compression time).
    '''
    if not os.path.exists(imwrite_root):
        os.makedirs(imwrite_root)
    
    imgs_root=Path(imgs_root)
    imgs_fp=[p for p in imgs_root.iterdir()][::gap]
    imgs_save_fp=[]
    for img_fp in tqdm(imgs_fp):
        img_save_fp=str(Path(imwrite_root).joinpath(img_fp.name))
        img=cv.imread(str(img_fp ))
        if img_fp.suffix=='.png':
            cv.imwrite(img_save_fp,img,[int(cv.IMWRITE_PNG_COMPRESSION), png_compression])
            imgs_save_fp.append(img_save_fp)
        elif img_fp.suffix=='.jpg':
            cv.imwrite(img_save_fp,img,[int(cv.IMWRITE_JPEG_QUALITY), jpg_quality])
            imgs_save_fp.append(strimg_save_fp)
        else:
            print("Only .jpg and .png format files are supported.")
   
    pd.DataFrame(imgs_save_fp,columns=['imgs_fp']).to_pickle(imgsPath_fp)
    return imgs_save_fp
 
def KITTI_info_gap(KITTI_info_fp,save_fp,gap=1):
    import pandas as pd
    from pathlib import Path
    '''
    function - 读取KITTI文件信息，1-包括经纬度，惯性导航系统信息等的.txt文件。只返回经纬度、海拔信息
    '''

    txt_root=Path(KITTI_info_fp)
    txt_fp=[str(p) for p in txt_root.iterdir()][::gap]
    # print(txt_fp)
    columns=["lat","lon","alt","roll","pitch","yaw","vn","ve","vf","vl","vu","ax","ay","ay","af","al","au","wx","wy","wz","wf","wl","wu","pos_accuracy","vel_accuracy","navstat","numsats","posmode","velmode","orimode"]
    drive_info=pd.concat([pd.read_csv(item,delimiter=' ',header=None) for item in txt_fp],axis=0)
    drive_info.columns=columns
    drive_info=drive_info.reset_index()    
    
    drive_info_coordi=drive_info[["lat","lon","alt"]]
    drive_info_coordi.to_pickle(save_fp)

    return drive_info_coordi

def KITTI_info2sqlite(imgsPath_fp,info_fp,replace_path,db_fp,table,method='fail'):
    from pathlib import Path
    import pandas as pd
    from sqlalchemy import create_engine  
    '''
    function - 将KITTI图像路径与经纬度信息对应起来，并存入SQLite数据库
    
    Paras:
    imgsPath_fp,
    info_fp,
    replace_path,
    db_fp,field,
    method='fail'    
    
    if_exists{‘fail’, ‘replace’, ‘append’}, default ‘fail’
    '''
    imgsPath=pd.read_pickle(imgsPath_fp)
    #flask Jinja的url_for仅支持'/,因此需要替换'\\'
    imgsPath_replace=imgsPath.imgs_fp.apply(lambda row:str(Path(replace_path).joinpath(Path(row).name)).replace('\\','/'))
    # print(imgsPath_replace)
    info=pd.read_pickle(info_fp)
    imgs_df=pd.concat([imgsPath_replace,info],axis=1)
    # print(imgs_df)
    
    engine=create_engine('sqlite:///'+'\\\\'.join(db_fp.split('\\')),echo=True)     
    # print(engine)
    # print("_"*50)
       
    try:
        imgs_df.to_sql('%s'%table,con=engine,index=False,if_exists="%s"%method)
        print("if_exists=%s:------Data has been written to the database!"%method)
    except:
        print("_"*15,'the %s table has been existed...'%table)


if __name__=="__main__":
    imgs_paths=[r'2011_09_26_drive_0009_sync',
                r'2011_09_29_drive_0071_sync',
                r'2011_09_28_drive_0001_sync',
                r'2011_09_29_drive_0026_sync',
                r'2011_09_28_drive_0002_sync',
                r'2011_09_26_drive_0117_sync',
                r'2011_09_26_drive_0113_sync',
                r'2011_09_26_drive_0106_sync',
                r'2011_09_26_drive_0104_sync',
                r'2011_09_26_drive_0096_sync',
                r'2011_09_26_drive_0095_sync',
                r'2011_09_26_drive_0093_sync',
                r'2011_09_26_drive_0084_sync',
                r'2011_09_26_drive_0060_sync',
                r'2011_09_26_drive_0059_sync',
                r'2011_09_26_drive_0057_sync',
                r'2011_09_26_drive_0056_sync',
                r'2011_09_26_drive_0051_sync',
                r'2011_09_26_drive_0048_sync',
                r'2011_09_26_drive_0018_sync',
                r'2011_09_26_drive_0017_sync',
                r'2011_09_26_drive_0014_sync',
                r'2011_09_26_drive_0013_sync',
                r'2011_09_26_drive_0011_sync',
                r'2011_09_26_drive_0005_sync',
                r'2011_09_26_drive_0002_sync',                
                ]
    g=10
    i=0
    for p in imgs_paths:
        print("\n%d-%s---处理中..."%(-(len(imgs_paths[1:])-i),p))
        imgs_path=p
        #A - 使用OpenCV的方法压缩保存图像 
        imgs_root=r'D:\dataset\KITTI\%s\image_03\data'%imgs_path
        imwrite_root=r'D:\dataset\KITTI\imgs_compression\%s'%imgs_path
        imgsPath_fp=r'D:\dataset\KITTI\imgs_compression\imgsPath_%s.pkl'%imgs_path
        imgs_save_fp=imgs_compression_cv(imgs_root,imwrite_root,imgsPath_fp,gap=g)
        
        #B - 读取KITTI经纬度信息，可以指定间隔提取距离
        KITTI_info_fp=r'D:\dataset\KITTI\%s\oxts\data'%imgs_path
        save_fp=r'D:\dataset\KITTI\imgs_compression\info_%s.pkl'%imgs_path
        drive_info=KITTI_info_gap(KITTI_info_fp,save_fp,gap=g)
        
        #C - 将文件路径信息写入数据库
        imgsPath_fp=r'D:\dataset\KITTI\imgs_compression\imgsPath_%s.pkl'%imgs_path
        info_fp=r'D:\dataset\KITTI\imgs_compression\info_%s.pkl'%imgs_path
        replace_path=r'KITTI\%s'%imgs_path
        db_fp=r'C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\data-dev.sqlite'
        KITTI_info2sqlite(imgsPath_fp,info_fp,replace_path,db_fp,table='vp_imgs',method="append") 
        
        i+=1
```

#### 3）定义路由和视图函数

配置完蓝本之后，在'app/visual_perception/views.py'文件下定义视图函数，指定路由（对应的模板页面）。首先可以定义一个简单的视图函数，例如Flask官方那个最简单的代码来测试是否蓝本配置成功：

```python
@app.route('/hello')
def hello_world():
    return 'Hello, World!'
```

只是需要重新分配一个路由，例如上述修改为`@app.route('/hello')`，这样可以在`http://127.0.0.1:5000/hello`统一资源定位系统(Uniform Resource Locater,URL)，即网页地址下打开，如果返回显示'Hello, World!'，则可以说明蓝本配置无误。目前包括两个模板页面'vp.html','imgs_classification.html'，分别对应视图函数`vp()`和`imgs_classification()`。vp视图函数网页地址（路由）指向`"/vp"`,对应模板'vp.html'下的内容比较简单，只是发布该实验项目的说明，并向模板中传入了`current_time=datetime.datetime.utcnow()`参数，可以在模板页面下显示本地时间。imgs_classification视图函数页面指向"/imgs_classification"，对应模板页面'imgs_classification.html'，因为对图像分类，需要读写数据库，以及显示图像，表单提交等动作，要稍显复杂。

首先确定分类的标准，将街道空间划分为：1-林荫道、2-窄巷（步行为主）多建筑、3-窄巷有林木、4-宽道（1-2条）多建筑、5-宽道多林木、 6-干（阔）道（大于3条，4条居多）多建筑、7-干（阔）道多林木、8-干（阔）道开阔。 分别标识为：林荫、窄建、窄木、宽建、宽木、阔建、阔木，及开阔，总共8类。根据分类信息可以在模板中定义表单，以及定义对应的数据库表结构。视图函数则需要思考'问卷调研'的动作，1-首先是读取图像路径信息，传入模板后可以显示图像；2-每一图像下对应表单，有8个单选按钮，当选择其中之一后，点击提交按钮，表单信息将返回到视图函数中(POST)；3-在视图函数中读取表单信息，将其写入到数据库中（这涉及到数据表结构的设计），不同人点击的分类可能不同，当单击一个分类，则对应写入该分类的数据库表字段中，计数方式为累加。确定图像的最终分类是对应哪个分类的累加数最多；4-因为对图像给了分类，分类信息也被写入到对应表中，则可以将分类结果显示在页面中。


```python
#app/visual_perception/views.py
from . import visual_perception
from flask import render_template,url_for, request, session,current_app,redirect
import datetime
from .. import db
from ..models import vp_imgs,vp_classification
import pandas as pd
from .forms import imgs_classi

@visual_perception.route("/vp",methods=['GET','POST'])
def vp():

    return render_template('vp/vp.html',current_time=datetime.datetime.utcnow())

@visual_perception.route("/imgs_classification",methods=['GET','POST'])
def imgs_classification():
    #form=imgs_classi()

    page=request.args.get('page', 1, type=int)
    query=vp_imgs.query
    pagination=query.order_by(vp_imgs.index).paginate(page, per_page=current_app.config['FLASKY_POSTS_PER_PAGE'],error_out=False)
    imgs_info=pagination.items

    vp_classi=vp_classification.query.all()
    #print("_"*50)
    #exist=db.session.query(db.exists().where(vp_classification.index == 0)).scalar()
    #print(exist)

    if request.method == 'GET':
        return render_template('vp/imgs_classification.html', imgs_info=imgs_info,vp_classi=vp_classi,pagination=pagination) #form=form,
    else:
        img_index=request.form.get('img_index')
        img_fp=request.form.get('img_fp')
        classi=int(request.form.get('classi'))
        img_current=vp_classification.query.filter(vp_classification.imgs_fp==img_fp).first()
        classi_dic_value={1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0}
        classi_dic_name={1:u'林荫',2:u'窄建',3:u'窄木',4:u'宽建',5:u'宽木',6:u'阔建',7:u'阔木',8:u'开阔'} #中文一定要加u，即unicode( str_name )，否则服务器段如果是py2.7，会提示错误。
        classi_dic_value.update({classi:1})
        img_classification=classi_dic_name[classi]
        if not img_current:
            img_classi_info=vp_classification(imgs_fp=img_fp,
                                              c_1=classi_dic_value[1],
                                              c_2=classi_dic_value[2],
                                              c_3=classi_dic_value[3],
                                              c_4=classi_dic_value[4],
                                              c_5=classi_dic_value[5],
                                              c_6=classi_dic_value[6],
                                              c_7=classi_dic_value[7],
                                              c_8=classi_dic_value[8],
                                              classification=img_classification,
                                              index=img_index)
            db.session.add(img_classi_info)
            db.session.commit()
        else:
            query_results=[{1:c_1,2:c_2,3:c_3,4:c_4,5:c_5,6:c_6,7:c_7,8:c_8} for c_1,c_2,c_3,c_4,c_5,c_6,c_7,c_8 in db.session.query(vp_classification.c_1,vp_classification.c_2,vp_classification.c_3,vp_classification.c_4,vp_classification.c_5,vp_classification.c_6,vp_classification.c_7,vp_classification.c_8).filter(vp_classification.imgs_fp==img_fp)][0]
            query_results_update=pd.DataFrame.from_dict(query_results,orient='index').add(pd.DataFrame.from_dict(classi_dic_value,orient='index'))
            img_classification_=classi_dic_name[query_results_update.idxmax()[0]]
            query_results_update_dic=query_results_update.squeeze('columns').to_dict()
            query_results_update_dic.update({'classification':img_classification_})
            query_results_update_dic_=dict(zip(['c_1','c_2','c_3','c_4','c_5','c_6','c_7','c_8','classification'],query_results_update_dic.values()))
            vp_classification.query.filter_by(imgs_fp=img_fp).update(query_results_update_dic_)
            db.session.commit()

    return render_template('vp/imgs_classification.html',imgs_info=imgs_info,vp_classi=vp_classi,pagination=pagination) #,form=form 
```

#### 4）定义模板

模板的定义中需要注意对Jinja(2)的使用，[Jinja](https://jinja.palletsprojects.com/en/2.11.x/)是现代而又设计友好用于python的模板语言，起源于'Django'。


```python
<!--templates/vp/vp.html --> 
{% extends "base.html" %}
{% import "bootstrap/wtf.html" as wtf %}

{% block title %}caDesign - visual perception{% endblock %}

{% block page_content %}
    <div class="jumbotron">
        <h1>视觉感知-基于图像的空间分类</h1>
        <p>
            计算机视觉的发展应用邻域日益广泛，例如机器人、无人驾驶、文档分析、医疗诊断等智能自主系统。其在规划设计领域的作用也日益凸显，
            尤其百度、Google的街景图像，以及无人驾驶项目带来的大量序列图像和社交网络的图像，都推动着计算机视觉在规划领域潜在的应用前景。
            视觉感知部分包括系列实验，例如基于图像的空间分类与城市空间类别分布、图像分割下空间分类识别、视觉评价、绿量研究，以及遥感影像用地类型
            解译及依据标准的空间生成等内容。
        </p>
        <p>
            基于图像的空间分类，方法一是应用Star、SIFT提取特征点（关键点）和描述子，聚类(K-Means)图像特征，进而建立视觉词袋（bag-of-words,BOW）。
            BOW作为特征向量，输入到图像分类器（例如应用Extremely randomized trees, Extra-Trees/ET）进行训练。训练好的模型作为图像识别器
            预测新的图像，并应用到更广泛的城市区域内，通过预测的空间分类研究城市类别分布。这个基于图像分类的空间类型可以根据不同的目的进行分类，例如
            研究城市空间地面视野的郁密度，空间的开合程度，可以分类有林荫道、窄巷（步行为主）多建筑、窄巷有林木、宽道（1-2条）多建筑、宽道多林木、
            干道（大于3条，4条居多）多建筑、干道多林木、干道开阔等。方法二是，并不计算图像特征，而是直接应用深度学习()的方法训练模型。
        </p>
        <p>
            如果将多项视觉感知的子项研究综合起来，以及结合非视觉感知类的分析技术，能够进一步拓展城市空间类型或感知的研究范畴。
        </p>

        <p>
            <a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.imgs_classification') }}" role="button">参与图像分类</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">预测图像分类</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">空间类型分布</a>
        </p>
        <p>本地时间：{{ moment(current_time).format('LLL') }}</p>
        <p>{{ moment(current_time).fromNow(refresh=True) }}</p>
    </div>
{% endblock %}
```


```python
<!--templates/vp/imgs_classification.html --> 
{% extends "base.html" %}
{% import "bootstrap/wtf.html" as wtf %}
{% import "_macros.html" as macros %}

{% block title %}caDesign - visual perception{% endblock %}

{% block page_content %}

    <div class="jumbotron">
        <h3>参与图像分类</h3>
        <p>
            将街道空间划分为：1-林荫道、2-窄巷（步行为主）多建筑、3-窄巷有林木、4-宽道（1-2条）多建筑、5-宽道多林木、 6-干（阔）道（大于3条，4条居多）多建筑、7-干（阔）道多林木、8-干（阔）道开阔。
            分别标识为：林荫、窄建、窄木、宽建、宽木、阔建、阔木，及开阔，总共8类。
        </p>

        <h6>@ARTICLE{Geiger2013IJRR, author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun}, title = {Vision meets Robotics: The KITTI Dataset}, journal = {International Journal of Robotics Research (IJRR)}, year = {2013} }</h6>

        <p>
            <a class="btn btn-secondary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">实验主页</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">预测图像分类</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">空间类型分布</a>
        </p>
    </div>

    <ul class="question-list-group">
        {% for img_info in imgs_info %}
            <li style="float:left">
                <div class="row">
                    <div class=" col-md-10">
                        <div class="thumbnail">
                            <img src="{{ url_for('static',filename=img_info.imgs_fp[7:]) }}" alt="">
                            <div class="caption">
                                <h4>ID：{{ img_info.index }}
                                    {{ img_info.vp_classification.classification}}
                                </h4>
                                        <iframe name="formDestination" class="iframe", style="display:none;"></iframe>
                                        <form action="" method="post" target="formDestination">
                                        <input type="radio" name="classi" value="1"/>林荫&nbsp
                                        <input type="radio" name="classi" value="2"/>窄建&nbsp
                                        <input type="radio" name="classi" value="3"/>窄木&nbsp
                                        <input type="radio" name="classi" value="4"/>宽建&nbsp
                                        <input type="radio" name="classi" value="5"/>宽木&nbsp
                                        <input type="radio" name="classi" value="6"/>阔建&nbsp
                                        <input type="radio" name="classi" value="7"/>阔木&nbsp
                                        <input type="radio" name="classi" value="8"/>开阔&nbsp

                                        <input type="hidden" name="img_index" value="{{img_info.index }}">
                                        <input type="hidden" name="img_fp" value="{{ img_info.imgs_fp}}">

                                        <input type="submit" value="提交" class="btn btn-secondary btn-sm" onClick="this.form.submit(); this.disabled=true; this.value='已提交'; ">
                                   </form>
                            </div>
                        </div>
                    </div>

                </div>
            </li>
        {% endfor %}
    </ul>

{% if pagination %}
<div class="pagination">
    {{ macros.pagination_widget(pagination, 'visual_perception.imgs_classification') }}
</div>
{% endif %}

{% endblock %}
```

<a href=""><img src="./imgs/2_5_2_04.png" height='auto' width=800 title="caDesign"></a>

### 2.5.2.3  视觉词袋与构建图像映射特征

计算每一张图像的关键点描述子（一个关键点描述子有128维，一幅图像有多个关键点），把所有图像的关键点描述子集合在一起，就是所有图像特征的集合。因为每一关键点描述子均不同，因此使用聚类的方法指定聚类的数量（32）聚合特征，即聚合所有图像的关键描述子为指定数量构造码本/BOW,Bag of Words（码本是将所有可用的码/聚类放在一起，组成类似字典的表，用序号给不同的码编号/或者是列表的排序。如果要对一个单词/或一句话编码，则可以应用码本编号），并建立聚类模型。该码本就是所有图像特征（聚类后）的集合，因为每一图像的关键点描述子都可以在该码本中找到对应的特征（32个聚类的1个或多个聚类组合），因此可以用码本编码每一图像（用聚类模型预测）。保持码本的形状（shape，32），将一幅图像的所有关键点描述子对应到码本的编号上，因为聚类的结果，一个编号通常对应有多个关键点，计算所有对应到编号上关键点的频数（有的编码频数也会为0，即没有对应的关键点），这个对应码本编号频数的结果就反映了该图像特征。

<a href=""><img src="./imgs/2_5_2_05.jpg" height='auto' width='1200' title="caDesign"></a>

读取'问卷调研'的数据库结果。因为数据库中存储的图像路径为相对路径，而此时的代码可能位于他处，因此将其转换为绝对路径。


```python
import sqlite3
import pandas as pd
db_fp=r'C:\Users\richi\omen_richiebao\omen_github\caDesign_ExperimentPlatform\data-dev.sqlite'
vp_classification=pd.read_sql_table('vp_classification', 'sqlite:///%s'%db_fp) #pd.read_sql_table从数据库中读取指定的表

import util_misc
import os
vp_classification['absolute_imags_fp']=vp_classification.apply(lambda row:os.path.join(r'C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static',row.imgs_fp),axis=1)
util_misc.print_html(vp_classification)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>imgs_fp</th>
      <th>c_1</th>
      <th>c_2</th>
      <th>c_3</th>
      <th>c_4</th>
      <th>c_5</th>
      <th>c_6</th>
      <th>c_7</th>
      <th>c_8</th>
      <th>classification</th>
      <th>timestamp</th>
      <th>index</th>
      <th>absolute_imags_fp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>KITTI/2011_09_26_drive_0009_sync/0000000000.png</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>宽木</td>
      <td>2020-12-20 13:10:36.725045</td>
      <td>1</td>
      <td>C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static\KITTI/2011_09_26_drive_0009_sync/0000000000.png</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>KITTI/2011_09_26_drive_0009_sync/0000000010.png</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>宽木</td>
      <td>2020-12-20 13:10:42.138661</td>
      <td>2</td>
      <td>C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static\KITTI/2011_09_26_drive_0009_sync/0000000010.png</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>KITTI/2011_09_26_drive_0009_sync/0000000020.png</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>宽木</td>
      <td>2020-12-20 13:10:46.728137</td>
      <td>3</td>
      <td>C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static\KITTI/2011_09_26_drive_0009_sync/0000000020.png</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>KITTI/2011_09_26_drive_0009_sync/0000000030.png</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>宽木</td>
      <td>2020-12-20 13:10:55.079670</td>
      <td>4</td>
      <td>C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static\KITTI/2011_09_26_drive_0009_sync/0000000030.png</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>KITTI/2011_09_26_drive_0009_sync/0000000040.png</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>宽木</td>
      <td>2020-12-20 13:10:58.771853</td>
      <td>5</td>
      <td>C:\Users\richi\omen-richiebao_s\omen_github\caDesign_ExperimentPlatform\app\static\KITTI/2011_09_26_drive_0009_sync/0000000040.png</td>
    </tr>
  </tbody>
</table>




```python
print("分类频数统计：\n",vp_classification.classification.value_counts())
```

    分类频数统计：
     阔木    189
    窄建    188
    宽木    104
    开阔     85
    窄木     78
    宽建     60
    阔建     47
    林荫     47
    Name: classification, dtype: int64
    

以一张图像的分类结果为键，以图像绝对路径为名建立对应到一张图像的字典。然后将所有图像的字典放置于一个列表中，用于构造所有图像的码本和计算每一图像的特征映射。


```python
def load_training_data(imgs_df,classi_field,classi_list,path_field):
    import pandas as pd
    '''
    function - 按照分类提取图像路径与规律
    
    Paras:
        imgs_df - 由pandas读取的数据库表数据，含分类信息
    '''
    imgs_group=imgs_df.groupby(['classification'])
    training_data=[[{'object_class':classi,'image_path':row} for row in imgs_group.get_group(classi)[path_field].tolist()] for classi in classi_list]
    flatten_lst=lambda lst: [m for n_lst in lst for m in flatten_lst(n_lst)] if type(lst) is list else [lst]
    return flatten_lst(training_data)

classi_list=['林荫','窄建','窄木','宽建','宽木','阔建','阔木','开阔']   
classi_field='classification'    
path_field='absolute_imags_fp'
training_data=load_training_data(vp_classification,classi_field,classi_list,path_field)    
print(training_data[:5])
```

    [{'object_class': '林荫', 'image_path': 'C:\\Users\\richi\\omen-richiebao_s\\omen_github\\caDesign_ExperimentPlatform\\app\\static\\KITTI/2011_09_26_drive_0117_sync/0000000657.png'}, {'object_class': '林荫', 'image_path': 'C:\\Users\\richi\\omen-richiebao_s\\omen_github\\caDesign_ExperimentPlatform\\app\\static\\KITTI/2011_09_26_drive_0117_sync/0000000036.png'}, {'object_class': '林荫', 'image_path': 'C:\\Users\\richi\\omen-richiebao_s\\omen_github\\caDesign_ExperimentPlatform\\app\\static\\KITTI/2011_09_26_drive_0117_sync/0000000354.png'}, {'object_class': '林荫', 'image_path': 'C:\\Users\\richi\\omen-richiebao_s\\omen_github\\caDesign_ExperimentPlatform\\app\\static\\KITTI/2011_09_26_drive_0117_sync/0000000038.png'}, {'object_class': '林荫', 'image_path': 'C:\\Users\\richi\\omen-richiebao_s\\omen_github\\caDesign_ExperimentPlatform\\app\\static\\KITTI/2011_09_26_drive_0117_sync/0000000196.png'}]
    

如果代码量比较大，逐行的分析代码之间互相调用的关系是费时和费力的事情，因此借助逆向工程。代码逆向工程的工具很多，这里使用了[Sourcetrail](https://www.sourcetrail.com/#intro)工具。定义`feature_builder_BOW`构造视觉码本和提取图像映射特征的类之后，主要调用了两个方法，先用`feature_builder_BOW().get_visual_BOW(training_data,)`构造码本，再用 `feature_builder_BOW().get_feature_map(training_data,kmeans)`返回每一图像映射特征。从下述逆向工程分析结果能够很清楚的梳理出两次类方法的调用所关联调用的其它类中函数。`.get_visual_BOW`函数内调用有`extract_features`和`visual_BOW`两个函数，同时显示了调用的内嵌方法，print,enumerate,tqdm等；`.get_feature_map`则调用有`extract_features`和`normalize`两个函数，同时使用了类变量` self.num_clusters`。同时，也可以查看`extract_features`分别被`.get_visual_BOW`和`.get_feature_map`调用。

<a href=""><img src="./imgs/2_5_2_06.png" height='auto' width='1200' title="caDesign"></a>


```python
class feature_builder_BOW:
    '''
    class - 根据所有图像关键点描述子聚类建立图像视觉词袋，获取每一图像的特征（码本）映射的频数统计
    '''   
    def __init__(self,num_cluster=32):
        self.num_clusters=num_cluster

    def extract_features(self,img):
        import cv2 as cv
        '''
        function - 提取图像特征
        
        Paras:
            img - 读取的图像
        '''
        star_detector=cv.xfeatures2d.StarDetector_create()
        key_points=star_detector.detect(img)
        img_gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)
        kp,des=cv.xfeatures2d.SIFT_create().compute(img_gray, key_points) #SIFT特征提取器提取特征
        return des
    
    def visual_BOW(self,des_all):
        from sklearn.cluster import KMeans
        '''
        function - 聚类所有图像的特征（描述子/SIFT），建立视觉词袋
        
        Paras:
            des_all - 所有图像的关键点描述子
        '''
        print("start KMean...")
        kmeans=KMeans(self.num_clusters)
        kmeans=kmeans.fit(des_all)
        #centroids=kmeans.cluster_centers_
        print("end KMean...")
        return kmeans         
    
    def get_visual_BOW(self,training_data):
        import cv2 as cv
        from tqdm import tqdm
        '''
        function - 提取图像特征，返回所有图像关键点聚类视觉词袋
        
        Paras:
            training_data - 训练数据集
        '''
        des_all=[]
        #i=0        
        for item in tqdm(training_data):            
            classi_judge=item['object_class']
            img=cv.imread(item['image_path'])
            #print(item['image_path'])
            #print(img)
            des=self.extract_features(img)
            des_all.extend(des)           
            #print(des.shape)
            #if i==10:break
            #i+=1        
        kmeans=self.visual_BOW(des_all)      
        return kmeans
    
    def normalize(self,input_data):
        import numpy as np
        '''
        fuction - 归一化数据
        
        Paras:
            input_data - 待归一化的数组
        '''
        sum_input=np.sum(input_data)
        if sum_input>0:
            return input_data/sum_input #单一数值/总体数值之和，最终数值范围[0,1]
        else:
            return input_data               
    
    def construct_feature(self,img,kmeans):
        import numpy as np
        '''
        function - 使用聚类的视觉词袋构建图像特征（构造码本）
        
        Paras:
            img - 读取的单张图像
            kmeans - 已训练的聚类模型
        '''
        des=self.extract_features(img)
        labels=kmeans.predict(des.astype(float)) #对特征执行聚类预测类标
        feature_vector=np.zeros(self.num_clusters)
        for i,item in enumerate(feature_vector): #计算特征聚类出现的频数/直方图
            feature_vector[labels[i]]+=1
        feature_vector_=np.reshape(feature_vector,((1,feature_vector.shape[0])))
        return self.normalize(feature_vector_)
    
    def get_feature_map(self,training_data,kmeans):
        import cv2 as cv
        '''
        function - 返回每个图像的特征映射（码本映射）
        
        Paras:
            training_data - 训练数据集
            kmeans - 已训练的聚类模型
        '''
        feature_map=[]
        for item in training_data:
            temp_dict={}
            temp_dict['object_class']=item['object_class']
            #print("Extracting feature for",item['image_path'])
            img=cv.imread(item['image_path'])
            temp_dict['feature_vector']=self.construct_feature(img,kmeans)
            if temp_dict['feature_vector'] is not None:
                feature_map.append(temp_dict)
        #print(feature_map[0]['feature_vector'].shape,feature_map[0])
        return feature_map
import util_misc
s_t=util_misc.start_time()

#修改发生了改变的路径
training_data_=[]
for i in training_data:
    path_update=i['image_path'].replace(r"C:\Users\richi\omen-richiebao_s",r"C:\Users\richi\omen_richiebao")
    i.update({'image_path':path_update})
    training_data_.append(i)

kmeans=feature_builder_BOW().get_visual_BOW(training_data_,)       
print("_"*50)

import pickle
with open('./model/visual_BOW.pkl','wb') as f: # 使用with结构避免手动的文件关闭操作
    pickle.dump(kmeans,f) #存储kmeans聚类模型
    
feature_map=feature_builder_BOW().get_feature_map(training_data_,kmeans)    
with open('./model/visual_feature.pkl','wb') as f:
    pickle.dump(feature_map,f) #存储图像特征

util_misc.duration(s_t)
```

    start time: 2022-01-08 18:01:30.911273
    

    100%|██████████| 798/798 [00:57<00:00, 13.91it/s]
    

    start KMean...
    end KMean...
    __________________________________________________
    end time: 2022-01-08 18:08:38.278630
    Total time spend:7.12 minutes
    

### 2.5.2.4  决策树（Decision trees）与随机森林（Random forests）

#### 1）决策树

*Mastering Machine Learning with scikit-learn* 对决策树和随机森林算法有清晰易于理解的阐述，这里以其为蓝本，应用案例数据，辅助python计算和应用Sklearn库加以说明。首先录入猫狗分类的特征数据集。所录入的数据为文本类型，将其通过pandas库提供的`pd.get_dummies`方法转换为独热编码(One-Hot Encoding)，主要应用于数据集的特征列；同时也应用了sklearn库的`preprocessing.LabelEncoder()`将其转换为整数编码，主要用于数据集的类标列。

决策树（见下文中定义函数`decisionTree_structure`计算的流程图）测试特征的内部节点，用盒子表示，节点之间通过边来连接，边表示了测试的可能输出，（根据阈值）将训练实例分到不同的子集中。子节点应用特征值继续测试训练实例的子集，直到满足一个停止标准。分类任务中，决策树中不再分支的节点为叶节点，表示类别（如果是在回归任务中，一个叶节点包含多个实例，这些实例对应的响应变量值可以通过求均值来估计这个叶节点对应的响应变量）；带有分支的节点通常称为分支节点（或子节点）。在决策树构建完成后，对于一个测试实例进行预测，只需要从根节点顺着对应的边到达某个叶节点。

训练决策树的优化算法，使用Ross Quinlan发明的迭代二叉树3代(Iterative Dichotomiser 3 (ID3)的算法。

> 参考文献
> 1. Cavin Hackeling. Mastering Machine Learning with scikit-learn[M].Packt Publishing Ltd.July 2017.Second published. 中文版为：Cavin Hackeling.张浩然译.scikit-learning 机器学习[M].人民邮电出版社.2019.2


```python
import pandas as pd
catDog_trainingData_df=pd.DataFrame({'plays_Fetch':['Yes','No','No','No','No','No','No','No','No','Yes','Yes','No','Yes','Yes'],
                                  'is_grumpy':['No','Yes','Yes','Yes','No','Yes','Yes','No','Yes','No','No','No','Yes','Yes'],
                                  'fvorite_food':['bacon','dog_food','cat_food','bacon','cat_food','bacon','cat_food','dog_food','cat_food','dog_food','bacon','cat_food','cat_food','bacon'],
                                  'species':['dog','dog','cat','cat','cat','cat','cat','dog','cat','dog','dog','cat','cat','dog']})
print("原始数据：\n",catDog_trainingData_df)
```

    原始数据：
        plays_Fetch is_grumpy fvorite_food species
    0          Yes        No        bacon     dog
    1           No       Yes     dog_food     dog
    2           No       Yes     cat_food     cat
    3           No       Yes        bacon     cat
    4           No        No     cat_food     cat
    5           No       Yes        bacon     cat
    6           No       Yes     cat_food     cat
    7           No        No     dog_food     dog
    8           No       Yes     cat_food     cat
    9          Yes        No     dog_food     dog
    10         Yes        No        bacon     dog
    11          No        No     cat_food     cat
    12         Yes       Yes     cat_food     cat
    13         Yes       Yes        bacon     dog
    

数据集的特征值完成独热编码后，或增加相应的列，对于仅存在有两个分类的列（例如'Yes'和'No'），虽然增加了新的一列，但是与单独一列实质上并没有区别（非1即0），因此在决策树中使用二者中的任何一列都是一样的（例如play_No或play_Yes）。但是分类在3类及其以上者，当用独热编码完成转换后，新增列之间是不能互相替换，例如food_bacon,food_cat food,food_dog food。


```python
def df_multiColumns_LabelEncoder(df,columns=None):
    from sklearn import preprocessing
    '''
    function - 根据指定的（多个）列，将分类转换为整数表示，区间为[0,分类数-1]
    
    Paras:
        df - DataFrame格式数据
        columns - 指定待转换的列名列表
    '''
    output=df.copy()
    if columns is not None:
        for col in columns:
            output[col]=preprocessing.LabelEncoder().fit_transform(output[col])
    else:
        for column_name, col in output.iteritems():
            output[column_name]=preprocessing.LabelEncoder().fit_transform(col)
            
    return output    
    
    
catDog_trainingData_df_encoder=df_multiColumns_LabelEncoder(df=catDog_trainingData_df,columns=['plays_Fetch','is_grumpy','fvorite_food','species'])    
print("encoder of each column\n",catDog_trainingData_df_encoder)

catDog_trainingData_dummies=pd.get_dummies(catDog_trainingData_df,prefix=['play','grumpy','food','species'])
print("fequency of each column:\n",catDog_trainingData_dummies.apply(pd.Series.value_counts))
print('one-hot(dummies):\n')
import util_misc
util_misc.print_html(catDog_trainingData_dummies,row_numbers=14)
```

    encoder of each column
         plays_Fetch  is_grumpy  fvorite_food  species
    0             1          0             0        1
    1             0          1             2        1
    2             0          1             1        0
    3             0          1             0        0
    4             0          0             1        0
    5             0          1             0        0
    6             0          1             1        0
    7             0          0             2        1
    8             0          1             1        0
    9             1          0             2        1
    10            1          0             0        1
    11            0          0             1        0
    12            1          1             1        0
    13            1          1             0        1
    fequency of each column:
        play_No  play_Yes  grumpy_No  grumpy_Yes  food_bacon  food_cat_food  \
    0        5         9          8           6           9              8   
    1        9         5          6           8           5              6   
    
       food_dog_food  species_cat  species_dog  
    0             11            6            8  
    1              3            8            6  
    one-hot(dummies):
    
    




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>play_No</th>
      <th>play_Yes</th>
      <th>grumpy_No</th>
      <th>grumpy_Yes</th>
      <th>food_bacon</th>
      <th>food_cat_food</th>
      <th>food_dog_food</th>
      <th>species_cat</th>
      <th>species_dog</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>



决策树通过检测一个特征序列的值来估计响应变量的值。即能产出只包含猫和只包含狗的子集的检测，要优于一个产出中同时包含猫狗的检测。因为一个子集中的成员同时包含不同的类，无法确定实例的分类。对于这个检测可以使用熵的衡量方式量化不确定性的程度（单位为比特，bits）。其公式为：$H(X)=- \sum_{i=1}^n P( x_{i})  log_{b} P( x_{i})$，其中$P( x_{i})$ 是输出$i$的概率，$b$常见值为2,e和10。由于一个小于1的数值的对数为负数，求和为负数，因此取反。为了方便计算一个对象的熵来查看计算的流程，定义`entropy_compomnent`函数实现。

找出对分类动物最有帮助的特征，即找出能把熵降到最低的特征（熵值越大，类别分布越均匀；熵值越小，类别分布越集中，即分布不均匀）。下述代码的过程是按照下图的执行指向过程计算，在层-A根节点中，根据类标计算猫（8只）狗（6只）熵值为0.985。先利用'food_cat food'特征列，左子节点不吃猫食（0）对应的狗分类为6只，猫分类为2只，信息熵为0.811；右子节点吃猫食（1）对应的狗分类为0只，猫分类为6只，信息熵为0.0；因为左子节点信息熵0.811>0.5，因此需要对左子节点继续检测。选择'grumpy_Yes'特征列对应左子节点，即'food_cat food'中值为0（不吃猫食）的行（实例），包括6只狗和2只猫。这8个实例可以根据'grumpy_Yes'特征列，即是否脾气暴躁（grumpy）划分为两类，其中脾气暴躁（1）的为4个实例（右子节点），而脾气不暴燥（0）也为4个实例（左子节点）。将其分布对应到类标列，可以得知脾气暴躁的4个实例中，为猫分类的有2个，为狗分类的亦有2个，信息熵为1；而脾气不暴躁的4个实例中，为猫分类的为0，为狗的分类为4，信息熵为0。就是说明，如果不吃猫食，而脾气不暴躁的实例分类为狗。

上述的决策树，根据两个特征列，'food_cat food'和'grumpy_Yes'判断物种分类（类标），因为层-C存在熵值为1的右子节点，需要继续删选特征，继续检测。例如想判断第12行实例（样本）的类标，已知'food_cat food'的值为1（吃猫食），'grumpy_Yes'的值为1（脾气暴躁），通过层-B可以判断出，该实例会被分配到右子节点(吃猫食)，该层右子节点信息熵为0，因此可以判断，该实例为猫，与实际相符。再例如，对于第10行实例，已知'food_cat food'的值为0（不吃猫食），'grumpy_Yes'的值为0（脾气不暴躁），通过层-B可以判断出，该实例会被分配到左子节点(不吃猫食)，因为该节点信息熵大于0.5，因此继续用层-C检测，由脾气不暴躁，将其指向左子节点，该节点信息熵为0，因此可以判定该实例为狗，与实际相符。

<a href=""><img src="./imgs/2_5_2_07.png" height='auto' width='1000' title="caDesign"></a>

* 信息增益(Information gain)

上述的计算直接选择了两个特征列，实际上是需要判断选择哪些特征列用于检测，以减少分类的不确定性。在层-B中，产生了两个子集（子节点），熵分别为0.811和0.0，其平均熵为$(0.811+0.0)/2=0.4055$，而根节点的熵为0.985，最大不确定性的熵为1。衡量熵的减少可以使用信息增益的指标，其公式为：$IG(T,a)=H(T)- \sum_{v \in vals(a)}^{}  \frac{ | \{x \in T |  x_{a}= \upsilon  \} | }{ |  T| }H(\{x \in T | x_{a}= \upsilon \})  $，其中$X_{a}  \in vals(a)$表示实例$x$对应的特征$a$的值，$x \in T | x_{a}= \upsilon$表示特征$a$的值等于$\upsilon$的实例数量，$H(\{x \in T | x_{a}= \upsilon \})$是特征$a$的值等于$\upsilon$的实例的子集的熵。

自定义信息增益函数`IG`，计算结果中food_cat_food特征列的值0.463587为所有特征列里最小信息熵，因此用该特征列检测。

* 基尼不纯度(Gini impurity)

除了通过创建能产生最大信息增益的节点来创建一个决策树，还可以用启发性算法基尼不纯度(Gini impurity)衡量一个集合中类的比例，其公式为：$Gini(t)=1- \sum_{i=1}^j  P(i | t)^{2} $，其中$j$是类的数量，$t$是节点对应的实例子集，$ P(i | t$是从节点的子集中选择一个属于类$i$元素的概率。当集合中所有元素都属于同一类时，选择任一元素属于这个类的概率均为1，因此Gini值为0。和熵一样，当每个被选择的类概率都相等时Gini达到最大值，其最大值依赖可能类的数量，公式为：$Gini_{max}=1- \frac{1}{n}  $。如果分类问题包括两个类，Gini的最大值等于1/2。

Sklearn库中DecisionTreeClassifier算法在参数criterion中，给出了上述两种方法`{“gini”, “entropy”}`，可以自行配置。


```python
def entropy_compomnent(numerator,denominator):
    import math
    '''
    function - 计算信息熵分量
    '''
    if numerator!=0:
        return -numerator/denominator*math.log2(numerator/denominator)    
    elif numerator==0:
        return 0

print('层-A 根节点- 类标species(8,6):entropy={entropy}'.format(entropy=(entropy_compomnent(6,14)+entropy_compomnent(8,14))))
print('层-B - food_cat food(8,6):左子节点8(2,6)-Left_entropy={L_entropy};右子节点6(6,0)-Right_entropy={R_entropy};'.format(L_entropy=(entropy_compomnent(2,8)+entropy_compomnent(6,8)),R_entropy=(entropy_compomnent(0,6)+entropy_compomnent(6,6))))
print('层-C - grumpy_Yes-8(4,4):左子节点4(0,4)-Left_entropy={L_entropy};右子节点4(2,2)-Right_entropy={R_entropy};'.format(L_entropy=(entropy_compomnent(0,4)+entropy_compomnent(4,4)),R_entropy=(entropy_compomnent(2,4)+entropy_compomnent(2,4))))
```

    层-A 根节点- 类标species(8,6):entropy=0.9852281360342516
    层-B - food_cat food(8,6):左子节点8(2,6)-Left_entropy=0.8112781244591328;右子节点6(6,0)-Right_entropy=0.0;
    层-C - grumpy_Yes-8(4,4):左子节点4(0,4)-Left_entropy=0.0;右子节点4(2,2)-Right_entropy=1.0;
    


```python
def IG(df_dummies):
    import pandas as pd    
    '''
    function - 计算信息增量(IG)
    
    para:
        df_dummies - DataFrame格式，独热编码的特征值
    '''
    weighted_frequency=df_dummies.apply(pd.Series.value_counts)
    #print(weighted_frequency)
    weighted_sum=weighted_frequency.sum(axis=0)
    #print(weighted_sum)
    feature_columns=weighted_frequency.columns.tolist()
    #print(weighted_sum.loc[feature_columns[0]])
    Parent_entropy=entropy_compomnent(weighted_frequency[feature_columns[-1]][0],14)+entropy_compomnent(weighted_frequency[feature_columns[-1]][1],14)
    #print(Parent_entropy)
    
    cal_info=[]
    for feature in feature_columns[:-2]:        
        v_0_frequency=df_dummies.query('%s==0'%feature).iloc[:,-1].value_counts().reindex(df_dummies[feature].unique(),fill_value=0) #频数可能为0，如果为0则会被舍弃（value_counts），因此需要补回（.reindex）
        v_1_frequency=df_dummies.query('%s==1'%feature).iloc[:,-1].value_counts().reindex(df_dummies[feature].unique(),fill_value=0)
        first_child_entropy=entropy_compomnent(v_0_frequency[0], v_0_frequency.sum(axis=0))+entropy_compomnent(v_0_frequency[1], v_0_frequency.sum(axis=0)) 
        second_child_entropy=entropy_compomnent(v_1_frequency[0], v_1_frequency.sum(axis=0))+entropy_compomnent(v_1_frequency[1], v_1_frequency.sum(axis=0))

        cal_dic={'test':feature,
                 'Parent_entropu':Parent_entropy,
                 'first_child_entropy':first_child_entropy,
                 'second_child_entropy':second_child_entropy,
                 'Weighted_average_expression':'%f*%d/%d+%f*%d/%d'%(first_child_entropy,weighted_frequency[feature][0],weighted_sum.loc[feature],second_child_entropy,weighted_frequency[feature][1],weighted_sum.loc[feature]),
                 'IG':first_child_entropy*(weighted_frequency[feature][0]/weighted_sum.loc[feature])+second_child_entropy*(weighted_frequency[feature][1]/weighted_sum.loc[feature])
                 
                }
        cal_info.append(cal_dic)
    cal_info_df=pd.DataFrame.from_dict(cal_info)    
    return cal_info_df

cal_info_df=IG(df_dummies=catDog_trainingData_dummies,)
util_misc.print_html(cal_info_df,row_numbers=7)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test</th>
      <th>Parent_entropu</th>
      <th>first_child_entropy</th>
      <th>second_child_entropy</th>
      <th>Weighted_average_expression</th>
      <th>IG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>play_No</td>
      <td>0.985228</td>
      <td>0.721928</td>
      <td>0.764205</td>
      <td>0.721928*5/14+0.764205*9/14</td>
      <td>0.749106</td>
    </tr>
    <tr>
      <th>1</th>
      <td>play_Yes</td>
      <td>0.985228</td>
      <td>0.764205</td>
      <td>0.721928</td>
      <td>0.764205*9/14+0.721928*5/14</td>
      <td>0.749106</td>
    </tr>
    <tr>
      <th>2</th>
      <td>grumpy_No</td>
      <td>0.985228</td>
      <td>0.811278</td>
      <td>0.918296</td>
      <td>0.811278*8/14+0.918296*6/14</td>
      <td>0.857143</td>
    </tr>
    <tr>
      <th>3</th>
      <td>grumpy_Yes</td>
      <td>0.985228</td>
      <td>0.918296</td>
      <td>0.811278</td>
      <td>0.918296*6/14+0.811278*8/14</td>
      <td>0.857143</td>
    </tr>
    <tr>
      <th>4</th>
      <td>food_bacon</td>
      <td>0.985228</td>
      <td>0.918296</td>
      <td>0.970951</td>
      <td>0.918296*9/14+0.970951*5/14</td>
      <td>0.937101</td>
    </tr>
    <tr>
      <th>5</th>
      <td>food_cat_food</td>
      <td>0.985228</td>
      <td>0.811278</td>
      <td>0.000000</td>
      <td>0.811278*8/14+0.000000*6/14</td>
      <td>0.463587</td>
    </tr>
    <tr>
      <th>6</th>
      <td>food_dog_food</td>
      <td>0.985228</td>
      <td>0.845351</td>
      <td>0.000000</td>
      <td>0.845351*11/14+0.000000*3/14</td>
      <td>0.664204</td>
    </tr>
  </tbody>
</table>



* 使用sklearn库的DecisionTreeClassifier实现决策树以及打印决策树流程图表。

* [交叉验证 cross_val_score](cross_val_score)

训练和测试数据集如果相同，即在相同的数据上训练和测试，模型（估计器，estimator）只会重复它刚刚看到的样本标签，可能会获得完美的分数，但无法正确预测其它的数据，这种情况称之为过拟合。因此在训练机器学习模型时，通常的做法是将数据集分为训练和测试数据集。但是手动配置超参（数）时，因为参数可以调整以使估计器达到最优，使得存在测试集过拟合的风险。这样，关于测试集的“知识”学习就会“泄露”到模型中，评估指标就不再报告泛化性能。为了解决这个问题，数据集被切分为训练数据集、验证数据集和测试数据集。然而，将数据集划分为三个集合，大大减少了学习模型的样本数量，以及训练和验证数据集组合随机选择的机会。解决这一问题的方案可以应用交叉验证（cross val score,CV）的方法。

在CV方法中，测试数据集仍然用于最终的模型评估，但是不再需要验证数据集。被称为k-fold（k-倍/重/折）的CV中，数据集被分成k个更小的集合，对于每k个folds，k-1个folds用于训练集，1个fold用于测试集。得到的模型在剩余的数据上进行验证(也就是说，它被用作一个测试集来计算性能度量，比如精度)。如下图所示(颜色表示数据集类型)。

<a href=""><img src="./imgs/2_5_2_08.png" height='auto' width='500' title="caDesign">引自：[Sklearn官网]( https://scikit-learn.org/stable/modules/cross_validation.html)</a>


```python
X=catDog_trainingData_dummies[catDog_trainingData_dummies.columns[:-2]].to_numpy()
y=catDog_trainingData_df_encoder[catDog_trainingData_df_encoder.columns[-1]].to_numpy()
```


```python
def decisionTree_structure(X,y,criterion='entropy',cv=None,figsize=(6, 6)):
    import numpy as np
    from matplotlib import pyplot as plt

    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_iris
    from sklearn.tree import DecisionTreeClassifier
    from sklearn import tree
    from sklearn.model_selection import cross_val_score
    '''
    function - 使用决策树分类，并打印决策树流程图表。迁移于Sklearn的'Understanding the decision tree structure', https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py
    
    Paras:
        X - 数据集-特征值（解释变量）
        y- 数据集-类标/标签(响应变量)
        criterion - DecisionTreeClassifier 参数，衡量拆分的质量，即衡量哪一项检测最能减少分类的不确定性
        cv - cross_val_score参数，确定交叉验证分割策略，默认值为None，即5-fole(折)的交叉验证    
    '''
    #X_train, X_test, y_train, y_test=train_test_split(X, y, random_state=0)
    X_train,y_train=X,y
    clf=DecisionTreeClassifier(criterion=criterion,max_leaf_nodes=3, random_state=0)
    clf.fit(X_train, y_train)        
    
    n_nodes=clf.tree_.node_count
    children_left=clf.tree_.children_left
    children_right=clf.tree_.children_right
    feature=clf.tree_.feature
    threshold=clf.tree_.threshold    
    print("n_nodes:{n_nodes},\nchildren_left:{children_left},\nchildren_right={children_right},\nthreshold={threshold}".format(n_nodes=n_nodes,children_left=children_left,children_right=children_right,threshold=threshold))
    print("_"*50)
    
    node_depth=np.zeros(shape=n_nodes, dtype=np.int64)
    is_leaves=np.zeros(shape=n_nodes, dtype=bool)
    stack=[(0, 0)]  # start with the root node id (0) and its depth (0)    

    while len(stack) > 0:
        # `pop` ensures each node is only visited once
        node_id, depth = stack.pop()
        node_depth[node_id] = depth

        # If the left and right child of a node is not the same we have a split
        # node
        is_split_node = children_left[node_id] != children_right[node_id]
        # If a split node, append left and right children and depth to `stack`
        # so we can loop through them
        if is_split_node:
            stack.append((children_left[node_id], depth + 1))
            stack.append((children_right[node_id], depth + 1))
        else:
            is_leaves[node_id] = True

    print("The binary tree structure has {n} nodes and has "
          "the following tree structure:\n".format(n=n_nodes))
    for i in range(n_nodes):
        if is_leaves[i]:
            print("{space}node={node} is a leaf node.".format(
                space=node_depth[i] * "\t", node=i))
        else:
            print("{space}node={node} is a split node: "
                  "go to node {left} if X[:, {feature}] <= {threshold} "
                  "else to node {right}.".format(
                      space=node_depth[i] * "\t",
                      node=i,
                      left=children_left[i],
                      feature=feature[i],
                      threshold=threshold[i],
                      right=children_right[i]))   
            
    plt.figure(figsize=figsize)      
    tree.plot_tree(clf)
    plt.show()
    
    CV_scores=cross_val_score(clf,X,y, cv=cv)
    print('cross_val_score:\n',CV_scores) #交叉验证每次运行的估计器得分数组   
    print("%0.2f accuracy with a standard deviation of %0.2f" % (CV_scores.mean(), CV_scores.std())) #同时给出了平均得分，和标准差
    return clf
    
clf=decisionTree_structure(X,y)       
```

    n_nodes:5,
    children_left:[ 1  3 -1 -1 -1],
    children_right=[ 2  4 -1 -1 -1],
    threshold=[ 0.5  0.5 -2.  -2.  -2. ]
    __________________________________________________
    The binary tree structure has 5 nodes and has the following tree structure:
    
    node=0 is a split node: go to node 1 if X[:, 5] <= 0.5 else to node 2.
    	node=1 is a split node: go to node 3 if X[:, 3] <= 0.5 else to node 4.
    	node=2 is a leaf node.
    		node=3 is a leaf node.
    		node=4 is a leaf node.
    


    
<a href=""><img src="./imgs/2_5_2_11.png" height="auto" width="auto" title="caDesign"></a>
    


    cross_val_score:
     [0.66666667 0.66666667 0.66666667 1.         0.5       ]
    0.70 accuracy with a standard deviation of 0.16
    

#### 2）随机森林（Random forests）

决策树属于勤奋学习模型(eager learners)，与之相反的是KNN算法这样的惰性学习模型（lazy learners）。决策树学习算法会产生出完美拟合每一个训练实例的巨型复杂的决策树模型，而无法对真实的关系进行泛化，即容易过拟合。解决决策树过拟合的方法可以通过剪枝的方法移除决策树中过深的节点和叶子，或者从训练数据和特征的子集中创建多棵决策树构成多个模型的集成（集成是指多个估计器的组合），被称为随机森林的决策树集合。而创建集成的方法有套袋法(bagging)，推进法(boosting)，和堆叠法(stacking)。

* classification_report

`sklearn.metrics.classification_report`用于显示分类指标的文本报告，在报告中显示有每个类的精确度(precision)，召回率(recall)，F1分数(f1-score)等信息。精确度和召回率可以从Wikipedia的[Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)中获取详细的解释，给出的阐释图表可以一目了然的理解计算方法。实心小圆代表类标狗总计12只，空心小圆代表类标猫总计10只。通过假设的模型预测，得到正确预测为狗的有5只，正确预测为猫的为3只，其它的均为错误预测。因此对于分类狗而言，$precision= \frac{5}{8} $，$recall= \frac{5}{12} $。f1-score是精确度和召回率的调和平均值，$\frac{2}{ F_{1} } = \frac{1}{P}+ \frac{1}{R}    \longrightarrow F_{1} =2 \frac{P \times R}{P+R} $。

support字段为每个标签出现的次数。avg行为均值和加权均值（参数sample_weight 配置权重值，默认为None），avg行对应的support为标签总和。

从计算结果可知，使用随机森林算法的各项指标均高于决策树算法的结果。

<a href=""><img src="./imgs/2_5_2_09.png" height='auto' width='400' title="caDesign">引自：[Wikipedia]( https://en.wikipedia.org/wiki/Precision_and_recall)</a>


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X,y=make_classification(n_samples=1000,n_features=100,n_informative=20,n_clusters_per_class=2,random_state=11)
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=11)
clf_DTC=DecisionTreeClassifier(random_state=11)
clf_DTC.fit(X_train,y_train)
predictions_DTC=clf_DTC.predict(X_test)
print(classification_report(y_test,predictions_DTC))
```

                  precision    recall  f1-score   support
    
               0       0.73      0.66      0.69       127
               1       0.68      0.75      0.71       123
    
        accuracy                           0.70       250
       macro avg       0.71      0.70      0.70       250
    weighted avg       0.71      0.70      0.70       250
    
    


```python
clf_RFC=RandomForestClassifier(n_estimators=10,random_state=11)
clf_RFC.fit(X_train,y_train)
predictions_RFC=clf_RFC.predict(X_test)
print(classification_report(y_test,predictions_RFC))
```

                  precision    recall  f1-score   support
    
               0       0.74      0.83      0.79       127
               1       0.80      0.70      0.75       123
    
        accuracy                           0.77       250
       macro avg       0.77      0.77      0.77       250
    weighted avg       0.77      0.77      0.77       250
    
    

### 2.5.2.5 图像分类_识别器

#### 1）图像分类器

应用极端随机森林(extremely randomized trees, Extra-Tress)训练分类模型，注意应用了`preprocessing.LabelEncoder()`方法编码，如果要映射回原来的类标，可以执行`.inverse_transform`实现。从对估计器的评测结果来看，f1-score的平均得分为0.62，分项得分中'开阔 '、'窄建 '和'阔建'分类的预测得分大于0.7，相对较好；而'林荫'、'窄木'、'宽建'和'宽木 '都小于0.5，预测精度并不理想。这里的一个主要原因是图像分类的确定，这几个分类中有很多图像并不容易区分之间的差异，也就导致了对图像的分类选择并不专业，最终致使建立的预测模型的预测精度并不是很好，可以尝试从新建立分类标准，提供数据集的类标精度。


```python
import pickle
import numpy as np

class ERF_trainer:
    '''
    class - 用极端随机森林训练图像分类器
    '''
    def __init__(self,X,label_words,save_path):
        from sklearn import preprocessing
        from sklearn.ensemble import ExtraTreesClassifier
        import os,pickle
        
        print('Start training...')
        self.le=preprocessing.LabelEncoder()
        self.clf=ExtraTreesClassifier(n_estimators=100,max_depth=16,random_state=0) #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html
        y=self.encode_labels(label_words)
        self.clf.fit(np.asarray(X),y)
        with open(os.path.join(save_path,'ERF_clf.pkl'), 'wb') as f:  #存储训练好的图像分类器模型
            pickle.dump(self.clf, f)   
        print("end training and saved estimator.")
            
    def  encode_labels(self,label_words):
        '''
        function - 对标签编码，及训练分类器
        '''
        self.le.fit(label_words)
        return np.array(self.le.transform(label_words),dtype=np.float64)
    
    def classify(self,X):
        '''
        function - 对未知数据的预测分类
        '''
        label_nums=self.clf.predict(np.asarray(X))
        label_words=self.le.inverse_transform([int(x) for x in label_nums])
        return label_words

feature_map_fp='./model/visual_feature.pkl'            
with open(feature_map_fp,'rb') as f:
    feature_map=pickle.load(f) #读取存储的图像特征
label_words=[x['object_class'] for x in feature_map]    
dim_size=feature_map[0]['feature_vector'].shape[1]   
X=[np.reshape(x['feature_vector'],(dim_size,)) for x in feature_map]
save_path='./model'

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X, label_words,test_size=0.3, random_state=42)
erf=ERF_trainer(X_train,y_train,save_path)      
```

    Start training...
    end training and saved estimator.
    


```python
from sklearn.metrics import classification_report
print(classification_report(y_test,erf.classify(X_test)))
```

                  precision    recall  f1-score   support
    
              宽建       0.44      0.21      0.29        19
              宽木       0.55      0.39      0.46        28
              开阔       1.00      0.68      0.81        28
              林荫       0.20      0.17      0.18        12
              窄建       0.74      0.93      0.83        61
              窄木       0.25      0.09      0.13        22
              阔建       0.88      0.64      0.74        11
              阔木       0.54      0.81      0.65        59
    
        accuracy                           0.62       240
       macro avg       0.57      0.49      0.51       240
    weighted avg       0.61      0.62      0.59       240
    
    

#### 2）图像识别器

图像识别器需要调用3个已经保存的文件，第一个是'ERF_clf.pkl'，保存有应用极端随机森林算法训练的图像分类器模型，用于类别预测；第二个是'visual_BOW.pkl'，保存有视觉词袋KMeans聚类模型，用于构建图像特征（位于前述的feature_builder_BOW类中）的参数输入；第三个是'visual_feature.pkl'，存储的是图像特征，主要是读取类标转换为整型编码，然后应用`inverse_transform`方法将预测的整型编码转换为原始的类标。


```python
class ImageTag_extractor:
    '''
    class - 图像识别器，基于图像分类模型，视觉词袋以及图像特征
    '''
    def __init__(self, ERF_clf_fp, visual_BOW_fp,visual_feature_fp):
        from sklearn import preprocessing
        import pickle
        with open(ERF_clf_fp,'rb') as f:  #读取存储的图像分类器模型
            self.clf=pickle.load(f)

        with open(visual_BOW_fp,'rb') as f:  #读取存储的聚类模型和聚类中心点
            self.kmeans=pickle.load(f)

        '''对标签编码'''
        with open(visual_feature_fp, 'rb') as f:
            self.feature_map=pickle.load(f)
        self.label_words=[x['object_class'] for x in self.feature_map]
        self.le=preprocessing.LabelEncoder()
        self.le.fit(self.label_words)   
        
    def predict(self,img):
        import util_A
        import numpy as np
        feature_vector=util_A.feature_builder_BOW().construct_feature(img,self.kmeans)  #提取图像特征，之前定义的feature_builder_BOW()类，可放置于util.py文件中，方便调用
        label_nums=self.clf.predict(np.asarray(feature_vector)) #进行图像识别/分类
        image_tag=self.le.inverse_transform([int(x) for x in label_nums])[0] #获取图像分类标签
        return image_tag
    
ERF_clf_fp=r'./model/ERF_clf.pkl'
visual_BOW_fp='./model/visual_BOW.pkl'
visual_feature_fp='./model/visual_feature.pkl'

import cv2 as cv
import os
imgs_fp=r'C:\Users\richi\omen_richiebao\omen_github\USDA_CH_final\USDA\notebook\data\kitti'
imgs_=[os.path.join(imgs_fp,f) for f in os.listdir(imgs_fp)]
imgs_pred_tag={fn:ImageTag_extractor(ERF_clf_fp,visual_BOW_fp,visual_feature_fp).predict(cv.imread(fn)) for fn in imgs_}
```

在GoogleEarth的街景（Street view）德国随机城市下随机的截取了6张尺寸大小不一的图像，应用图像识别器预测分类，其结果如下。


```python
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib
matplotlib.rcParams['font.family'] = ['SimHei']

fig,axes=plt.subplots(2,3,sharex=True,sharey=True,figsize=(25,10))   #布局多个子图，每个子图显示一幅图像
ax=axes.flatten()  #降至1维，便于循环操作子图
i=0
for f,tag in imgs_pred_tag.items():
    img_array=Image.open(f)
    ax[i].imshow(img_array)  #显示图像
    ax[i].set_title("pred:{tag}".format(tag=tag))
    i+=1
fig.tight_layout() #自动调整子图参数，使之填充整个图像区域
fig.suptitle("images show",fontsize=14,fontweight='bold',y=1.02)
plt.show()
```


    
<a href=""><img src="./imgs/2_5_2_12.png" height="auto" width="auto" title="caDesign"></a>
    


#### 3）嵌入图像识别器到网络实验平台

将估计器部署到网络实验平台，需要将图像识别器及其相关的代码和文件（估计器、视觉词袋和图像特征）整合起来。下述将`ImageTag_extractor`和`feature_builder_BOW`类置于同一个文件中(`app/visual_perception/ImageTag_extractor.py`)，同时将`feature_builder_BOW`类中不需要的功能移除，保持代码简洁，避免干扰，并增加了`ImageTag_extractor_execution`类，方便外部调用该类，直接执行预测。而'ERF_clf.pkl'，'visual_BOW.pkl'和'visual_feature.pkl'三个必需文件置于文件夹visual_perception/vp_model中。

预测图像分类的功能是可以在页面端上传一幅图像后，用图像识别器（已经训练好的图像分类器）预测分类，因此增加了一个新的文件夹uploads用于保存上传的图像文件。图像上传的配置主要使用'flask_uploads'库实现，需要在'app/__init__.py'文件中增加相应的配置。

<a href=""><img src="./imgs/2_5_2_10.png" height='auto' width='800' title="caDesign"></a>

嵌入到网络实验平台的图像识别器


```python
#app/visual_perception/ImageTag_extractor.py
class feature_builder_BOW:
    '''
    class - (仅保留construct_feature及关联部分函数)根据所有图像关键点描述子聚类建立图像视觉词袋，获取每一图像的特征（码本）映射的频数统计
    '''

    def __init__(self, num_cluster=32):
        self.num_clusters = num_cluster

    def extract_features(self, img):
        import cv2 as cv
        '''
        function - 提取图像特征

        Paras:
        img - 读取的图像
        '''
        star_detector = cv.xfeatures2d.StarDetector_create()
        key_points = star_detector.detect(img)
        img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
        kp, des = cv.xfeatures2d.SIFT_create().compute(img_gray, key_points)  # SIFT特征提取器提取特征
        return des

    def normalize(self, input_data):
        import numpy as np
        '''
        fuction - 归一化数据

        input_data - 待归一化的数组
        '''
        sum_input = np.sum(input_data)
        if sum_input > 0:
            return input_data / sum_input  # 单一数值/总体数值之和，最终数值范围[0,1]
        else:
            return input_data

    def construct_feature(self, img, kmeans):
        import numpy as np
        '''
        function - 使用聚类的视觉词袋构建图像特征（构造码本）

        Paras:
        img - 读取的单张图像
        kmeans - 已训练的聚类模型
        '''
        des = self.extract_features(img)
        labels = kmeans.predict(des.astype(np.float))  # 对特征执行聚类预测类标
        feature_vector = np.zeros(self.num_clusters)
        for i, item in enumerate(feature_vector):  # 计算特征聚类出现的频数/直方图
            feature_vector[labels[i]] += 1
        feature_vector_ = np.reshape(feature_vector, ((1, feature_vector.shape[0])))
        return self.normalize(feature_vector_)

class ImageTag_extractor:
    '''
    class - 图像识别器，基于图像分类模型，视觉词袋以及图像特征
    '''

    def __init__(self, ERF_clf_fp, visual_BOW_fp, visual_feature_fp):
        from sklearn import preprocessing
        import pickle
        with open(ERF_clf_fp, 'rb') as f:  # 读取存储的图像分类器模型
            self.clf = pickle.load(f)

        with open(visual_BOW_fp, 'rb') as f:  # 读取存储的聚类模型和聚类中心点
            self.kmeans = pickle.load(f)

        '''对标签编码'''
        with open(visual_feature_fp, 'rb') as f:
            self.feature_map = pickle.load(f)
        self.label_words = [x['object_class'] for x in self.feature_map]
        self.le = preprocessing.LabelEncoder()
        self.le.fit(self.label_words)

    def predict(self, img):
        import numpy as np
        feature_vector=feature_builder_BOW().construct_feature(img, self.kmeans)  # 提取图像特征，之前定义的feature_builder_BOW()类，可放置于util.py文件中，方便调用
        label_nums = self.clf.predict(np.asarray(feature_vector))  # 进行图像识别/分类
        image_tag = self.le.inverse_transform([int(x) for x in label_nums])[0]  # 获取图像分类标签
        return image_tag


class ImageTag_extractor_execution:
    def __init__(self,img_url):
        self.img_url=img_url
        self.ERF_clf_fp='app/visual_perception/vp_model/ERF_clf.pkl'
        self.visual_BOW_fp = 'app/visual_perception/vp_model/visual_BOW.pkl'
        self.visual_feature_fp = 'app/visual_perception/vp_model/visual_feature.pkl'

    def execution(self):
        import cv2 as cv
        print("*"*50)
        print(self.img_url)
        imgs_pred_tag=ImageTag_extractor(self.ERF_clf_fp, self.visual_BOW_fp, self.visual_feature_fp).predict(cv.imread(self.img_url))
        return  imgs_pred_tag
```

配置'flask_uploads'


```python
#app/__init__.py
#...
from flask_uploads import UploadSet, configure_uploads, IMAGES, patch_request_class
import os
#...
photos=UploadSet('photos', IMAGES)

def create_app(config_name):
    #...
    basedir=os.path.abspath(os.path.dirname(__file__))
    app.config['UPLOADED_PHOTOS_DEST']=os.path.join(basedir,'uploads')  # you'll need to create a folder named uploads
    configure_uploads(app, photos)
    patch_request_class(app)
    #...
```

配置图像文件上传Web表达


```python
#app/visual_perception/forms.py
#...
from .. import photos
#...
class upload_img(FlaskForm):
    photo=FileField(validators=[FileAllowed(photos, 'Image only!'), FileRequired('File was empty!')])
    submit=SubmitField('Upload')
```

配置路由和视图函数，在视图函数中调用图像识别器。


```python
#app/visual_perception/views.py
#...
from .. import photos
from .ImageTag_extractor import ImageTag_extractor_execution
#...
@visual_perception.route("/img_prediction",methods=['GET','POST'])
def img_prediction():
    import os
    form=upload_img()
    if form.validate_on_submit():
        img_name=photos.save(form.photo.data)
        img_url=photos.url(img_name)
        img_fp=os.path.join(current_app.config['UPLOADED_PHOTOS_DEST'],img_name)
        imgs_pred_tag=ImageTag_extractor_execution(img_fp).execution()
        return render_template('vp/img_prediction.html', form=form, img_url=img_url, imgs_pred_tag=imgs_pred_tag)
    else:
        img_url=None
        return render_template('vp/img_prediction.html', form=form, img_url=img_url)
```

配置'预测图像分类'的模板页面


```python
<!--templates/vp/img_prediction.html --> 
{% extends "base.html" %}
{% import "bootstrap/wtf.html" as wtf %}
{% import "_macros.html" as macros %}

{% block title %}caDesign - visual perception{% endblock %}

{% block page_content %}
    <div class="jumbotron">
        <h3>预测图像分类</h3>

        <p>
            通过'参与图像分类'获取训练数据集（798幅图像）;--->应用Start特征检测器和SIFT尺度不变特征变换提取图像关键点描述子;--->聚类图像描述子建立视觉词袋(BOW);
            --->提取图像特征映射，建立训练数据集特征向量;--->极端随机森林(extremely randomized trees, Extra-Tress)训练分类估计器，建立图像分类器;--->应用估计器构建图像识别器。
        </p>

        <p>
            <a class="btn btn-secondary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">实验主页</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.imgs_classification') }}" role="button">参与图像分类</a>
            &nbsp<a class="btn btn-primary btn-lg" href="{{ url_for('visual_perception.vp') }}" role="button">空间类型分布/待</a>
        </p>
    </div>

    <form method="POST" enctype="multipart/form-data">
         {{ form.hidden_tag() }}
         {{ form.photo }}
         {% for error in form.photo.errors %}
             <span style="color: red;">{{ error }}</span>
         {% endfor %}
         {{ form.submit }}
    </form>

    {% if img_url %}
        <br>
        <div class="thumbnail">
            <img src="{{ img_url }}" >
            <div class="caption">
                <h4>预测结果：{{ imgs_pred_tag }}</h4>
                <p>[林荫,窄建,窄木,宽建,宽木,阔建,阔木,开阔] </p>
            </div>
        </div>
    {% endif %}

{% endblock %}
```

#### 4）部署

在服务器(Linux系统)下部署基于Flask构建的网络实验平台，其基本流程是：1.建立python虚拟环境，并根据生成的requirements.txt文件安装库配置环境；2.安装nginx，uwsgi，flask；3.建立一个简单的App应用，测试程序；4.本地项目上传至服务器虚拟环境目录下；5.建立uWSGI入口点(entry points)；6.配置uWSGI；7.建立Upstart Script；8. 配置Nginx；8.supervisor进程守护程序。

虽然网络上可以搜索到大量在Linux系统服务器部署Flask的教程，但是因为系统可能会存在不同，说明文字上的不清晰，Flask文件结构的差异，python环境的变化，尤其库版本的变化，通常在部署时很难一帆风顺。即使给出的配置步骤没有问题，部署的过程也会容易出现不经意的错误，因此需要耐心的跟随步骤操作，同时最好用markdown工具将步骤，所用到的命令记录下来，方便查看。具体可以尝试参考[How To Serve Flask Applications with uWSGI and Nginx on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uwsgi-and-nginx-on-ubuntu-14-04)或者自行搜索。
