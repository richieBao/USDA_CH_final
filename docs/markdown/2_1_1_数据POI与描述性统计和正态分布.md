> Created on Tue Sep 12 15:58:43 2017  @author: Richie Bao-caDesign设计(cadesign.cn)
> __+updated on Tue Jun 16 14:26:28 2020 by Richie Bao +updated on Sun Dec 19 12:05:23 2021 by Richie Bao

# 2.1.1 数据POI与描述性统计和正态分布

### 2.1.1.1 单个分类POI数据爬取与地理空间点地图

在城市空间分析中，必然会涉及到两个方面的数据，一类是实体的物质空间（强调几何意义），其亦可以划分为二维平面数据（例如遥感影像，城市地图等）和三维空间数据（例如雷达数据（高空扫描和地面扫描），一般意义上各类格式的城市三维模型等）；以及由物质空间承载的各类属性数据，一类反映二维平面数据的属性（与二维地理位置结合，可以划分到二维平面数据当中），例如用地类型、自然资源分布、地址名称等；和反映人类、动物及无形物质各类活动性质的数据，例如人们的活动轨迹（出租车和共享单车车行轨迹、夜间灯光、手机基站定位用户数量等）、动物迁徙路径和各类小气候测量指标变化等。

[百度地图开放平台](http://lbsyun.baidu.com/index.php?title=%E9%A6%96%E9%A1%B5)提供地图相关的功能与服务，其Web服务API为开发者提供http/https接口，即开发者通过http/https形式发起检索请求，获取返回json或xml格式的检索数据。其中[兴趣点(POI,points of interest)](http://lbsyun.baidu.com/index.php?title=lbscloud/poitags)数据,目前包括21个大类，和无数小类，是反映物质空间承载的人类活动属性的数据（业态分布），如下：

| 一级行业分类 | 二级行业分类                                                                                                                                                                         |
|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 美食         |  中餐厅、外国餐厅、小吃快餐店、蛋糕甜品店、咖啡厅、茶座、酒吧、其他                                                                                                                  |
|  酒店        |  星级酒店、快捷酒店、公寓式酒店、民宿、其他                                                                                                                                          |
|  购物        |  购物中心、百货商场、超市、便利店、家居建材、家电数码、商铺、市场、其他                                                                                                              |
|  生活服务    |  通讯营业厅、邮局、物流公司、售票处、洗衣店、图文快印店、照相馆、房产中介机构、公用事业、维修点、家政服务、殡葬服务、彩票销售点、宠物服务、报刊亭、公共厕所、步骑行专用道驿站、其他  |
|  丽人        |  美容、美发、美甲、美体、其他                                                                                                                                                        |
|  旅游景点    |  公园、动物园、植物园、游乐园、博物馆、水族馆、海滨浴场、文物古迹、教堂、风景区、景点、寺庙、其他                                                                                    |
|  休闲娱乐    |  度假村、农家院、电影院、ktv、剧院、歌舞厅、网吧、游戏场所、洗浴按摩、休闲广场、其他                                                                                                 |
|  运动健身    |  体育场馆、极限运动场所、健身中心、其他                                                                                                                                              |
|  教育培训    |  高等院校、中学、小学、幼儿园、成人教育、亲子教育、特殊教育学校、留学中介机构、科研机构、培训机构、图书馆、科技馆、 其他                                                             |
|  文化传媒    |  新闻出版、广播电视、艺术团体、美术馆、展览馆、文化宫、其他                                                                                                                          |
|  医疗        |  综合医院、专科医院、诊所、药店、体检机构、疗养院、急救中心、疾控中心、医疗器械、医疗保健、其他                                                                                      |
|  汽车服务    |  汽车销售、汽车维修、汽车美容、汽车配件、汽车租赁、汽车检测场、其他                                                                                                                  |
|  交通设施    |  飞机场、火车站、地铁站、地铁线路、长途汽车站、公交车站、公交线路、港口、停车场、加油加气站、服务区、收费站、桥、充电站、路侧停车位、普通停车位、接送点、其他                        |
|  金融        |  银行、ATM、信用社、投资理财、典当行、其他                                                                                                                                           |
|  房地产      |  写字楼、住宅区、宿舍、内部楼栋、其他                                                                                                                                                |
|  公司企业    |  公司、园区、农林园艺、厂矿、其他                                                                                                                                                    |
|  政府机构    |  中央机构、各级政府、行政单位、公检法机构、涉外机构、党派团体、福利机构、政治教育机构、社会团体、民主党派、居民委员会、其他                                                          |
|  出入口      |  高速公路出口、高速公路入口、机场出口、机场入口、车站出口、车站入口、门（备注：建筑物和建筑物群的门）、停车场出入口、自行车高速出口、自行车高速入口、自行车高速出入口、其他          |
|  自然地物    |  岛屿、山峰、水系、其他                                                                                                                                                              |
|  行政地标    |  省、省级城市、地级市、区县、商圈、乡镇、村庄、其他                                                                                                                                  |
|  门址        |  门址点、其他  

> 引自：[百度地图开放平台](http://lbsyun.baidu.com/index.php?title=lbscloud/poitags) 更新于2020年5月11日。

#### 1）单个分类爬取

要爬取数据，需要查看百度提供的检索方法，根据要求来配置对应参数实现下载，具体查看[服务文档](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)。本次实验对应配置检索语句为，矩形区域检索:`http://api.map.baidu.com/place/v2/search?query=银行&bounds=39.915,116.404,39.975,116.414&output=json&ak={您的密钥} //GET请求`，百度地图例举的语句仅包含个别请求参数，实际上提供的行政区划区域检索请求参数约15个左右，可以根据对数据的下载需求来确定使用哪些请求参数。同时，提供了圆形区域检索、矩形区域检索、地点详情检索服务等。此次行政区域检索的请求参数配置如下：
```python
query_dic={
   'query':检索关键字。圆形区域检索和矩形区域内检索支持多个关键字并集检索，不同关键字间以\$符号分隔，最多支持10个关键字检索。如:”银行$酒店”
如果需要按POI分类进行检索，将分类通过query参数进行设置，例如query='旅游景点',
   'page_size': 单次召回POI数量，默认为10条记录，最大返回20条。多关键字检索时，返回的记录数为关键字个数*page_size，例如page_size='20', 
   'page_num':分页页码，默认为0,0代表第一页，1代表第二页，以此类推。常与page_size搭配使用，例如page_num='0',
   'scope':检索结果详细程度。取值为1 或空，则返回基本信息；取值为2，返回检索POI详细信息，例如 scope='2',
   'bounds':检索矩形区域，为左下角和右上角坐标，多组坐标间以","分隔 ,例如str(leftBottomCoordi[1]) + ',' + str(leftBottomCoordi[0]) + ','+str(rightTopCoordi[1]) + ',' + str(rightTopCoordi[0]),
   'output':输出格式为json或者xml ，例如output='json',
   'ak':开发者的访问密钥，必填项。v2之前该属性为key,                   
}
```
如需下载数据，请求参数是需要申请访问密码'ak'，[申请地址](http://lbsyun.baidu.com/apiconsole/key/create)。注意，需要注册登录。
首先来配置基本的参数，最终要实现的目的是将下载后的数据分别存储为.csv（Comma-Separated Values，逗号（字符）分隔值，以纯文本形式存储表格数据）和.json（JavaScript Object Notation，轻量级数据交换格式，层次结构简洁清晰）数据格式,因此调入json和csv库，辅助读写对应格式的文件。因为要通过网页地址来检索数据，因此调入urllib的HTTP库实现相应的URL（Uniform Resource Locator,统一资源定位符，即网络地址）处理。对文件路径的管理则可以使用os以及pathlib库。

* 矩形区域检索 （百度地图开放平台规定：目前矩形区域检索为高级权限，如有需求需提交【工单】咨询。）

> 如果无法通过矩形区域检索，可以从本书仓库中下载已经检索的POI数据，继续后续的实验


```python
import os
data_path='./data' #配置数据存储位置
#定义存储文件名
poi_fn_csv=os.path.join(data_path,'poi_csv.csv')
poi_fn_json=os.path.join(data_path,'poi_json.json')
```

配置请求参数，注意其中page_num参数为页数递增，初始参数时，配置页数范围`page_num_range=range(20)`。而output参数直接配置为固定的'json'，因此直接在函数内部实现。同时，由于百度API的限制，检索区域内最多返回的POI数据量有限制，造成下载疏漏，因此如果下载区域很大时，最好是将其切分为数个矩形逐一下载，从而增加一个配置参数`partition`实现检索区域的切分次数，如果设置为2，则切分矩形为4份检索区域分别下载。

注意在配置坐标时，使用[百度地图坐标拾取系统](http://api.map.baidu.com/lbsapi/getpoint/index.html)


```python
bound_coordinate={'leftBottom':[108.776852,34.186027],'rightTop':[109.129275,34.382171]} 
page_num_range=range(20)
partition=4 
query_dic={
    'query':'旅游景点',
    'page_size':'20', 
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0', 
}
```

通过定义一个函数实现百度地图开放平台下，基于矩形区域检索爬取POI数据，方便日后相关项目的代码迁移或者调用，增加代码融合的力度。因此，需要谨慎确定输入参数，避免在调用时还需调整函数内的变量，造成不必要的错误。由百度地图下载数据，其经纬度坐标为百度坐标系，因此需要对其进行转换，调入转换代码文件`coordinate_transformation.py`(该转换文件来自于网络)。

在坐标转换过程中，调用了两个转换函数，函数`bd09togcj02(bd_lon, bd_lat)`和`gcj02towgs84(lng, lat)`，因此，可以直接迁移，但是，因为这两个函数还有依赖函数，因此直接调用源文件能够让代码结构更清晰。关于坐标转换文件可以从配套该书的GitHub托管仓库查看。当运行爬取函数时，需要将坐标转换文件放置于与该.ipynb文件同一文件夹下，以备调用。


```python
def baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=False):
    import coordinate_transformation as cc
    import urllib,json,csv,os,pathlib
    '''
    function - 百度地图开放平台POI数据爬取
    
    Paras:
        query_dic - 请求参数配置字典，详细参考上文或者百度服务文档
        bound_coordinate - 以字典形式配置下载区域
        partition - 检索区域切分次数
        page_num_range - 配置页数范围
        poi_fn_list=False - 定义的存储文件名列表
    '''
    urlRoot='http://api.map.baidu.com/place/v2/search?' #数据下载网址，查询百度地图服务文档
    #切分检索区域
    if bound_coordinate:
        xDis=(bound_coordinate['rightTop'][0]-bound_coordinate['leftBottom'][0])/partition
        yDis=(bound_coordinate['rightTop'][1]-bound_coordinate['leftBottom'][1])/partition    
    #判断是否要写入文件
    if poi_fn_list:
        for file_path in poi_fn_list:
            fP=pathlib.Path(file_path)
            if fP.suffix=='.csv':
                poi_csv=open(fP,'w',encoding='utf-8')
                csv_writer=csv.writer(poi_csv)    
            elif fP.suffix=='.json':
                poi_json=open(fP,'w',encoding='utf-8')
    num=0
    jsonDS=[] #存储读取的数据，用于.json格式数据的保存
    #循环切分的检索区域，逐区下载数据
    print("Start downloading data...")
    for i in range(partition):
        for j in range(partition):
            leftBottomCoordi=[bound_coordinate['leftBottom'][0]+i*xDis,bound_coordinate['leftBottom'][1]+j*yDis]
            rightTopCoordi=[bound_coordinate['leftBottom'][0]+(i+1)*xDis,bound_coordinate['leftBottom'][1]+(j+1)*yDis]
            for p in page_num_range:  
                #更新请求参数
                query_dic.update({'page_num':str(p),
                                  'bounds':str(leftBottomCoordi[1]) + ',' + str(leftBottomCoordi[0]) + ','+str(rightTopCoordi[1]) + ',' + str(rightTopCoordi[0]),
                                  'output':'json',
                                 })
                
                url=urlRoot+urllib.parse.urlencode(query_dic)
                data=urllib.request.urlopen(url)
                responseOfLoad=json.loads(data.read())     
                #print(url,responseOfLoad.get("message"))
                if responseOfLoad.get("message")=='ok':
                    results=responseOfLoad.get("results") 
                    for row in range(len(results)):
                        subData=results[row]
                        baidu_coordinateSystem=[subData.get('location').get('lng'),subData.get('location').get('lat')] #获取百度坐标系
                        Mars_coordinateSystem=cc.bd09togcj02(baidu_coordinateSystem[0], baidu_coordinateSystem[1]) #百度坐标系-->火星坐标系
                        WGS84_coordinateSystem=cc.gcj02towgs84(Mars_coordinateSystem[0],Mars_coordinateSystem[1]) #火星坐标系-->WGS84
                        
                        #更新坐标
                        subData['location']['lat']=WGS84_coordinateSystem[1]
                        subData['detail_info']['lat']=WGS84_coordinateSystem[1]
                        subData['location']['lng']=WGS84_coordinateSystem[0]
                        subData['detail_info']['lng']=WGS84_coordinateSystem[0]   
                        if csv_writer:
                            csv_writer.writerow([subData]) #逐行写入.csv文件
                        jsonDS.append(subData)
            num+=1       
            print("No."+str(num)+" was written to the .csv file.")
    if poi_json:       
        json.dump(jsonDS,poi_json)
        poi_json.write('\n')
        poi_json.close()
    if poi_csv:
        poi_csv.close()
    print("The download is complete.")

baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=[poi_fn_csv,poi_fn_json])  
```

    Start downloading data...
    No.1 was written to the .csv file.
    No.2 was written to the .csv file.
    No.3 was written to the .csv file.
    No.4 was written to the .csv file.
    No.5 was written to the .csv file.
    No.6 was written to the .csv file.
    No.7 was written to the .csv file.
    No.8 was written to the .csv file.
    No.9 was written to the .csv file.
    No.10 was written to the .csv file.
    No.11 was written to the .csv file.
    No.12 was written to the .csv file.
    No.13 was written to the .csv file.
    No.14 was written to the .csv file.
    No.15 was written to the .csv file.
    No.16 was written to the .csv file.
    The download is complete.
    

* 圆形区域检索（百度地图开放平台没有限制该方法）

可设置圆心和半径，检索圆形区域内的地点信息。官方给出的检索方式`https://api.map.baidu.com/place/v2/search?query=银行&location=39.915,116.404&radius=2000&output=xml&ak=您的密钥 //GET请求`


```python
def baiduPOI_dataCrawler_circle(query_dic,poi_save_path,page_num_range):
    import coordinate_transformation as cc
    import urllib,json,csv,os,pathlib
    from tqdm import tqdm
    
    urlRoot='http://api.map.baidu.com/place/v2/search?' #数据下载网址，查询百度地图服务文档
    poi_json=open(poi_save_path,'w',encoding='utf-8')  
    
    #url=urlRoot+urllib.parse.urlencode(query_dic)
    #print(url)
    jsonDS=[] #存储读取的数据，用于.json格式数据的保存
    for p in tqdm(page_num_range): 
        #更新请求参数
        query_dic.update({'page_num':str(p)})
        url=urlRoot+urllib.parse.urlencode(query_dic)
        data=urllib.request.urlopen(url)
        responseOfLoad=json.loads(data.read())     
        #print(url,responseOfLoad.get("message"))  
        if responseOfLoad.get("message")=='ok':
            results=responseOfLoad.get("results") 
            for row in range(len(results)):
                subData=results[row]
                baidu_coordinateSystem=[subData.get('location').get('lng'),subData.get('location').get('lat')] #获取百度坐标系
                Mars_coordinateSystem=cc.bd09togcj02(baidu_coordinateSystem[0], baidu_coordinateSystem[1]) #百度坐标系-->火星坐标系
                WGS84_coordinateSystem=cc.gcj02towgs84(Mars_coordinateSystem[0],Mars_coordinateSystem[1]) #火星坐标系-->WGS84

                #更新坐标
                subData['location']['lat']=WGS84_coordinateSystem[1]
                subData['detail_info']['lat']=WGS84_coordinateSystem[1]
                subData['location']['lng']=WGS84_coordinateSystem[0]
                subData['detail_info']['lng']=WGS84_coordinateSystem[0]  
                jsonDS.append(subData)
        #break
    if poi_json:       
        json.dump(jsonDS,poi_json)
        poi_json.write('\n')
        poi_json.close()
    print("The download is complete.")                          
    
page_num_range=range(20)
query_dic={
    'location':'34.265708,108.953431',
    'radius':1000,
    'query':'旅游景点',   
    'page_size':'20',
    'scope':2, 
    'output':'json',
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0'        
}    
poi_save_path='./data/poi_circle.json'
baiduPOI_dataCrawler_circle(query_dic,poi_save_path,page_num_range)
```

    100%|██████████| 20/20 [00:05<00:00,  3.98it/s]

    The download is complete.
    

    
    


```python
import json
with open(poi_save_path) as f:
    poi_circle=json.load(f)
print(poi_circle[0]) 
```

    {'name': '西安城墙', 'location': {'lat': 34.25321785828024, 'lng': 108.94356940023889}, 'address': '陕西省西安市碑林区南大街', 'province': '陕西省', 'city': '西安市', 'area': '碑林区', 'street_id': '279fd0de9f76357b90114ce9', 'telephone': '(029)87272792', 'detail': 1, 'uid': '6e7e1320b113d8e3bea82230', 'detail_info': {'distance': 874, 'tag': '旅游景点;文物古迹', 'navi_location': {'lng': 108.95428871168, 'lat': 34.254593262873}, 'type': 'scope', 'detail_url': 'http://api.map.baidu.com/place/detail?uid=6e7e1320b113d8e3bea82230&output=html&source=placeapi_v2', 'overall_rating': '4.3', 'comment_num': '200', 'children': [], 'lat': 34.25321785828024, 'lng': 108.94356940023889}}
    

> 行政区划区域检索、地点详情检索等方法可以查看[服务文档](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)。

#### 2）将.csv格式的POI数据转换为pandas的DataFrame

读取已经保存的poi_csv.csv文件。因为在文件保存时，使用的是csv和json库，因此读取时仍旧使用对应的库。最为经常使用的pandas库中也有'read_csv()'和`read_json()`等方法，在csv文件具体存储时，保存的方式也是多样的，因此pandas读取.csv或者.json文件最好是其自身所存储的文件，数据格式能够对应。而如果直接读取上述保存的POI的.csv文件，则会出现错误。只有知道数据格式，才能够有目的的提取数据，读取每一行的数据格式如下： 
```python
["{'name': '昆明池遗址', 'location': {'lat': 34.210991, 'lng': 108.779778}, 'address': '西安市长安区昆明池七夕公园内', 'province': '陕西省', 'city': '西安市', 'area': '长安区', 'detail': 1, 'uid': 'c7332cd7fbcc0d82ebe582d9', 'detail_info': {'tag': '旅游景点;景点', 'navi_location': {'lng': 108.7812626866, 'lat': 34.217484892966}, 'type': 'scope', 'detail_url': 'http://api.map.baidu.com/place/detail?uid=c7332cd7fbcc0d82ebe582d9&amp;output=html&amp;source=placeapi_v2', 'overall_rating': '4.3', 'comment_num': '77', 'children': []}}"]
```
读取.csv数据之后，直接使用.csv的格式数据来提取和分析数据并不是很方便。最为常用的数据格式是numpy库提供的数组（array）和pandas提供的DataFrame及series。其中numpy的数据组织形式更倾向于科学计算，为数阵形式，每一数组为同一数据类型；而pandas的DataFrame与地理信息数据中的属性表类似，其列（column）可以理解为属性字段(field)，每一列的数据类型相同，因此一个DataFrame可以包括多种数据类型，因此就POI的.csv数据而言，将其转换为DataFrame的数据格式是最好的。进一步而言，在城市空间数据分析方法中，更多的数据是基于地理空间位置的，因此对于具有地理属性的数据而言，最好能够以地理信息系统的逻辑来处理。在处理为DataFrame数据格式之后，则进一步应用GeoPandas等地理信息库来处理地理空间信息数据。

原始数据可能存在错误的数据格式，在数据格式转换时往往会出现错误，例如下述不正确的数据：
```python
["{'name': '励进海升酒店-多功能厅', 'location': {'lat': 34.218525, 'lng': 108.891524}, 'address': '西安市高新区沣惠南路34号励进海升酒店4层', 'province': '陕西省', 'city': '西安市', 'a{'name': '红蚂蚁少儿创意美术馆'", " 'location': {'lat': 34.306666", " 'lng': 108.822465}", " 'address': '陕西省西安市未央区后围寨启航佳苑B区3层商铺'", " 'province': '陕西省'", " 'city': '西安市'", " 'area': '未央区'", " 'telephone': '18209227178", "15229372642'", " 'detail': 1", " 'uid': 'e3fd730bb528b40015c9050c'", " 'detail_info': {'tag': '文化传媒;美术馆'", " 'type': 'scope'", " 'detail_url': 'http://api.map.baidu.com/place/detail?uid=e3fd730bb528b40015c9050c&amp;output=html&amp;source=placeapi_v2'", " 'overall_rating': '0.0'", ' \'children\': []}}"']
```
其中`'a{'name': '红蚂蚁少儿创意美术馆'",`为多余的部分，并不符合任何语法，因此在数据处理时，需要对此做出反应。通常使用`try/except`语句处理，最好返回可以索引的信息，并加以处理，避免数据损失。


```python
def csv2df(poi_fn_csv):
    import pandas as pd
    from benedict import benedict #benedict库是dict的子类，支持键列表（keylist）/键路径（keypath），应用该库的flatten方法展平嵌套的字典，准备用于DataFrame数据结构
    import csv
    
    '''
    function-转换.csv格式的POI数据为pandas的DataFrame
    
    Paras:
        poi_fn_csv - 存储有POI数据的.csv格式文件路径                
    '''
    n=0
    with open(poi_fn_csv, newline='',encoding='utf-8') as csvfile:
        poi_reader=csv.reader(csvfile)
        poi_dict={}    
        poiExceptions_dict={}
        for row in poi_reader:    
            if row:
                try:
                    row_benedict=benedict(eval(row[0])) #用eval方法，将字符串字典"{}"转换为字典{}
                    flatten_dict=row_benedict.flatten(separator='_') #展平嵌套字典
                    poi_dict[n]=flatten_dict
                except:                    
                    print("incorrect format of data_row number:%s"%n)                    
                    poiExceptions_dict[n]=row
            n+=1
            #if n==5:break #因为循环次数比较多，在调试代码时，可以设置停止的条件，节省时间与方便数据查看
    poi_df=pd.concat([pd.DataFrame(poi_dict[d_k].values(),index=poi_dict[d_k].keys(),columns=[d_k]).T for d_k in poi_dict.keys()], sort=True,axis=0)
    print("_"*50)
    for col in poi_df.columns:
        try:
            poi_df[col]=pd.to_numeric(poi_df[col])
        except:
            print("%s data type is not converted..."%(col))
    print("_"*50)
    print(".csv to DataFrame completed!")
    #print(poi_df.head()) #查看最终DataFrame格式下POI数据
    #print(poi_df.dtypes) #查看数据类型
    return poi_df
```

几乎python的所有数据类型，列表、字典、集合和类等都可以用pickle来序列化存储，而pandas同样提供了写入`pandas.DataFrame.to_pickle`，读取`pandas.DataFrame.read_pickle`的方法，因此为了避免每次将.csv转换为DataFrame，可以将pandas类型的文件按pandas所提供的方法加以存储，或写入数据库。


```python
poi_fn_csv='./data/poi_csv.csv'
poi_df=csv2df(poi_fn_csv)
poi_df.to_pickle('./data/poi_df.pkl')
print("_"*50)
print(poi_df.columns)
poi_df.head()
```

    incorrect format of data_row number:4
    incorrect format of data_row number:28
    incorrect format of data_row number:36
    __________________________________________________
    address data type is not converted...
    area data type is not converted...
    city data type is not converted...
    detail_info_detail_url data type is not converted...
    detail_info_indoor_floor data type is not converted...
    detail_info_tag data type is not converted...
    detail_info_type data type is not converted...
    name data type is not converted...
    province data type is not converted...
    street_id data type is not converted...
    telephone data type is not converted...
    uid data type is not converted...
    __________________________________________________
    .csv to DataFrame completed!
    __________________________________________________
    Index(['address', 'area', 'city', 'detail', 'detail_info_checkin_num',
           'detail_info_children', 'detail_info_comment_num',
           'detail_info_detail_url', 'detail_info_facility_rating',
           'detail_info_favorite_num', 'detail_info_hygiene_rating',
           'detail_info_image_num', 'detail_info_indoor_floor', 'detail_info_lat',
           'detail_info_lng', 'detail_info_navi_location_lat',
           'detail_info_navi_location_lng', 'detail_info_overall_rating',
           'detail_info_price', 'detail_info_service_rating', 'detail_info_tag',
           'detail_info_type', 'location_lat', 'location_lng', 'name', 'province',
           'street_id', 'telephone', 'uid'],
          dtype='object')
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>address</th>
      <th>area</th>
      <th>city</th>
      <th>detail</th>
      <th>detail_info_checkin_num</th>
      <th>detail_info_children</th>
      <th>detail_info_comment_num</th>
      <th>detail_info_detail_url</th>
      <th>detail_info_facility_rating</th>
      <th>detail_info_favorite_num</th>
      <th>...</th>
      <th>detail_info_service_rating</th>
      <th>detail_info_tag</th>
      <th>detail_info_type</th>
      <th>location_lat</th>
      <th>location_lng</th>
      <th>name</th>
      <th>province</th>
      <th>street_id</th>
      <th>telephone</th>
      <th>uid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>西安市长安区昆明池七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=c733...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.206767</td>
      <td>108.768459</td>
      <td>昆明池遗址</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c7332cd7fbcc0d82ebe582d9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>西安市高新区云萃路</td>
      <td>雁塔区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>34.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=6ccb...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;公园</td>
      <td>scope</td>
      <td>34.215761</td>
      <td>108.835407</td>
      <td>云水公园</td>
      <td>陕西省</td>
      <td>6ccb87a451f19a27626858b9</td>
      <td>NaN</td>
      <td>6ccb87a451f19a27626858b9</td>
    </tr>
    <tr>
      <th>6</th>
      <td>西安市长安区昆明池·七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=66f2...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.214174</td>
      <td>108.765677</td>
      <td>汉武大帝</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>66f2005e22dcafdbc7f50d07</td>
    </tr>
    <tr>
      <th>8</th>
      <td>西安市长安区沣东新城斗门新型工业园区汇新路北段昆明池·七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=576a...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.211592</td>
      <td>108.765670</td>
      <td>凤求凰</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>576a6060e0ea8ae21ce999c0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>陕西省西安市雁塔区西宝疏导线关中驾校南200米</td>
      <td>雁塔区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=8ed2...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;其他</td>
      <td>scope</td>
      <td>34.224172</td>
      <td>108.815592</td>
      <td>大明怀远将军园</td>
      <td>陕西省</td>
      <td>8ed2e7b406ac382b4bcc4a18</td>
      <td>NaN</td>
      <td>8ed2e7b406ac382b4bcc4a18</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 29 columns</p>
</div>



#### 3）将数据格式为DataFramed的POI数据转换为GeoPandas地理空间数据GeoDataFrame

可以使用GeoPandas将pandas的DataFrame和Series数据转换为具有地理意义的GeoDataFrame和GeoSeries地理数据。GeoPandas基于pandas，因此在数据结构上二者之间最大的区别是GeoPandas有一个列（column）名为'geometry'，用来存储几何数据，例如点数据`POINT (163.85316 -17.31631)`，单个多边形`POLYGON ((33.90371 -0.95000, 31.86617 -1.02736...`,多边形集合` MULTIPOLYGON (((120.83390 12.70450, 120.32344 ...`等，注意其中对几何数据的表达是使用shapely库实现，大部分的.shp地理信息矢量数据，在python语言中，通常使用该库为基础来建立几何对象。同时注意，GeoDataFrame对象的建立，需要配置坐标系统，这是地理信息数据显著的标志，对于坐标系统可以查看[spatialreference](https://spatialreference.org/ )

GeoDataFrame可以直接通过.plot()方法显示地理空间信息数据。


```python
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

poi_df=pd.read_pickle('./data/poi_df.pkl') #读取已经保存的.pkl(pickle)数据格式的POI
poi_2gdf=poi_df.copy(deep=True)
poi_2gdf['geometry']=poi_2gdf.apply(lambda row:Point(row.location_lng,row.location_lat),axis=1) 
epsg_wgs84=4326 #配置坐标系统，参考：https://spatialreference.org/  
poi_gdf=gpd.GeoDataFrame(poi_2gdf,crs=epsg_wgs84)

xian_region_gdf=gpd.read_file('./data/xian region/xian region.shp')

import matplotlib.pyplot as plt
from pylab import mpl
mpl.rcParams['font.sans-serif']=['DengXian'] #解决中文字符乱码问题

fig, ax=plt.subplots(figsize=(10,15))
xian_region_plot=xian_region_gdf.plot(ax=ax,color='white', edgecolor='black')
xian_region_gdf.apply(lambda row: xian_region_plot.annotate(s=row.NAME, xy=row.geometry.centroid.coords[0], ha='center',fontsize=13),axis=1) #标注
poi_gdf.plot(ax=ax,column='detail_info_comment_num',markersize=10)

plt.show()
```


    
<a href=""><img src="./imgs/2_1_1_01.png" height='auto' width='auto' title="caDesign"></a>
    


* 使用plotly库建立地图

geopandas库提供的地图显示方法功能有限，但是因为便捷，通常用于数据的查看。当需要地图具有一定质量，表达更多信息，甚至能够交互操作时，可以使用[plotly](https://plotly.com/)图表库实现（通常使用[Plotly Python](https://plotly.com/python/)）。其地图底图使用了[mapbox](https://www.mapbox.com/)提供的地图数据。要使用其功能需要注册，并获取访问许可（access token）。该部分操作相当便捷，可以自行查看。

使用plotly库建立地图，不需要将DataFrame转换为GeoDataFrame。


```python
import plotly.express as px

poi_gdf.detail_info_price=poi_gdf.detail_info_price.fillna(0) #pandas库的方法同样适用于geopandas库，例如对`nan`位置填充指定数值
mapbox_token='pk.eyJ1IjoicmljaGllYmFvIiwiYSI6ImNrYjB3N2NyMzBlMG8yc254dTRzNnMyeHMifQ.QT7MdjQKs9Y6OtaJaJAn0A'

px.set_mapbox_access_token(mapbox_token)
fig=px.scatter_mapbox(poi_gdf,lat=poi_gdf.location_lat, lon=poi_gdf.location_lng,color="detail_info_comment_num",color_continuous_scale=px.colors.cyclical.IceFire, size_max=15, zoom=10) #亦可以选择列，通过size=""配置增加显示信息
fig.show()
fig.write_html('./graph/POI_singlClassi.html')
```

<a href=""><img src="./imgs/2_1_1_02.jpg" height='auto' width='auto' title="caDesign"></a>


### 2.1.1.2 多个分类POI数据爬取

#### 1）多个分类POI爬取

前文定义了两个函数，‘百度地图开放平台POI数据爬取’和‘转换.csv格式的POI数据为pandas的DataFrame’。为了能够方便应用所建立的函数工具，使用Anaconda的Spyder创建一个新的文件为util_A.py，将上述两个函数置于其中，同时包括函数所使用的库。对于所包括的库，为方便日后函数迁移，明确每个函数所调用的库，将对应调用库的语句分别置于各个函数内部。util_A.py与调用该工具的代码文件置于同一文件夹下。调入语句如下：


```python
import util_A
```

根据百度地图一级行业分类，建立映射字典，用于多个分类POI的数据爬取。注意，可以根据数据的需求选择行业分类，在进一步分析中不需要的分类包括出入口、自然地物、行政地标和门址，因此在映射字典中未包含上述分类。


```python
poi_classificationName={
        "美食 ":"delicacy",
        #"酒店 ":"hotel",
        #"购物 ":"shopping",
        #"生活服务":"lifeService",
        #"丽人 ":"beauty",
        #"旅游景点":"spot",
        #"休闲娱乐":"entertainment",
        #"运动健身":"sports",
        #"教育培训":"education",
        #"文化传媒":"media",
        #"医疗 ":"medicalTreatment",
        #"汽车服务":"carService",
        #"交通设施":"trafficFacilities",
        #"金融":"finance",
        #"房地产":"realEstate",
        #"公司企业":"corporation",
        #"政府机构":"government"
        }
```

配置基本参数。上文配置用到`query_dic`。此次批量下载将所有参数在循环函数外以字典形式单独给出，方便调用。而`query_dic`字典参数在批量下载函数内配置。
```python
query_dic={
    'query':'旅游景点',
    'page_size':'20',
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0',
}
```


```python
poi_config_para={
    'data_path':'./data/poi_batchCrawler/', #配置数据存储位置
    'bound_coordinate':{'leftBottom':[108.776852,34.186027],'rightTop':[109.129275,34.382171]}, #百度地图坐标拾取系统 http://api.map.baidu.com/lbsapi/getpoint/index.html
    'page_num_range':range(20),
    'partition':6, #4
    'page_size':'20', #20
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0',
}
```

建立批量下载的循环函数，依据给出的`poi_classificationName`字典键值逐次调用单个分类POI爬取函数下载POI数据。在爬取的过程中，可以将每一次小批量下载数据存储在同一变量下，待全部下载完后一次性存储。但是这种一次性存储的方式并不推荐，其一，网络有时并不稳定，可能造成下载中断，那么已下载的数据未得以存储，造成数据丢失和不必要的重复下载；其二，有时数据量很大，如果都存储在一个变量下，可能造成内存溢出。


```python
def baiduPOI_batchCrawler(poi_config_para):
    import os
    import util_A
    
    '''
    function - 百度地图开放平台POI数据批量爬取，需要调用单个分类POI爬取函数baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=False)
    
    Paras:
        poi_config_para - 参数配置，包含：'data_path'（配置数据存储位置），'bound_coordinate'（矩形区域检索坐下、右上经纬度坐标），
                          'page_num_range'（配置页数范围），'partition'（检索区域切分次数），'page_size'（单次召回POI数量），'scope'（检索结果详细程度），'ak'（开发者的访问密钥）
    '''
    for idx,(poi_ClassiName,poi_classMapping) in enumerate(poi_classificationName.items()):
        print(str(idx+16)+"_"+poi_ClassiName)
        poi_subFileName="poi_"+str(idx+16)+"_"+poi_classMapping
        data_path=poi_config_para['data_path']
        poi_fn_csv=os.path.join(data_path,poi_subFileName+'.csv')
        poi_fn_json=os.path.join(data_path,poi_subFileName+'.json')
        
        query_dic={
            'query':poi_ClassiName,
            'page_size':poi_config_para['page_size'],
            'scope':poi_config_para['scope'],
            'ak':poi_config_para['ak']                        
        }
        bound_coordinate=poi_config_para['bound_coordinate']
        partition=poi_config_para['partition']
        page_num_range=poi_config_para['page_num_range']
        #调用单个分类POI爬取函数
        #print([poi_fn_csv,poi_fn_json])
        util_A.baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=[poi_fn_csv,poi_fn_json])  
        
baiduPOI_batchCrawler(poi_config_para)
```

    16_美食 
    Start downloading data...
    No.1 was written to the .csv file.
    No.2 was written to the .csv file.
    No.3 was written to the .csv file.
    No.4 was written to the .csv file.
    No.5 was written to the .csv file.
    No.6 was written to the .csv file.
    No.7 was written to the .csv file.
    No.8 was written to the .csv file.
    No.9 was written to the .csv file.
    No.10 was written to the .csv file.
    No.11 was written to the .csv file.
    No.12 was written to the .csv file.
    No.13 was written to the .csv file.
    No.14 was written to the .csv file.
    No.15 was written to the .csv file.
    No.16 was written to the .csv file.
    No.17 was written to the .csv file.
    No.18 was written to the .csv file.
    No.19 was written to the .csv file.
    No.20 was written to the .csv file.
    No.21 was written to the .csv file.
    No.22 was written to the .csv file.
    No.23 was written to the .csv file.
    No.24 was written to the .csv file.
    No.25 was written to the .csv file.
    No.26 was written to the .csv file.
    No.27 was written to the .csv file.
    No.28 was written to the .csv file.
    No.29 was written to the .csv file.
    No.30 was written to the .csv file.
    No.31 was written to the .csv file.
    No.32 was written to the .csv file.
    No.33 was written to the .csv file.
    No.34 was written to the .csv file.
    No.35 was written to the .csv file.
    No.36 was written to the .csv file.
    The download is complete.
    

#### 2）批量转换.csv格式数据为GeoDataFrame

在单个分类部分是逐步实现.csv格式数据到GeoDataFrame数据的转换。基于已有代码，该部分将达到两个目的，一个是定义单独函数实现.csv批量转换为GeoDataFame格式数据并存储为.pkl文件；二是批量读取存储为.pkl文件的GeoDataFrame格式数据，并根据需要提取信息存储在单一变量下，并再存储为.pkl文件。需要注意，当读取所有数据于单一变量时，内存需要满足要求，如果有内存溢出，则需要考虑是否根据内存情况调整每次读取的数据数量。

##### 1. 定义提取文件夹下所有文件路径的函数

因为批量下载POI数据为多个.csv文件及.json文件，因此在批量处理这些数据时，第一件事是要提取所有文件的路径。定义返回所有指定后缀名文件路径的函数是最为常用的函数之一，在之后的很多实验中，都需要调用该函数，因此可以将其保存在util_misc.py文件中，方便日后调用。同时需要注意，文件夹下通常包括子文件夹，需要`os.walk()`遍历目录，给出条件语句判断是否存在子文件夹，如果存在则需要返回该文件夹下的文件路径。


```python
def filePath_extraction(dirpath,fileType):
    import os
    
    '''
    funciton  -以所在文件夹路径为键，值为包含该文件夹下所有文件名的列表。文件类型可以自行定义 
    
    Paras:
        dirpath - 根目录，存储所有待读取的文件
        fileType - 待读取文件的类型
    '''
    filePath_Info={}
    i=0
    for dirpath,dirNames,fileNames in os.walk(dirpath): #os.walk()遍历目录，使用help(os.walk)查看返回值解释
       i+=1
       if fileNames: #仅当文件夹中有文件时才提取
           tempList=[f for f in fileNames if f.split('.')[-1] in fileType]
           if tempList: #剔除文件名列表为空的情况，即文件夹下存在不为指定文件类型的文件时，上一步列表会返回空列表[]
               filePath_Info.setdefault(dirpath,tempList)
    return filePath_Info

dirpath='./data/poi_batchCrawler/'
fileType=["csv"]
poi_paths=filePath_extraction(dirpath,fileType)
print(poi_paths)
```

    {'./data/poi_batchCrawler/': ['poi_0_delicacy.csv', 'poi_10_medicalTreatment.csv', 'poi_11_carService.csv', 'poi_12_trafficFacilities.csv', 'poi_13_finance.csv', 'poi_14_realEstate.csv', 'poi_15_corporation.csv', 'poi_16_government.csv', 'poi_1_hotel.csv', 'poi_2_shopping.csv', 'poi_3_lifeService.csv', 'poi_4_beauty.csv', 'poi_5_spot.csv', 'poi_6_entertainment.csv', 'poi_7_sports.csv', 'poi_8_education.csv', 'poi_9_media.csv']}
    

##### 2. .csv格式POI数据批量转换为GeoDataFrame

```python
Index(['address', 'area', 'city', 'detail', 'detail_info_checkin_num','detail_info_children', 'detail_info_comment_num',
       'detail_info_detail_url', 'detail_info_facility_rating','detail_info_favorite_num', 'detail_info_hygiene_rating',
       'detail_info_image_num', 'detail_info_indoor_floor','detail_info_navi_location_lat', 'detail_info_navi_location_lng',
       'detail_info_overall_rating', 'detail_info_price','detail_info_service_rating', 'detail_info_tag', 'detail_info_type',
       'location_lat', 'location_lng', 'name', 'province', 'street_id','telephone', 'uid'],dtype='object')
```
上述为POI数据字段，可以用来确定提取的字段名。除了增加循环语句循环POI的.csv文件逐一转换为pandas的DataFrame格式，再进一步转换为geopandas的GeoDataFrame格式之外，其它的条件同前文所述。直接GeoDataFrame.plot()方式初步查看地理空间信息数据。在文件保存部分，可以有多种保存格式，GeoPandas提供了Shapefile、GeoJSON和GeoPackage三种方式，以及pickle，或存入数据库。GeoPandas提供的保存格式再读取后不再包含多重索引，而pickle格式则保持。转换为.shp格式文件时，在QGIS等桌面GIS平台下打开时会出现两个问题，一个是如果列名称过长，转化为属性表的字段名会被字段压缩修改，往往不能反映字段的意义，因此需要置换列名称；二是用POI一级行业分类名作为index时，列中并不包含该字段，转化为Shapefile文件时也不包含该字段，因此需要将index转换为列，再存储为.shp文件。


```python
fields_extraction=['name','location_lat', 'location_lng','detail_info_tag','detail_info_overall_rating', 'detail_info_price'] #配置需要提取的字段，即列（columns）
save_path={'geojson':'./data/poisInAll/poisInAll_gdf.geojson','shp':'./data/poisInAll/poisInAll_gdf.shp','pkl':'./data/poisInAll/poisInAll_gdf.pkl'} #分别存储为GeoJSON、Shapefile和pickle三种数据格式

def poi_csv2GeoDF_batch(poi_paths,fields_extraction,save_path):
    import os,pathlib
    import util_A
    import pandas as pd
    import geopandas as gpd
    from shapely.geometry import Point
    from pyproj import CRS
    
    '''
    funciton - .csv格式POI数据批量转换为GeoDataFrame，需要调用转换.csv格式的POI数据为pandas的DataFrame函数csv2df(poi_fn_csv)
    
    Paras:
        poi_paths - 文件夹路径为键，值为包含该文件夹下所有文件名列表的字典
        fields_extraction - 配置需要提取的字段
        save_path - 存储数据格式及保存路径的字典
    '''
    #循环读取与转换poi的.csv文件为pandas的DataFrame数据格式
    poi_df_dic={}
    i=0
    for key in poi_paths:
        for val in poi_paths[key]:
            poi_csvPath=os.path.join(key,val)
            poi_df=util_A.csv2df(poi_csvPath) #注释掉了csv2df()函数内部的print("%s data type is not converted..."%(col))语句，以pass替代，减少提示内容，避免干扰
            print(val)
            poi_df_path=pathlib.Path(val)
            poi_df_dic[poi_df_path.stem]=poi_df
            
            #if i==2:break
            i+=1
    poi_df_concat=pd.concat(poi_df_dic.values(),keys=poi_df_dic.keys(),sort=True)
    #print(poi_df_concat.loc[['poi_0_delicacy'],:]) #提取index为 'poi_0_delicacy'的行，验证结果
    poi_fieldsExtraction=poi_df_concat.loc[:,fields_extraction]
    poi_geoDF=poi_fieldsExtraction.copy(deep=True)
    poi_geoDF['geometry']=poi_geoDF.apply(lambda row:Point(row.location_lng,row.location_lat),axis=1) 
    crs_4326=CRS('epsg:4326') #配置坐标系统，参考：https://spatialreference.org/        
    poisInAll_gdf=gpd.GeoDataFrame(poi_geoDF,crs=crs_4326)
    
    poisInAll_gdf.to_pickle(save_path['pkl'])
    poisInAll_gdf.to_file(save_path['geojson'],driver='GeoJSON',encoding='utf-8')
    
    poisInAll_gdf2shp=poisInAll_gdf.reset_index() #不指定level参数，例如Level=0，会把多重索引中的所有索引转换为列
    poisInAll_gdf2shp.rename(columns={
        'location_lat':'lat', 'location_lng':'lng',
        'detail_info_tag':'tag','detail_info_overall_rating':'rating', 'detail_info_price':'price'},inplace=True)
    poisInAll_gdf2shp.to_file(save_path['shp'],encoding='utf-8')
        
    return poisInAll_gdf
            
poisInAll_gdf=poi_csv2GeoDF_batch(poi_paths,fields_extraction,save_path)
```

    __________________________________________________
    .csv to DataFrame completed!
    poi_0_delicacy.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_10_medicalTreatment.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_11_carService.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_12_trafficFacilities.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_13_finance.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_14_realEstate.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_15_corporation.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_16_government.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_1_hotel.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_2_shopping.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_3_lifeService.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_4_beauty.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_5_spot.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_6_entertainment.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_7_sports.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_8_education.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_9_media.csv
    


```python
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import geopandas as gpd
from pylab import mpl
mpl.rcParams['font.sans-serif']=['DengXian'] #解决中文字符乱码问题

xian_region_gdf=gpd.read_file('./data/xian region/xian region.shp')

fig, ax=plt.subplots(figsize=(15,20))
xian_region_plot=xian_region_gdf.plot(ax=ax,color='white', edgecolor='black')
xian_region_gdf.apply(lambda row: xian_region_plot.annotate(s=row.NAME, xy=row.geometry.centroid.coords[0], ha='center',fontsize=13),axis=1) #标注

divider=make_axes_locatable(ax)
cax=divider.append_axes("right", size="5%", pad=0.1)
poisInAll_gdf.loc[['poi_0_delicacy'],:].plot(ax=ax,column='detail_info_overall_rating',markersize=10,cmap='cividis',legend=True,cax=cax) #提取index为'poi_0_delicacy'的行查看结果

plt.show()
```


    
<a href=""><img src="./imgs/2_1_1_03.png" height='auto' width='auto' title="caDesign"></a>
    


> 在[QGIS开源桌面GIS平台](https://www.qgis.org/en/site/index.html)下打开保存的.shp数据。虽然所有工作基本都是在python中完成，但是有些工作与其它平台相互联系，需要以这些平台为辅助。在辅助平台选择上，我们尽可能使用具有广泛应用的开源软件。在地理信息系统（GIS，Geographic Information Systems）中主要使用的集成式平台有QGIS和ArcGIS。

<a href=""><img src="./imgs/2_1_1_23.jpg" height='auto' width='auto' title="caDesign"></a>


#### 3）使用plotly库建立地图

用颜色表示POI一级分类，用大小表示rating字段。


```python
import geopandas as gpd
import plotly.express as px

poi_gdf=gpd.read_file('./data/poisInAll/poisInAll_gdf.shp') #读取存储的.shp格式文件
poi_gdf.rating=poi_gdf.rating.fillna(0) #pandas库的方法同样适用于geopandas库，例如对`nan`位置填充指定数值
mapbox_token='pk.eyJ1IjoicmljaGllYmFvIiwiYSI6ImNrYjB3N2NyMzBlMG8yc254dTRzNnMyeHMifQ.QT7MdjQKs9Y6OtaJaJAn0A'

px.set_mapbox_access_token(mapbox_token)
fig=px.scatter_mapbox(poi_gdf,lat=poi_gdf.lat, lon=poi_gdf.lng,color="level_0",size='rating',color_continuous_scale=px.colors.cyclical.IceFire, size_max=5, zoom=10) #亦可以选择列，通过size=""配置增加显示信息
fig.show()
```

<a href=""><img src="./imgs/2_1_1_04.jpg" height='auto' width='auto' title="caDesign"></a>

### 2.1.1.3 描述性统计图表

#### 1）读取与查看数据

* 读取已经保存的.pkl数据。通过.plot()确认读取的数据是否正常，或者直接`poi_gdf.head()`查看数据。


```python
import pandas as pd

poi_gdf=pd.read_pickle('./data/poisInAll/poisInAll_gdf.pkl')
poi_gdf.plot(marker=".",markersize=5,column='detail_info_overall_rating',figsize=(10,15)) #只有不设置columns参数时，可以使用color='green'参数
print(poi_gdf.columns) #查看列名称
```

    Index(['name', 'location_lat', 'location_lng', 'detail_info_tag',
           'detail_info_overall_rating', 'detail_info_price', 'geometry'],
          dtype='object')
    


    
<a href=""><img src="./imgs/2_1_1_05.png" height='auto' width='auto' title="caDesign"></a>
    


#### 2）用plotly表格显示DataFrame数据

print()是查看数据最为主要的方式，主要用于代码调试。当需要展示数据时，对于DataFrame格式的数据可以直接使用plotly转换为表格形式，因为POI数据有万行之多，仅显示每一一级行业分类(含17类)的前两行的内容，总共$2\times17=34$行。为方便调用，将其功能定义为一个函数。提取数据时，因为数据格式是多重索引DataFrame，因此使用pandas.IndexSlice()函数辅助执行多重索引切分。同时用plotly显示表格时，如果为多重索引则会显示错误，因此需要df.reset_index()重置索引。plotly也不能够显示'geometry'几何对象，在列提取时需要移除该列。


```python
df=poi_gdf.loc[pd.IndexSlice[:,:2],:]
df=df.reset_index()
column_extraction=['level_0','name', 'location_lat', 'location_lng', 'detail_info_tag','detail_info_overall_rating', 'detail_info_price']

def ployly_table(df,column_extraction):
    import plotly.graph_objects as go
    import pandas as pd
    '''
    funciton - 使用plotly以表格形式显示DataFrame格式数据
    
    Paras:
        df - 输入的DataFrame或者GeoDataFrame
        column_extraction - 提取的字段（列）
    '''
    fig=go.Figure(data=[go.Table(
        header=dict(values=column_extraction,
                    fill_color='paleturquoise',
                    align='left'),
        cells=dict(values=df[column_extraction].values.T.tolist(), #values参数值为按列的嵌套列表，因此需要使用参数.T反转数组
                   fill_color='lavender',
                   align='left'))
    ])
    fig.show()    
    
ployly_table(df,column_extraction)
```

<a href=""><img src="./imgs/2_1_1_07.png" height='auto' width='auto' title="caDesign"></a>

#### 3） 描述性统计

> 参考 (日)高桥 信著,株式会社TREND-PRO漫画制作，陈刚译.漫画统计学[M].科学出版社.北京.  以及维基百科（Wikipedia）。枯燥的知识以漫画的方式讲出来，并结合实际的案例由简入繁使枯燥的学习变得有趣起来，欧姆社学习漫画系列和众多以漫画和图示的方式讲解知识的优秀图书都值得推荐。但是有利有弊，大部分的漫画图书往往以基础知识为主，深入的研究还是要搜索科学文献和相关论著。同时漫画形式可以引起读者的兴趣，但是因为穿插故事情节，知识点不易定位，核心的知识内容相对分散，阅读上也要花费更多的时间。因此想学习一门知识，想以哪种形式入手，需要根据个人的情况确定。

描述性统计分析是对调查总体所有变量的有关数据做统计性描述，了解各变量内的观察值集中与分散的情况。

-表示集中趋势（集中量数）的有平均数、中位数、众数、几何平均数、调和平均数等；表示离散程度（变异量数）的有极差（全距）、平均差、标准差、相对差、四分差等。数据的次数分配情况，往往会呈现正态分布。

-数据的频数（次数）分配情况，往往会呈现正态分布。为了表示测量数据与正态分布偏离的情况，会使用偏度、峰度这两种统计数据。

-为了解个别观察值在整体中所占的位置，会需要将观察值转换为相对量数，如百分等级、标准分数、四分位数等。

通常在描述性统计中将数据图表化，以直观的方式了解整体资料分布的情况，包括直方图、散点图、饼图、折现图、箱型图等

##### 1. 数据种类

通常数据可以分为两类，不可测量的数据称为分类数据；可测量的数据称为数值数据。上述图表中'level_0', 'detail_info_tag'字段均为分类数据，'location_lat', 'location_lng','detail_info_overall_rating', 'detail_info_price'均为数值数据。而'name'字段则是数据的索引名称。

##### 2. 数值数据的描述性统计

* 频数（次数）分布表和直方图

建立简单数据示例，数据来源于《漫画统计学》'美味拉面畅销前50'上刊载的拉面馆的拉面价格。虽然一组数据通常使用pandas.Series()建立Series格式数据，但是后续分析会加入新的数据，因此仍旧建立DataFrame格式数据。使用df.describe()可以粗略查看主要统计值。


```python
ranmen_price=pd.DataFrame([700,850,600,650,980,750,500,890,880,700,890,720,680,650,790,670,680,900,880,720,850,700,780,850,750,
                           780,590,650,580,750,800,550,750,700,600,800,800,880,790,790,780,600,690,680,650,890,930,650,777,700],columns=["price"])
print(ranmen_price.describe())
```

                price
    count   50.000000
    mean   743.340000
    std    108.261891
    min    500.000000
    25%    672.500000
    50%    750.000000
    75%    800.000000
    max    980.000000
    

因为有些价格是相同的，一般可以直接使用上述ranmen_price数据直接计算频数，但是很多时候相同的数据并不多，而且希望分析内容为数值区段间的比较，分析才更具有意义，因此转换为相对量数，以100间隔为一级。范围根据数据的最大和最小值来确定。


```python
bins=range(500,1000+100,100) #配置分割区间（组距）
ranmen_price['price_bins']=pd.cut(x=ranmen_price.price,bins=bins,right=False) #参数right=False指定为包含左边值，不包括右边值。
ranmenPrice_bins=ranmen_price.sort_values(by=['price']) #按照分割区间排序
ranmenPrice_bins.set_index(['price_bins',ranmenPrice_bins.index],drop=False,inplace=True) #以price_bins和原索引值设置多重索引，同时配置drop=False参数保留原列。
ranmenPrice_bins.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>price</th>
      <th>price_bins</th>
    </tr>
    <tr>
      <th>price_bins</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="4" valign="top">[500, 600)</th>
      <th>6</th>
      <td>500</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>31</th>
      <td>550</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>28</th>
      <td>580</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>26</th>
      <td>590</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">[600, 700)</th>
      <th>34</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>41</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>44</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>27</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
  </tbody>
</table>
</div>



数值区段间的频数计算


```python
ranmenPriceBins_frequency=ranmenPrice_bins.price_bins.value_counts() #dropna=False  
ranmenPriceBins_relativeFrequency=ranmenPrice_bins.price_bins.value_counts(normalize=True) #参数normalize=True将计算相对频数(次数)
ranmenPriceBins_freqANDrelFreq=pd.DataFrame({'fre':ranmenPriceBins_frequency,'relFre':ranmenPriceBins_relativeFrequency})
ranmenPriceBins_freqANDrelFreq
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fre</th>
      <th>relFre</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>[700, 800)</th>
      <td>18</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>[600, 700)</th>
      <td>13</td>
      <td>0.26</td>
    </tr>
    <tr>
      <th>[800, 900)</th>
      <td>12</td>
      <td>0.24</td>
    </tr>
    <tr>
      <th>[500, 600)</th>
      <td>4</td>
      <td>0.08</td>
    </tr>
    <tr>
      <th>[900, 1000)</th>
      <td>3</td>
      <td>0.06</td>
    </tr>
  </tbody>
</table>
</div>



组中值计算


```python
ranmenPriceBins_median=ranmenPrice_bins.median(level=0)
ranmenPriceBins_median.rename(columns={'price':'median'},inplace=True)
ranmenPriceBins_median
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median</th>
    </tr>
    <tr>
      <th>price_bins</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>[500, 600)</th>
      <td>565</td>
    </tr>
    <tr>
      <th>[600, 700)</th>
      <td>650</td>
    </tr>
    <tr>
      <th>[700, 800)</th>
      <td>750</td>
    </tr>
    <tr>
      <th>[800, 900)</th>
      <td>865</td>
    </tr>
    <tr>
      <th>[900, 1000)</th>
      <td>930</td>
    </tr>
  </tbody>
</table>
</div>



合并分割区间、频数计算和组中值的DataFrame格式数据。


```python
ranmen_fre=ranmenPriceBins_freqANDrelFreq.join(ranmenPriceBins_median).sort_index().reset_index() #在合并时会自动匹配index
ranmen_fre
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[500, 600)</td>
      <td>4</td>
      <td>0.08</td>
      <td>565</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[600, 700)</td>
      <td>13</td>
      <td>0.26</td>
      <td>650</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[700, 800)</td>
      <td>18</td>
      <td>0.36</td>
      <td>750</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[800, 900)</td>
      <td>12</td>
      <td>0.24</td>
      <td>865</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[900, 1000)</td>
      <td>3</td>
      <td>0.06</td>
      <td>930</td>
    </tr>
  </tbody>
</table>
</div>



计算频数比例，即各个区间频数占总数的百分比，能够更清晰比较之间的差异大小。配合使用df.apply()和lambda匿名函数。这两个方法经常被用到，能够巧妙的以一种简洁的方式解决相关问题。


```python
ranmen_fre['fre_percent%']=ranmen_fre.apply(lambda row:row['fre']/ranmen_fre.fre.sum()*100,axis=1)
ranmen_fre
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
      <th>fre_percent%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[500, 600)</td>
      <td>4</td>
      <td>0.08</td>
      <td>565</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[600, 700)</td>
      <td>13</td>
      <td>0.26</td>
      <td>650</td>
      <td>26.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[700, 800)</td>
      <td>18</td>
      <td>0.36</td>
      <td>750</td>
      <td>36.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[800, 900)</td>
      <td>12</td>
      <td>0.24</td>
      <td>865</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[900, 1000)</td>
      <td>3</td>
      <td>0.06</td>
      <td>930</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>



直方图（pandas） pandas自身就带有不少图表打印的功能（基于Matplotlib库），可以迅速的预览，而不必要过多的调整数据结构。但是不像plotly具有交互功能。


```python
ranmen_fre.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(5,5))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x287cf8a4d30>




    
<a href=""><img src="./imgs/2_1_1_08.png" height='auto' width='auto' title="caDesign"></a>
    


有了上述简单数据的示例，再返回到POI实验数据，可以直接迁移上述代码，略作调整后分析POI一级分类美食'poi_0_delicacy'的价格总体分布情况。将上述的所有分析代码放置于一个函数中，函数的主要功能是计算DataFrame数据格式下，指定组距，一列数据的频数分布。将上述零散的代码纳入到一个函数中，需要注意几点事宜。一个是尽可能让变量名具有普适性，例如原变量名ranmenPrice_bins在函数中更改为df_bins，因为该函数同样可以计算'detail_info_overall_rating'字段的频数分布情况；二是公用的常用变量尽量开始配置，例如column_name和column_bins_name，这样避免不断的用原始的语句，例如重复使用`df.columns[0]+'_bins'`，从而导致代码可读性较差；再者，基本不可能直接迁移上述代码于函数中使用，需要逐句或者逐段的迁移测试，例如原代码`ranmenPrice_bins.price_bins`，在函数中改为df_bins[column_bins_name]，因为列名是以变量名形式存储，无法直接用`.`的方式读取数据。最后是需要注意函数返回值的灵活性，例如并未在函数内部定义图表打印，而是返回DataFrame格式的数据，因为打印的方式比较多样化，可以仅打印一列数据的柱状图，或者多列，以及图表的形式也会多样化，因此这部分工作放置于函数外处理，增加灵活性。


```python
def frequency_bins(df,bins,field):
    import pandas as pd
    '''
    function - 频数分布计算
    
    Paras:
        df - 单列（数值类型）的DataFrame数据
        bins - 配置分割区间（组距）
        field - 字段名
    '''
    
    #A-组织数据
    column_name=df.columns[0]
    column_bins_name=df.columns[0]+'_bins'
    df[column_bins_name]=pd.cut(x=df[column_name],bins=bins,right=False) #参数right=False指定为包含左边值，不包括右边值。
    df_bins=df.sort_values(by=[column_name]) #按照分割区间排序
    df_bins.set_index([column_bins_name,df_bins.index],drop=False,inplace=True) #以price_bins和原索引值设置多重索引，同时配置drop=False参数保留原列。
    #print(df_bins.head(10))
    
    #B-频数计算
    dfBins_frequency=df_bins[column_bins_name].value_counts() #dropna=False  
    dfBins_relativeFrequency=df_bins[column_bins_name].value_counts(normalize=True) #参数normalize=True将计算相对频数(次数) dividing all values by the sum of values
    dfBins_freqANDrelFreq=pd.DataFrame({'fre':dfBins_frequency,'relFre':dfBins_relativeFrequency})
    #print(dfBins_freqANDrelFreq)
    
    #C-组中值计算
    df_bins[field]=df_bins[field].astype(float)
    dfBins_median=df_bins.median(level=0)
    dfBins_median.rename(columns={column_name:'median'},inplace=True)
    #print(dfBins_median)
    
    #D-合并分割区间、频数计算和组中值的DataFrame格式数据。
    df_fre=dfBins_freqANDrelFreq.join(dfBins_median).sort_index().reset_index() #在合并时会自动匹配index
    #print(ranmen_fre)
    
    #E-计算频数比例
    df_fre['fre_percent%']=df_fre.apply(lambda row:row['fre']/df_fre.fre.sum()*100,axis=1)
    
    return df_fre

pd.options.mode.chained_assignment=None
bins=range(0,600+50,50) #配置分割区间（组距）  
delicacy_price_df=poi_gdf[["detail_info_price"]]
delicacy_price_df["detail_info_price"]=delicacy_price_df["detail_info_price"].apply(pd.to_numeric,errors='coerce') #将str(object)数据类型转换为数值类型
poiPrice_fre_50=frequency_bins(delicacy_price_df.dropna(),bins,"detail_info_price")    
poiPrice_fre_50
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
      <th>fre_percent%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[0, 50)</td>
      <td>2824</td>
      <td>0.445918</td>
      <td>20.0</td>
      <td>44.591821</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[50, 100)</td>
      <td>2030</td>
      <td>0.320543</td>
      <td>73.0</td>
      <td>32.054319</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[100, 150)</td>
      <td>689</td>
      <td>0.108795</td>
      <td>115.0</td>
      <td>10.879520</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[150, 200)</td>
      <td>286</td>
      <td>0.045160</td>
      <td>170.0</td>
      <td>4.516027</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[200, 250)</td>
      <td>164</td>
      <td>0.025896</td>
      <td>223.0</td>
      <td>2.589610</td>
    </tr>
    <tr>
      <th>5</th>
      <td>[250, 300)</td>
      <td>145</td>
      <td>0.022896</td>
      <td>268.0</td>
      <td>2.289594</td>
    </tr>
    <tr>
      <th>6</th>
      <td>[300, 350)</td>
      <td>71</td>
      <td>0.011211</td>
      <td>321.0</td>
      <td>1.121112</td>
    </tr>
    <tr>
      <th>7</th>
      <td>[350, 400)</td>
      <td>45</td>
      <td>0.007106</td>
      <td>373.0</td>
      <td>0.710564</td>
    </tr>
    <tr>
      <th>8</th>
      <td>[400, 450)</td>
      <td>25</td>
      <td>0.003948</td>
      <td>423.0</td>
      <td>0.394758</td>
    </tr>
    <tr>
      <th>9</th>
      <td>[450, 500)</td>
      <td>21</td>
      <td>0.003316</td>
      <td>475.0</td>
      <td>0.331596</td>
    </tr>
    <tr>
      <th>10</th>
      <td>[500, 550)</td>
      <td>21</td>
      <td>0.003316</td>
      <td>522.0</td>
      <td>0.331596</td>
    </tr>
    <tr>
      <th>11</th>
      <td>[550, 600)</td>
      <td>12</td>
      <td>0.001895</td>
      <td>573.5</td>
      <td>0.189484</td>
    </tr>
  </tbody>
</table>
</div>




```python
poiPrice_fre_50.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(15,5))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x287d0428340>




    
<a href=""><img src="./imgs/2_1_1_09.png" height='auto' width='auto' title="caDesign"></a>
    


调整组距，查看总体频数分布情况


```python
import matplotlib.pyplot as plt

bins=list(range(0,300+5,5))+[600] #通过df.describe()查看数据后，发现72%的价格位于72元之下，结合上图柱状图，重新配置组距，尽可能的显示数据变化的趋势。
poiPrice_fre_5=frequency_bins(delicacy_price_df,bins,"detail_info_price")    
poiPrice_fre_5.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(30,5))
plt.xticks(rotation=90)
```




    (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,
            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,
            51, 52, 53, 54, 55, 56, 57, 58, 59, 60]),
     <a list of 61 Text major ticklabel objects>)




    
<a href=""><img src="./imgs/2_1_1_10.png" height='auto' width='auto' title="caDesign"></a>
    


* 集中量数与变异量数

建立简单数据示例，数据来源于《漫画统计学》保龄球大赛的结果。首先建立嵌套字典，然后将其转换为多重索引的DataFrame数据。


```python
bowlingContest_scores_dic={'A_team':{'Barney':86,'Harold':73,'Chris':124,'Neil':111,'Tony':90,'Simon':38},
                            "B_team":{'Jo':84,'Dina':71,'Graham':103,'Joe':85,'Alan':90,'Billy':89},
                            'C_team':{'Gordon':229,'Wade':77,'Cliff':59,'Arthur':95,'David':70,'Charles':88}
                          }
bowlingContest_scores=pd.DataFrame.from_dict(bowlingContest_scores_dic, orient='index').stack().to_frame(name='score') #可以逐步拆解来查看每一步的数据结构，结合搜索相关方法解释，可以理解每一步的作用。例如df.stack()是由列返回多重索引的DataFrame,具体可以查看官方案例，更直观的理解其作用
bowlingContest_scores #使用print()或者直接在每一JupyterLab的Cell最后给出要查看的变量名，都可以查看数据，只是可能显示的模式略有差异。但是建议使用print()查看，因为涉及到代码迁移时，单独变量的出现可能会造成代码运行错误。
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="6" valign="top">A_team</th>
      <th>Barney</th>
      <td>86.0</td>
    </tr>
    <tr>
      <th>Harold</th>
      <td>73.0</td>
    </tr>
    <tr>
      <th>Chris</th>
      <td>124.0</td>
    </tr>
    <tr>
      <th>Neil</th>
      <td>111.0</td>
    </tr>
    <tr>
      <th>Tony</th>
      <td>90.0</td>
    </tr>
    <tr>
      <th>Simon</th>
      <td>38.0</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">B_team</th>
      <th>Jo</th>
      <td>84.0</td>
    </tr>
    <tr>
      <th>Dina</th>
      <td>71.0</td>
    </tr>
    <tr>
      <th>Graham</th>
      <td>103.0</td>
    </tr>
    <tr>
      <th>Joe</th>
      <td>85.0</td>
    </tr>
    <tr>
      <th>Alan</th>
      <td>90.0</td>
    </tr>
    <tr>
      <th>Billy</th>
      <td>89.0</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">C_team</th>
      <th>Gordon</th>
      <td>229.0</td>
    </tr>
    <tr>
      <th>Wade</th>
      <td>77.0</td>
    </tr>
    <tr>
      <th>Cliff</th>
      <td>59.0</td>
    </tr>
    <tr>
      <th>Arthur</th>
      <td>95.0</td>
    </tr>
    <tr>
      <th>David</th>
      <td>70.0</td>
    </tr>
    <tr>
      <th>Charles</th>
      <td>88.0</td>
    </tr>
  </tbody>
</table>
</div>



求每一队的均值（算数平均数）


```python
bowlingContest_mean=bowlingContest_scores.mean(level=0)
bowlingContest_mean
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>103.0</td>
    </tr>
  </tbody>
</table>
</div>



求每一队的中位数。C队的均值最高，其原因不是每一个队员的成绩都高，而是Gordon获得了229远超其他队员的得分。因此求中位数更为适合。


```python
bowlingContest_median=bowlingContest_scores.median(level=0)
bowlingContest_median
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>88.0</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>82.5</td>
    </tr>
  </tbody>
</table>
</div>



箱型图（Box plot），又称盒须图，盒式（状）图或箱线图，一种用作显示一组数据分散情况的统计图。显示的一组数据包括最大值，最小值，中位数和上下四分位数，因此使用箱型图较之单一的数值而言可以更清晰的观察数据分布情况。如图（Wikipedia）：

```
                            +-----+-+       
  *           o     |-------|   + | |---|
                            +-----+-+    
                                         
+---+---+---+---+---+---+---+---+---+---+   分数
0   1   2   3   4   5   6   7   8   9  10
```

这组数据显示：最小值（minimum）=5；下四分位数（Q1）=7；中位数（Med,即Q2）=8.5；上四分位数（Q3）=9；最大值（maximum）=10；平均值=8；四分位间距(interquartile range)=(Q3-Q2)=2(即ΔQ)。使用pandas自带的plot功能打印箱型图，查看各队分数的分布情况。


```python
bowlingContest_scores_transpose=bowlingContest_scores.stack().unstack(level=0)
boxplot=bowlingContest_scores_transpose.boxplot(column=['A_team', 'B_team', 'C_team'])
```


    
<a href=""><img src="./imgs/2_1_1_11.png" height='auto' width='auto' title="caDesign"></a>
    


plotly库所提供的箱型图可以互动显示具体的数值，具有相对更强的图示能力。


```python
import plotly.express as px

fig=px.box(bowlingContest_scores.xs('C_team',level=0), y="score")
fig.update_layout(
    autosize=False,
    width=500,
    height=500,
    title='C_team'
    )
fig.show()
```

<a href=""><img src="./imgs/2_1_1_12.jpg" height='auto' width='500' title="caDesign"></a>


求标准差，又称准偏差或均方差（Standard Deviation，缩写SD），数学符号通常用σ（sigma）。用于测量一组数值的离散程度，公式：$SD= \sqrt{ \frac{1}{N}  \sum_{i=1}^N { ( x_{i}- \mu )^{2}  } } $ 其中$\mu$ 为平均值。虽然A_team和B_team具有相同的均值，但是数值的分布情况迥然不同，通过计算标准差比较离散程度，标准差越小，代表数据的离散程度越小；反之，标准差越大，离散程度越大。

> 注意上述公式为整体标准差，本书的实验数据通常为全部数据样本，而不是抽样，因此使用整体标准差。计算样本的标准差，公式为：$SD= \sqrt{ \frac{1}{N-1}  \sum_{i=1}^N { ( x_{i}- \mu )^{2}  } }$


```python
bowlingContest_std=bowlingContest_scores.std(level=0)
bowlingContest_std
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>30.172835</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>10.373042</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>63.033325</td>
    </tr>
  </tbody>
</table>
</div>



返回到POI实验数据，因为一级行业分类中美食部分的'detial_info_tag'包含餐厅的子分类，使用箱型图显示子分类评分'detail_info_overall_rating'的数值分布情况，以及计算标准差。


```python
delicacy=poi_gdf.xs('poi_0_delicacy',level=0)
delicacy_rating=delicacy[['detail_info_tag','detail_info_overall_rating']] 
delicacy_rating
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_tag</th>
      <th>detail_info_overall_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>美食;中餐厅</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>美食;中餐厅</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>美食;中餐厅</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>美食;中餐厅</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>8</th>
      <td>美食;中餐厅</td>
      <td>4.6</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6590</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6592</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6594</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6596</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6598</th>
      <td>美食;茶座</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>3300 rows × 2 columns</p>
</div>



查看餐厅类型。并移除错误的分类数据，例如`'教育培训;其他'`。同时可以调整子分类的名称，例如由'美食;中餐厅' 修改为'中餐厅'，其中使用了df.applay()方法。最后将其映射为英文字符，在打印时也可以避免显示错误，如果显示中文字符错误，需要增加相应处理语句。


```python
pd.options.mode.chained_assignment = None
print(delicacy_rating.detail_info_tag.unique())
print("_"*50)
delicacy_rating["clean_bool"]=delicacy_rating.detail_info_tag.apply(lambda row:row.split(";")[0]=="美食" if isinstance(row,str) else False)
delicacy_rating_clean=delicacy_rating[delicacy_rating.clean_bool].drop(columns=["clean_bool"]) 
print(delicacy_rating_clean.detail_info_tag.unique())
print("_"*50)

#定义一个函数，传入df.apply()函数处理字符串
def str_row(row):
    if type(row)==str:
        row_=row.split(';')[-1]
    else:
        #print(type(row))
        row_='nan' #原数据类型为nan，通过type(row)查看后为float数据类型，此时将其转换为字符串
    return row_
delicacy_rating_clean.loc[:,["detail_info_tag"]]=delicacy_rating_clean["detail_info_tag"].apply(str_row)  
print(delicacy_rating_clean.detail_info_tag.unique())
print("_"*50)

tag_mapping={'中餐厅':'Chinese_restaurant','小吃快餐店':'Snake_bar','nan':'nan','其他':'others','外国餐厅':'Foreign_restaurant',
             '蛋糕甜品店':'CakeANDdessert_shop','咖啡厅':'cafe','茶座':'teahouse','酒吧':'bar','美食':'delicacy','公司':'company',
             '商铺':'store','洗浴按摩':'massage','超市':'supermarket','快捷酒店':'budgetHotel','园区':'Park'}
delicacy_rating_clean.loc[:,["detail_info_tag"]]=delicacy_rating_clean["detail_info_tag"].replace(tag_mapping)
print(delicacy_rating_clean.detail_info_tag.unique())
```

    ['美食;中餐厅' '美食;其他' '美食;小吃快餐店' '美食;咖啡厅' '美食;蛋糕甜品店' '美食;茶座' '房地产;其他'
     '美食;外国餐厅' '美食' '公司企业;公司' '购物;商铺' nan '休闲娱乐;洗浴按摩' '美食;酒吧' '交通设施;其他'
     '公司企业;其他' '购物;超市' '酒店;快捷酒店' '教育培训;其他' '酒店;其他' '公司企业;园区']
    __________________________________________________
    ['美食;中餐厅' '美食;其他' '美食;小吃快餐店' '美食;咖啡厅' '美食;蛋糕甜品店' '美食;茶座' '美食;外国餐厅' '美食'
     '美食;酒吧']
    __________________________________________________
    ['中餐厅' '其他' '小吃快餐店' '咖啡厅' '蛋糕甜品店' '茶座' '外国餐厅' '美食' '酒吧']
    __________________________________________________
    ['Chinese_restaurant' 'others' 'Snake_bar' 'cafe' 'CakeANDdessert_shop'
     'teahouse' 'Foreign_restaurant' 'delicacy' 'bar']
    


```python
delicacy_rating_clean.boxplot(column=['detail_info_overall_rating'],by=['detail_info_tag'],figsize=(25,8))
delicacy_rating_clean_std=delicacy_rating_clean.set_index(['detail_info_tag']).std(level=0)
delicacy_rating_clean_std
```

    C:\Users\richi\anaconda3\envs\USDA_database\lib\site-packages\matplotlib\cbook\__init__.py:1376: VisibleDeprecationWarning:
    
    Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_overall_rating</th>
    </tr>
    <tr>
      <th>detail_info_tag</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Chinese_restaurant</th>
      <td>0.703622</td>
    </tr>
    <tr>
      <th>others</th>
      <td>0.751135</td>
    </tr>
    <tr>
      <th>Snake_bar</th>
      <td>0.826895</td>
    </tr>
    <tr>
      <th>cafe</th>
      <td>0.273044</td>
    </tr>
    <tr>
      <th>CakeANDdessert_shop</th>
      <td>0.375599</td>
    </tr>
    <tr>
      <th>teahouse</th>
      <td>0.678449</td>
    </tr>
    <tr>
      <th>Foreign_restaurant</th>
      <td>0.265208</td>
    </tr>
    <tr>
      <th>delicacy</th>
      <td>0.529150</td>
    </tr>
    <tr>
      <th>bar</th>
      <td>0.589164</td>
    </tr>
  </tbody>
</table>
</div>




    
<a href=""><img src="./imgs/2_1_1_13.png" height='auto' width='auto' title="caDesign"></a>
    


* 标准计分（分数）

建立简单数据示例，数据来源于《漫画统计学》考试成绩。在这个案例中，虽然Mason和Reece分别在English和Chinese科目中，以及history和biology科目中具有相同的分数，但是因为标准差，即离散程度不同，所表示的重要程度亦不一样。标准差越小，离散程度越小，则数值每一单位的变化都会影响最终排名，即每一分都很重要。也可以理解为标准差小时，其他同学相对容易追上你的成绩，但是标准差大时，其他同学相对不容易追上你的成绩。


```python
test_score_dic={"English":{"Mason":90,"Reece":81,'A':73,'B':97,'C':85,'D':60,'E':74,'F':64,'G':72,'H':67,'I':87,'J':78,'K':85,'L':96,'M':77,'N':100,'O':92,'P':86},
                "Chinese":{"Mason":71,"Reece":90,'A':79,'B':70,'C':67,'D':66,'E':60,'F':83,'G':57,'H':85,'I':93,'J':89,'K':78,'L':74,'M':65,'N':78,'O':53,'P':80},
                "history":{"Mason":73,"Reece":61,'A':74,'B':47,'C':49,'D':87,'E':69,'F':65,'G':36,'H':7,'I':53,'J':100,'K':57,'L':45,'M':56,'N':34,'O':37,'P':70},
                "biology":{"Mason":59,"Reece":73,'A':47,'B':38,'C':63,'D':56,'E':75,'F':53,'G':80,'H':50,'I':41,'J':62,'K':44,'L':26,'M':91,'N':35,'O':53,'P':68},
               }

test_score=pd.DataFrame.from_dict(test_score_dic)
test_score.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>Chinese</th>
      <th>history</th>
      <th>biology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Mason</th>
      <td>90</td>
      <td>71</td>
      <td>73</td>
      <td>59</td>
    </tr>
    <tr>
      <th>Reece</th>
      <td>81</td>
      <td>90</td>
      <td>61</td>
      <td>73</td>
    </tr>
    <tr>
      <th>A</th>
      <td>73</td>
      <td>79</td>
      <td>74</td>
      <td>47</td>
    </tr>
    <tr>
      <th>B</th>
      <td>97</td>
      <td>70</td>
      <td>47</td>
      <td>38</td>
    </tr>
    <tr>
      <th>C</th>
      <td>85</td>
      <td>67</td>
      <td>49</td>
      <td>63</td>
    </tr>
  </tbody>
</table>
</div>



求标准计分（Standard Score），又称z-score即Z-分数，或标准化值。z-score代表原始数值和平均值之间的距离，并以标准差为单位计算，即z-score是从感兴趣的点到均值之间有多少个标准差，这样就可以在不同组数据间比较某一数值的重要程度。公式为：$z=(x- \mu )/ \sigma $其中，$\sigma  \neq 0$  并$x$是需要被标准化的原始分数，$\mu$是平均值，$\sigma$是标准差。

标准计分的特征：

1. 无论作为变量的满分为几分，其标准计分的平均数势必为0， 而其标准差势必为1；
2. 无论作为变量的单位是什么，其标准计分的平均数势必为0， 而其标准差势必为1. 


```python
from scipy.stats import zscore
test_Zscore=test_score.apply(zscore)
test_Zscore.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>Chinese</th>
      <th>history</th>
      <th>biology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Mason</th>
      <td>0.770054</td>
      <td>-0.296174</td>
      <td>0.780635</td>
      <td>0.162355</td>
    </tr>
    <tr>
      <th>Reece</th>
      <td>-0.029617</td>
      <td>1.392020</td>
      <td>0.207107</td>
      <td>1.014719</td>
    </tr>
    <tr>
      <th>A</th>
      <td>-0.740436</td>
      <td>0.414644</td>
      <td>0.828429</td>
      <td>-0.568242</td>
    </tr>
    <tr>
      <th>B</th>
      <td>1.392020</td>
      <td>-0.385027</td>
      <td>-0.462008</td>
      <td>-1.116191</td>
    </tr>
    <tr>
      <th>C</th>
      <td>0.325792</td>
      <td>-0.651584</td>
      <td>-0.366420</td>
      <td>0.405887</td>
    </tr>
  </tbody>
</table>
</div>



其中Mason在English科目中的标准计分为0.77，在整体分布中位于平均分之上0.71个标准差的地位；而Reece在Chinese中的标准计分为1.39，在整体分布中位于平均分之上1.39个标准差的地位，即Reece获得的每一分值价值高于Mason所获取的每一分值。

返回到POI实验数据，分别计算美食部分总体评分'detail_info_overall_rating'和价格 'detail_info_price'的标准计分。


```python
pd.options.mode.chained_assignment=None
delicacy=poi_gdf.xs('poi_0_delicacy',level=0)
delicacy_dropna=delicacy.dropna(subset=['detail_info_overall_rating', 'detail_info_price'])
delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']]=delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']].astype(float)
delicacy_Zscore=delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']].apply(zscore).join(delicacy["name"])
delicacy_Zscore
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_overall_rating</th>
      <th>detail_info_price</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>0.500851</td>
      <td>-0.903030</td>
      <td>关中印象咥长安(创汇店)</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.597102</td>
      <td>-0.493965</td>
      <td>哪儿托海鲜焖面</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1.598804</td>
      <td>-0.493965</td>
      <td>赛百味(昆明池店)</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.720441</td>
      <td>-0.396569</td>
      <td>太平洋咖啡(昆明池店)</td>
    </tr>
    <tr>
      <th>28</th>
      <td>-1.255874</td>
      <td>-0.260214</td>
      <td>文丰厨8</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6432</th>
      <td>-0.597102</td>
      <td>-0.347870</td>
      <td>佳美蛋糕(庆华店)</td>
    </tr>
    <tr>
      <th>6448</th>
      <td>1.598804</td>
      <td>-0.786154</td>
      <td>袁师傅腊汁肉夹馍</td>
    </tr>
    <tr>
      <th>6462</th>
      <td>-7.184819</td>
      <td>-0.864071</td>
      <td>千里香馄饨王(灞桥堡路店)</td>
    </tr>
    <tr>
      <th>6490</th>
      <td>-0.377511</td>
      <td>0.051455</td>
      <td>窑村猪蹄坊总店(纺渭路店)</td>
    </tr>
    <tr>
      <th>6518</th>
      <td>0.500851</td>
      <td>-0.727716</td>
      <td>食膳坊中式快餐</td>
    </tr>
  </tbody>
</table>
<p>2286 rows × 3 columns</p>
</div>



在实验数据里可以计算z_score比较某一饭店的价格和评分的重要性，其意义是，是否价格越接近均值，对应的评分越高于（低于）均值。但是单独看单一饭店的数据很难判断是否存在这样的一种关系，因此可以打印曲线，观察曲线的变化规律。使用df.rolling()方法平滑数据后绘制曲线观察数据。


```python
import matplotlib.pyplot as plt
delicacy_Zscore.rolling(50, win_type='triang').sum().plot.line(figsize=(25,8))
plt.axhline(y=0,color='gray',linestyle='--')
plt.show()
```


    
<a href=""><img src="./imgs/2_1_1_14.png" height='auto' width='auto' title="caDesign"></a>
    


### 2.1.1.4 正态分布与概率密度函数，异常值处理

#### 1）正态分布

在开始概率密度函数之前，认识下正态分布，又称常态分布、高斯分布（normal distribution/Gaussian distribution），因为正态分布的形状，正态分布常见的名称为钟形曲线（bell curve）。用numpy库中的`numpy.random.normal()`方法生成满足指定平均值、和标准差的一维数据（也可以用scipy库stats类中提供的norm的方法），并打印直方图和对应的由概率密度函数计算的分布曲线。下图y轴表示随机生成数值的频数(因为配置了density=Ture，返回的频数为标准化后的结果)，x轴为随机生成的数据集数值分布，因为设置了plt.hist()中`bins=30`，即将数值分为30等分，计算每一等分（频数宽度）的频数。曲线的顶点为均值、中位数和众数，频数为最高，对应的值为0，该值也为众数。可以从新定义平均数mu参数的值，曲线顶点也会随之变动。从众数向两侧移动，曲线高度下降，表示这些值出现的情形逐渐减少，即频数降低。在统计学上下图可以文字描述为：x服从平均值为0，标准差为0.1的正态分布。

依据正态分布的形状，其三个基本性质为：一，它是对称的，意味左右部分互为镜像；二，均值、中位数和众数处于同一位置，且在分布的中心，即钟形的顶点。表现为曲线中心最高，首尾两端向下倾斜，呈单峰状；三是，正态分布是渐近的，意味分布的左尾和右尾永远不会触及底线，即x轴。

正态分布具有重要意义，自然界与人类社会中经常出现正态分布的各类数据，例如经济中的收入水平，人的智商（IQ）分数和考试成绩，自然界中受大量微小随机扰动影响的事物等。因此能够依据这一现象推断出现某种情形的准确概率。同时，需要注意，正态分布是统计学中所谓的理论分布，即很少有数值严格服从正态分布，而是近似于，也有可能相差较远。违背正态分布假设，则依据正态分布假设所计算的概率结果将不再有效。

> 对于统计学的知识，通常是依据已出版的著作或者教材为依据进行解说，并根据阐述的内容，以python语言为工具分析数据的变化，从而可以直接应用代码来解决对应的问题，并能够通过具体的数据分析更深入的理解统计学相关的知识。本节参考：Timothy C.Urdan.Statistics in Plain English(白话统计学)[M].中国人民大学出版社.2013,12.第3版.


```python
#下述案例参考SciPy.org 中`numpy.random.normal`案例
import numpy as np
import math

#依据分布配置参数生成数据 Draw samples from the distribution
mu, sigma=0, 0.1 #配置平均值和标准差 mean and standard deviation
s=np.random.normal(mu, sigma, 1000)
#验证生成的数据是否满足配置参数 Verify the mean and the variance
print("mean<0.01:",abs(mu-np.mean(s)) < 0.01)
print("sigmag variance<0.01",abs(sigma - np.std(s, ddof=1)) < 0.01)
import matplotlib.pyplot as plt
count, bins, ignored=plt.hist(s,bins=30,density=True) #参数density=True；If True, the first element of the return tuple will be the counts normalized to form a probability density
PDF=1/(sigma * np.sqrt(2 * np.pi)) *np.exp( - (bins - mu)**2 / (2 * sigma**2) ) #y方向的值计算公式即为概率密度函数
plt.plot(bins,PDF ,linewidth=2, color='r',label='PDF') 

CDF=PDF.cumsum() #计算累积分布函数
CDF/=CDF[-1] #通过除以最大值，将CDF数值分布调整在[0,1]的区间
plt.plot(bins,CDF,linewidth=2, color='g',label='CDF') 

plt.legend()
plt.show()
```

    mean<0.01: True
    sigmag variance<0.01 True
    


    
<a href=""><img src="./imgs/2_1_1_15.png" height='auto' width='auto' title="caDesign"></a>
    


#### 2）概率密度函数（Probability density function，PDF）

当直方图的组距无限缩小至极限后，能够拟合出一条曲线，计算这个分布曲线的公式即为概率密度函数：$f(x)= \frac{1}{ \sigma  \times  \sqrt{2 \pi } }  \times  e^{- \frac{1}{2}  [ \frac{x- \mu }{  \sigma  } ]^{2} } $ 其中，$\sigma$为标准差；$\mu$为平均值;$e$为自然对数的底，其值大约为2.7182...。在上述程序中，计算概率论密度函数时，并未计算每个数值，而是使用Plt.hist()返回值bins替代，即每一组距的左边沿和右边沿，这里是划分了30份，因此首位数为第1个频数宽度的左边沿，末位数为最后一个频数宽度的右边沿，而中间的所有是左右边沿重叠。概率密度函数的积分，即为累积分分布函数（cumulative distribution function ,CDF），可以用`numpy.cumsum()`计算 ，为给定轴上数组元素的累积和。

图表打印的库主要包括[Matplotlib](https://matplotlib.org/)，[plotly(含dash)](https://plotly.com/)，[bokeh](https://docs.bokeh.org/en/latest/index.html)，[seaborn](https://seaborn.pydata.org/)等，具体选择哪个打印图表，没有固定的标准，通常根据数据图表打印的目的、哪个库能够满足要求，以及个人更习惯用哪个库来确定。在上述使用Matplotlib库打印密度函数曲线时，是自行计算，而Seaborn提供了`seaborn.distplot()`方法，指定bins参数后可以直接获取上述结果。需要注意bins参数的配置，如果为一个整数值，则是划分同宽度频数宽度（bin）的数量，如果是列表，则为频数宽度的边缘，例如[1,2,3,4]，表示[[1,2),[2,3),[3,4]]的频数宽度列表，`[`代表包含左边沿数据，`]`代表包含右边沿数据,而`(`，和`)`则是分别代表不包含左或右边沿数据。同pandas的`pandas.core.indexes.range.RangeIndex`，即RangeIndex数据格式。

返回到POI实验数据，直接计算打印POI美食数据的价格概率密度函数的值（纵轴），并连为曲线（分布曲线）。为了进一步观察，组距不断缩小时，与曲线的拟合程度，定义一个循环打印多个连续组距变化的直方图，来观察直方图的变化情况。


```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math

poi_gdf=pd.read_pickle('./data/poisInAll/poisInAll_gdf.pkl') #读取已经存储为.pkl格式的POI数据，其中包括geometry字段，为GeoDataFrame地理信息数据，可以通过poi_gpd.plot()迅速查看数据。
delicacy_price=poi_gdf.xs('poi_0_delicacy',level=0).detail_info_price  #提取美食价格数据
delicacy_price_df=delicacy_price.to_frame(name='price').astype(float)

sns.set(style="white", palette="muted", color_codes=True)
bins=list(range(0,50,5))[1:]
bin_num=len(bins)

ncol=5
nrow=math.ceil(bin_num/ncol)
fig, axs = plt.subplots(ncols=ncol,nrows=nrow,figsize=(30,10),sharex=True)
ax=[(row,col) for row in range(nrow) for col in range(ncol)]
i=0
for bin in bins:
    #sns.distplot(delicacy_price_df.price, bins=bin, color="r",ax=axs[ax[i]],kde = True,norm_hist=True).set(title="bins=%d"%bin) #`distplot` is a deprecated function and will be removed in a future version.
    sns.histplot(delicacy_price_df.price, bins=bin, color="r",ax=axs[ax[i]],kde=True, stat="density", linewidth=0).set(title="bins=%d"%bin)    
    i+=1
```


    
<a href=""><img src="./imgs/2_1_1_16.png" height='auto' width='auto' title="caDesign"></a>
    


#### 3）偏度与峰度

描述正态分布的特征（或概率密度函数的分布曲线）有两个概念，一个是偏度（skew）,另一个是峰度（kurtosis）。

由scipy库skewnorm方法建立具有正偏态、和负偏态属性的数据，并由skew方法计算偏度值，其公式为：$skewness= \frac{3( \ \mu  -median)}{ \sigma } $ 其中$\mu$为均值；$sigma$为标准差。偏度值为负，即负偏态，则概率密度函数左侧尾部比右侧长；偏度值为正，即正偏态，概率密度函数右侧尾部比左侧长。

由numpy建立具有尖峰分布（瘦尾）、和扁峰分布（厚尾）属性的数据，配置绝对值参数均为0，变化标准差，并使用scipy库kurtosis方法计算其峰度值(峰度值计算有多个公式，不同软件平台公式也会有所差异)。其中当平均值为0，标准差为1时的正态分布为标准正态分布，概率密度函数公式可以简化为：$f(x)= \frac{1}{   \sqrt{2 \pi } }  \times  e^{- \frac{ x^{2}}{2}}$

> 注意，在用numpy生成数据集时，间接使用了方差$ \sigma ^{2} $的参数，其概率密度函数公式可以调整为：$f(x |  \mu ,  \sigma ^{2} )= \frac{1}{ \sqrt{2 \pi } \sigma ^{2} }  \times  e^{- \frac{ (x- \mu )^{2} }{2\sigma ^{2} }  } $, $\sigma ^{2}$即方差：  $\sigma ^{2} =   \frac{1}{N}   \sum_{i=1}^N { ( x_{i}- \mu ) ^{2} } $


```python
#建立具有正偏态、和负偏态属性的数据
from scipy.stats import skewnorm
from scipy.stats import skew
import matplotlib.pyplot as plt

skew_list=[7,-7]
skewNorm_list=[skewnorm.rvs(a, scale=1,size=1000,random_state=None) for a in skew_list]
skewness=[skew(d) for d in skewNorm_list]
print('skewness for data:',skewness) #验证偏度值

#建立具有尖峰分布、和扁峰分布属性的数据
import numpy as np
import math
from scipy.stats import kurtosis

mu_list=[0,0,0,]
variance=[0.2,1.0,5.0,]
normalDistr_paras=zip(mu_list,[math.sqrt(sig2) for sig2 in variance])#配置多个平均值和标准差对
s_list=[np.random.normal(para[0], para[1], 1000) for para in normalDistr_paras]
kur=[kurtosis(s,fisher=True) for s in s_list]
print("kurtosis for data:",kur)

i=0
for skew_data in skewNorm_list:
    sns.kdeplot(skew_data,label="skewness=%.2f"%skewness[i])
    i+=1

n=0
for s in s_list:
    sns.kdeplot(s,label="μ=%s, $σ^2$=%s,kur=%.2f"%(mu_list[n],variance[n],kur[n]))
    n+=1  
plt.legend()
plt.plot()
```

    skewness for data: [1.0830400029635794, -0.8562037571706261]
    kurtosis for data: [0.023725421889412956, -0.1242413147438346, -0.11527366327344746]
    




    []




    
<a href=""><img src="./imgs/2_1_1_17.png" height='auto' width='auto' title="caDesign"></a>
    


返回POI实验数据，计算POI美食数据价格的偏度和峰度。偏度计算结果值为正，为正偏态，概率密度函数右侧尾部比左侧长，说明多数价格较低，少数较高价格将分布的尾巴托向另一端。峰度为尖峰分布。


```python
delicacy_price_df_clean=delicacy_price_df.dropna()
print("skewness:%.2f, kurtosis:%.2f"%(skew(delicacy_price_df_clean.price),kurtosis(delicacy_price_df_clean.price,fisher=True)))
```

    skewness:4.18, kurtosis:29.84
    

#### 4）检验数据集是否服从正态分布

首先计算标准计分，标准化价格数据集后其平均值为0，标准差为1，绘制概率密度函数分布曲线（理论值）。同时叠加满足上述条件由numpy随机生成，满足正态分布数据集的分布曲线（观察值），可以比较二者的差异。因为实验数据即美食价格是正偏态，可以观察到位于均值左侧的数据有所差异，而右侧趋势基本吻合。同时，具有较高的峰度值，高于标准正态分布曲线。


```python
import pandas as pd
pd.options.mode.chained_assignment=None

def comparisonOFdistribution(df,field,bins=100):
    import pandas as pd
    import numpy as np
    import seaborn as sns
    '''
    funciton-数据集z-score概率密度函数分布曲线(即观察值/实验值 observed/empirical data)与标准正态分布(即理论值 theoretical set)比较
    
    Params:
        df - 包含待分析数据集的DataFrame格式类型数据
        field - 指定所分析df数据的列名
        bins - 指定频数宽度，为单一整数代表频数宽度（bin）的数量；或者列表，代表多个频数宽度的列表
    '''
    df_field_mean=df[field].mean()
    df_field_std=df[field].std()
    print("mean:%.2f, SD:%.2f"%(df_field_mean,df_field_std))

    df['field_norm']=df[field].apply(lambda row: (row-df_field_mean)/df_field_std) #标准化价格(标准计分，z-score)，即标准计分，或者使用`from scipy.stats import zscore`方法

    #验证z-score，标准化后的均值必为0， 标准差必为1.0
    df_fieldNorm_mean=df['field_norm'].mean()
    df_fieldNorm_std=df['field_norm'].std()
    print("norm_mean:%.2f, norm_SD:%.2f"%(df_fieldNorm_mean,df_fieldNorm_std))
  
    sns.histplot(df['field_norm'], bins=bins,kde=True,stat="density",linewidth=0,color='r')

    s=np.random.normal(0, 1, len(df[field]))
    sns.histplot(s, bins=bins,kde=True,stat="density",linewidth=0,color='b')
    
comparisonOFdistribution(delicacy_price_df_clean,'price',bins=100)    
```

    mean:54.19, SD:51.22
    norm_mean:-0.00, norm_SD:1.00
    


    
<a href=""><img src="./imgs/2_1_1_18.png" height='auto' width='auto' title="caDesign"></a>
    


* 异常值处理

偏度的差异从直方图或者箱型图中可以观察到有少数部分较高的价格出现，即为异常值。

处理异常值，先建立简单数据来理解异常值，并找到相应的方法。参考：Boris Iglewicz and David Hoaglin (1993), "Volume 16: How to Detect and Handle Outliers", The ASQC Basic References in Quality Control: Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. 使用其中提供的方法，其公式为：

$MAD= median_{i}{ \{ |  x_{i}-   \widetilde{x} |\} }$ ，其中MAD（the median of the absolute deviation about the median /Median Absolute Deviation）为绝对中位差，其中$\widetilde{x}$为中位数；

$M_{i}= \frac{0.6745( x_{i} - \widetilde{x} )}{MAD}$  ，其中$M_{i}$为修正的z-score(modified  z-score)，$\widetilde{x}$为中位数。

由计算结果，确定该公式能够有效的判断异常值。


```python
import numpy as np
import matplotlib.pyplot as plt

outlier_data=np.array([2.1,2.6,2.4,2.5,2.3,2.1,2.3,2.6,8.2,8.3]) #建立简单的数据，便于观察
ax1=plt.subplot(221)
ax1.margins(0.05) 
ax1.boxplot(outlier_data)
ax1.set_title('simple data before')

def is_outlier(data,threshold=3.5):
    import numpy as np
    '''
    function-判断异常值
        
    Params:
        data - 待分析的数据，列表或者一维数组
        threshold - 判断是否为异常值的边界条件    
    '''
    MAD=np.median(abs(data-np.median(data)))
    modified_ZScore=0.6745*(data-np.median(data))/MAD
    #print(modified_ZScore)
    is_outlier_bool=abs(modified_ZScore)>threshold    
    return is_outlier_bool,data[~is_outlier_bool]
    
is_outlier_bool,data_clean=is_outlier(outlier_data,threshold=3.5)    
print(is_outlier_bool,data_clean)

plt.rcParams["figure.figsize"]=(10,10)
ax2=plt.subplot(222)
ax2.margins(0.05) 
ax2.boxplot(data_clean)
ax2.set_title('simple data after')

_,delicacyPrice_outliersDrop=is_outlier(delicacy_price_df_clean.price,threshold=3.5)
print("原始数据描述:",delicacy_price_df_clean.price.describe())
print("-"*50)
print("异常值处理后数据描述:",delicacyPrice_outliersDrop.describe())

ax3=plt.subplot(223)
ax3.margins(0.05) 
ax3.boxplot(delicacy_price_df_clean.price)
ax3.set_title('experimental data before')

ax3=plt.subplot(224)
ax3.margins(0.05) 
ax3.boxplot(delicacyPrice_outliersDrop)
ax3.set_title('experimental data after')


plt.show()
```

    [False False False False False False False False  True  True] [2.1 2.6 2.4 2.5 2.3 2.1 2.3 2.6]
    原始数据描述: count    2303.000000
    mean       54.186062
    std        51.215379
    min         0.000000
    25%        22.000000
    50%        41.000000
    75%        72.000000
    max       617.000000
    Name: price, dtype: float64
    --------------------------------------------------
    异常值处理后数据描述: count    2232.000000
    mean       47.695116
    std        30.958886
    min         0.000000
    25%        22.000000
    50%        40.000000
    75%        68.000000
    max       155.000000
    Name: price, dtype: float64
    


    
<a href=""><img src="./imgs/2_1_1_19.png" height='auto' width='auto' title="caDesign"></a>
    



```python
plt.rcParams["figure.figsize"]=(5,5)
comparisonOFdistribution(pd.DataFrame(delicacyPrice_outliersDrop,columns=['price']),'price',bins=100)
```

    mean:47.70, SD:30.96
    norm_mean:0.00, norm_SD:1.00
    


    
<a href=""><img src="./imgs/2_1_1_20.png" height='auto' width='auto' title="caDesign"></a>
    


如果将较高的价格视为异常值移除，调用定义的`comparisonOFdistribution(df,field,bins=100)`函数打印概率密度函数，比较标准正态分布可以发现右侧，即较高值的部分发生了变化，其它部分未发生明显变化。那么如何检验数据集是否服从正态分布？scipy库提供有多种正态性检验的方法，分别为`kstest`，`shapiro`，`normaltest`，`anderson`。计算结果显示的p-value基本为0，即p-value<0.05，拒绝原假设，清理异常值后的美食价格数据集仍不服从正态分布，即为非正态数据集。只有正态分布可以计算从一个总体中选取特定值或区间的概率，本次美食价格数据集为正偏和尖峰分布，为非正态数据集，正态分布的概率不能很好适用于该类数据集。


```python
from scipy import stats
kstest_test=stats.kstest(delicacyPrice_outliersDrop,cdf='norm')
print("original data:",kstest_test)
z_score_kstest_test=stats.kstest(stats.zscore(delicacyPrice_outliersDrop),cdf='norm')
print('z_score:',z_score_kstest_test)

shapiro_test=stats.shapiro(delicacyPrice_outliersDrop) #官方文档注释，当N>5000时，只有统计量正确，但是p-value值不一定正确.本次实验`len(delicacyPrice_outliersDrop)`数据量为770，可以使用
print("original data - shapiroResults(statistic=%f,pvalue=%f)"%(shapiro_test))

normaltest_test=stats.normaltest(delicacyPrice_outliersDrop,axis=None)
print("original data:",normaltest_test)

anderson_test=stats.anderson(delicacyPrice_outliersDrop,dist='norm')
print("original data:",anderson_test)
```

    original data: KstestResult(statistic=0.999551684674593, pvalue=0.0)
    z_score: KstestResult(statistic=0.11464960111854455, pvalue=5.153632014592587e-26)
    original data - shapiroResults(statistic=0.918487,pvalue=0.000000)
    original data: NormaltestResult(statistic=232.93794469002853, pvalue=2.6191959927510468e-51)
    original data: AndersonResult(statistic=53.19820994109705, critical_values=array([0.575, 0.655, 0.786, 0.916, 1.09 ]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))
    

#### 5）给定特定值计算概率，以及找到给定概率的值

如果数据集服从正态分布，则可以直接使用scipy库stats类中提供的norm的方法进行计算。下述案例参考scipy官网。使用norm.cdf()、norm.sf()和norm.ppf()分别计算小于或等于特定值、大于或等于特定值、或找到给定概率的值进行计算。


```python
from scipy.stats import norm
import matplotlib.pyplot as plt
import numpy as np

fig, ax=plt.subplots(1, 1)
mean, var, skew, kurt = norm.stats(moments='mvsk')  
print('mean, var, skew, kurt=',(mean, var, skew, kurt)) #验证符合标准正态分布的相关统计量
x=np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100) #norm.ppf 百分比点函数 - Percent point function (inverse of cdf — percentiles)
ax.plot(x, norm.pdf(x), 'r-', lw=5, alpha=0.6, label='norm pdf')  #norm.pdf为概率密度函数
rv=norm() #固定形状（偏度和峰度）、位置loc（平均值）和比例scale（标准差）参数，即指定固定值
ax.plot(x, rv.pdf(x), 'k-', lw=3, label='frozen pdf') #固定/“冻结”分布（frozen distribution）

vals = norm.ppf([0.001, 0.5, 0.999]) #返回概率为0.1%、50%和99.9%的值，默认loc=0,scale=1
print("验证累积分布函数CDF返回值与其PPF返回值是否相等或近似：",np.allclose([0.001, 0.5, 0.999], norm.cdf(vals)))

r=norm.rvs(size=1000) #指定数据集大小
ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
ax.legend(loc='best', frameon=False)
plt.show()
```

    mean, var, skew, kurt= (array(0.), array(1.), array(0.), array(0.))
    验证累积分布函数CDF返回值与其PPF返回值是否相等或近似： True
    


    
<a href=""><img src="./imgs/2_1_1_21.png" height='auto' width='auto' title="caDesign"></a>
    



```python
#如果需要计算概率则定义固定分布的数据集。
print("用.cdf计算值小于或等于113的概率为：",norm.cdf(113,100,12)) #pdf(x, loc=0, scale=1) 配置Loc(均值)和scale(标准差)
print("用.sf计算值大于或等于113待概率为：",norm.sf(113,100,12)) 
print("可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为：",norm.cdf(113,100,12)+norm.sf(113,100,12))
print("用.ppf找到给定概率值为98%的数值为：",norm.ppf(0.86066975,100,12))
```

    用.cdf计算值小于或等于113的概率为： 0.8606697525503779
    用.sf计算值大于或等于113待概率为： 0.13933024744962208
    可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为： 1.0
    用.ppf找到给定概率值为98%的数值为： 112.99999986204986
    

在传统的概率计算中，已知z-score（标准计分），可以通过查表的方式来获取对应的概率值，这种方式已经很少使用。为了直观的观察概率值，由曲线、横轴和通过对应概率密度函数值的垂直线围合的面积即为概率值，通过绘制图表可以更清晰的观察。


```python
def probability_graph(x_i,x_min,x_max,x_s=-9999,left=True,step=0.001,subplot_num=221,loc=0,scale=1):
    import math
    '''
    function - 正态分布概率计算及图形表述
    
    Paras:
        x_i - 待预测概率的值
        x_min - 数据集区间最小值
        x_max - 数据集区间最大值
        x_s - 第2个带预测概率的值，其值大于x_i值
        left - 是否计算小于或等于，或者大于或等于指定值的概率
        step - 数据集区间的步幅
        subplot_num - 打印子图的序号，例如221中，第一个2代表列，第二个2代表行，第三个是子图的序号，即总共2行2列总共4个子图，1为第一个子图
        loc - 即均值
        scale - 标准差
    '''
    x=np.arange(x_min,x_max,step)
    ax=plt.subplot(subplot_num)
    ax.margins(0.2) 
    ax.plot(x,norm.pdf(x,loc=loc,scale=scale))
    ax.set_title('N(%s,$%s^2$),x=%s'%(loc,scale,x_i))
    ax.set_xlabel('x')
    ax.set_ylabel('pdf(x)')
    ax.grid(True)
    
    if x_s==-9999:
        if left:
            px=np.arange(x_min,x_i,step)
            ax.text(loc-loc/10,0.01,round(norm.cdf(x_i,loc=loc,scale=scale),3), fontsize=20)
        else:
            px=np.arange(x_i,x_max,step)
            ax.text(loc+loc/10,0.01,round(1-norm.cdf(x_i,loc=loc,scale=scale),3), fontsize=20)
        
    else:
        px=np.arange(x_s,x_i,step)
        ax.text(loc-loc/10,0.01,round(norm.cdf(x_i,loc=loc,scale=scale)-norm.cdf(x_s,loc=loc,scale=scale),2), fontsize=20)
    ax.set_ylim(0,norm.pdf(loc,loc=loc,scale=scale)+0.005)
    ax.fill_between(px,norm.pdf(px,loc=loc,scale=scale),alpha=0.5, color='g')
    
plt.figure(figsize=(10,10))
probability_graph(x_i=113,x_min=50,x_max=150,step=1,subplot_num=221,loc=100,scale=12)
probability_graph(x_i=113,x_min=50,x_max=150,step=1,left=False,subplot_num=223,loc=100,scale=12)
probability_graph(x_i=113,x_min=50,x_max=150,x_s=90,step=1,subplot_num=222,loc=100,scale=12)
probability_graph(x_i=90,x_min=50,x_max=150,step=1,subplot_num=224,loc=100,scale=20)
plt.show()
```


    
<a href=""><img src="./imgs/2_1_1_22.png" height='auto' width='auto' title="caDesign"></a>
    

